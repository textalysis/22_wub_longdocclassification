[nltk_data] Downloading package punkt to /vol/fob-
[nltk_data]     vol3/nebenf20/wubingti/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/usr/lib64/python3.6/site-packages/h5py/__init__.py:39: UserWarning: h5py is running against HDF5 1.10.8 when it was built against 1.10.7, this may cause problems
  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)
datasets imported
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
ECtHR_ToBERT_256_50_5
----------
Epoch 1/40
time = 1156.02 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Train loss 0.2001404967763134 micro_f1_score 0.7313287247126681 
 
time = 57.52 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss 0.16889469056832987 micro_f1_score 0.7538160469667319
 
----------
Epoch 2/40
time = 1145.30 secondes

Train loss 0.1371012013455903 micro_f1_score 0.8201526469632996 
 
time = 57.91 secondes

Val loss 0.16247995271057378 micro_f1_score 0.7663335895465028
 
----------
Epoch 3/40
time = 1146.91 secondes

Train loss 0.11861063042269633 micro_f1_score 0.8513302034428795 
 
time = 57.03 secondes

Val loss 0.15989352457347464 micro_f1_score 0.7829722538958572
 
----------
Epoch 4/40
time = 1145.85 secondes

Train loss 0.10521007830881186 micro_f1_score 0.8723327882582859 
 
time = 57.37 secondes

Val loss 0.16396029470641105 micro_f1_score 0.7879472693032015
 
----------
Epoch 5/40
time = 1144.41 secondes

Train loss 0.09427191351229945 micro_f1_score 0.8872299872935198 
 
time = 57.91 secondes

Val loss 0.1666264686672414 micro_f1_score 0.7877428998505231
 
----------
Epoch 6/40
time = 1151.91 secondes

Train loss 0.08359505664347461 micro_f1_score 0.902443839077737 
 
time = 58.39 secondes

Val loss 0.1754034823570095 micro_f1_score 0.7864834756776828
 
----------
Epoch 7/40
time = 1145.78 secondes

Train loss 0.07438017401995288 micro_f1_score 0.9142362341087102 
 
time = 57.08 secondes

Val loss 0.18429521965931672 micro_f1_score 0.7925219941348974
 
----------
Epoch 8/40
time = 1132.09 secondes

Train loss 0.0664629488770204 micro_f1_score 0.9256995062308958 
 
time = 57.46 secondes

Val loss 0.19097534609866923 micro_f1_score 0.7905454545454546
 
----------
Epoch 9/40
time = 1150.74 secondes

Train loss 0.05860848386419584 micro_f1_score 0.9352029059094638 
 
time = 57.71 secondes

Val loss 0.20095286463372042 micro_f1_score 0.7858459270180612
 
----------
Epoch 10/40
time = 1140.08 secondes

Train loss 0.052601729368849715 micro_f1_score 0.9422492401215806 
 
time = 57.27 secondes

Val loss 0.2043246744108982 micro_f1_score 0.793743179337941
 
----------
Epoch 11/40
time = 1142.06 secondes

Train loss 0.04636756011603719 micro_f1_score 0.950087260034904 
 
time = 56.32 secondes

Val loss 0.23047142194919898 micro_f1_score 0.779312787814382
 
----------
Epoch 12/40
time = 1114.58 secondes

Train loss 0.04039854531067315 micro_f1_score 0.9561648079306072 
 
time = 45.43 secondes

Val loss 0.22297811031830114 micro_f1_score 0.7951114306254492
 
----------
Epoch 13/40
time = 668.08 secondes

Train loss 0.03443213124373542 micro_f1_score 0.964135428328765 
 
time = 43.98 secondes

Val loss 0.23846257948240296 micro_f1_score 0.7959914101646385
 
----------
Epoch 14/40
time = 669.03 secondes

Train loss 0.030587558780675168 micro_f1_score 0.9677816698102313 
 
time = 43.63 secondes

Val loss 0.25157872172164136 micro_f1_score 0.7922497308934338
 
----------
Epoch 15/40
time = 668.85 secondes

Train loss 0.026500408780165352 micro_f1_score 0.9732365702876012 
 
time = 44.08 secondes

Val loss 0.2705812740948845 micro_f1_score 0.7928571428571427
 
----------
Epoch 16/40
time = 669.03 secondes

Train loss 0.021778981915207343 micro_f1_score 0.9779228258782875 
 
time = 43.24 secondes

Val loss 0.29088400682953536 micro_f1_score 0.781715095676825
 
----------
Epoch 17/40
time = 669.13 secondes

Train loss 0.01845699803146475 micro_f1_score 0.9811248516405682 
 
time = 45.32 secondes

Val loss 0.283506162342478 micro_f1_score 0.7896935933147632
 
----------
Epoch 18/40
time = 666.98 secondes

Train loss 0.01504603734998733 micro_f1_score 0.9864301823324796 
 
time = 43.01 secondes

Val loss 0.3032542861631659 micro_f1_score 0.7871657754010695
 
----------
Epoch 19/40
time = 669.14 secondes

Train loss 0.012898981030547974 micro_f1_score 0.9881247852151666 
 
time = 43.66 secondes

Val loss 0.3103027842083915 micro_f1_score 0.7864527629233512
 
----------
Epoch 20/40
time = 668.30 secondes

Train loss 0.011723130593652657 micro_f1_score 0.9888825214899712 
 
time = 43.77 secondes

Val loss 0.3335140501378012 micro_f1_score 0.7828877005347594
 
----------
Epoch 21/40
time = 668.14 secondes

Train loss 0.009550834182752894 micro_f1_score 0.9906962556241898 
 
time = 43.64 secondes

Val loss 0.31794096553911927 micro_f1_score 0.7887527036770007
 
----------
Epoch 22/40
time = 667.79 secondes

Train loss 0.008402184325730248 micro_f1_score 0.9922959572845156 
 
time = 43.71 secondes

Val loss 0.34601713845231497 micro_f1_score 0.7921288014311271
 
----------
Epoch 23/40
time = 664.84 secondes

Train loss 0.007418682005005099 micro_f1_score 0.9931071251761302 
 
time = 43.55 secondes

Val loss 0.343955575992338 micro_f1_score 0.7877917414721725
 
----------
Epoch 24/40
time = 667.39 secondes

Train loss 0.00623320322173075 micro_f1_score 0.9940192754561731 
 
time = 43.26 secondes

Val loss 0.35252522139764225 micro_f1_score 0.7862068965517242
 
----------
Epoch 25/40
Exception
CUDA out of memory. Tried to allocate 720.00 MiB (GPU 0; 79.20 GiB total capacity; 63.60 GiB already allocated; 607.31 MiB free; 63.72 GiB reserved in total by PyTorch)
