[nltk_data] Downloading package punkt to /vol/fob-
[nltk_data]     vol3/nebenf20/wubingti/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
datasets imported
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
ECtHR_Bigbird_1024_64_1
----------
Epoch 1/40
time = 673.52 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Train loss 0.2747667538086036 micro_f1_score 0.5970350404312669 
 
time = 20.39 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss 0.21524867855134558 micro_f1_score 0.6876456876456876
 
----------
Epoch 2/40
time = 678.13 secondes

Train loss 0.1748460274860934 micro_f1_score 0.7602787743824488 
 
time = 20.80 secondes

Val loss 0.18726136633118645 micro_f1_score 0.7402496099843995
 
----------
Epoch 3/40
time = 676.63 secondes

Train loss 0.14676110969000572 micro_f1_score 0.8108723958333335 
 
time = 20.79 secondes

Val loss 0.19894481548031823 micro_f1_score 0.7288519637462235
 
----------
Epoch 4/40
time = 679.96 secondes

Train loss 0.1286774612090609 micro_f1_score 0.8376997062256025 
 
time = 21.27 secondes

Val loss 0.1989308208471439 micro_f1_score 0.7403636363636364
 
----------
Epoch 5/40
time = 683.96 secondes

Train loss 0.11146830747927632 micro_f1_score 0.8615802291691621 
 
time = 20.71 secondes

Val loss 0.19647692571409414 micro_f1_score 0.7532086541987533
 
----------
Epoch 6/40
time = 677.88 secondes

Train loss 0.09675895595872724 micro_f1_score 0.8857788526345003 
 
time = 20.23 secondes

Val loss 0.20962173308505386 micro_f1_score 0.7545520757465405
 
----------
Epoch 7/40
time = 676.29 secondes

Train loss 0.08440328408032656 micro_f1_score 0.9041784480050268 
 
time = 20.14 secondes

Val loss 0.2170173567338068 micro_f1_score 0.7605633802816901
 
----------
Epoch 8/40
time = 672.87 secondes

Train loss 0.07175547977708079 micro_f1_score 0.9216330361332709 
 
time = 20.28 secondes

Val loss 0.24299950861051434 micro_f1_score 0.753851666069509
 
----------
Epoch 9/40
time = 678.43 secondes

Train loss 0.060780696792377006 micro_f1_score 0.933872409905532 
 
time = 20.61 secondes

Val loss 0.25103322624183094 micro_f1_score 0.7615942028985507
 
----------
Epoch 10/40
time = 672.14 secondes

Train loss 0.05045988216140383 micro_f1_score 0.9472542792967238 
 
time = 20.11 secondes

Val loss 0.2677754491689752 micro_f1_score 0.7614579574160951
 
----------
Epoch 11/40
time = 684.37 secondes

Train loss 0.042259596122705655 micro_f1_score 0.9546893091470475 
 
time = 20.04 secondes

Val loss 0.29988659808381657 micro_f1_score 0.7542857142857142
 
----------
Epoch 12/40
time = 693.41 secondes

Train loss 0.03618078001539919 micro_f1_score 0.9627835447904651 
 
time = 19.96 secondes

Val loss 0.30817885745744233 micro_f1_score 0.7575971731448764
 
----------
Epoch 13/40
time = 693.70 secondes

Train loss 0.030553089549370648 micro_f1_score 0.9667857005721306 
 
time = 19.96 secondes

Val loss 0.3095569801135141 micro_f1_score 0.7678311499272198
 
----------
Epoch 14/40
time = 692.67 secondes

Train loss 0.02648843726424316 micro_f1_score 0.9728964091570325 
 
time = 20.43 secondes

Val loss 0.35009293270404224 micro_f1_score 0.7503506311360449
 
----------
Epoch 15/40
time = 690.30 secondes

Train loss 0.023830213264283213 micro_f1_score 0.9742494182275969 
 
time = 20.09 secondes

Val loss 0.36055789374914327 micro_f1_score 0.7594564818215204
 
----------
Epoch 16/40
time = 691.32 secondes

Train loss 0.021329797982229844 micro_f1_score 0.9784662459361254 
 
time = 20.08 secondes

Val loss 0.40656282850464837 micro_f1_score 0.7476764199655765
 
----------
Epoch 17/40
time = 692.76 secondes

Train loss 0.01719713285817085 micro_f1_score 0.9816461250810853 
 
time = 20.06 secondes

Val loss 0.42822732251198564 micro_f1_score 0.7451248717071502
 
----------
Epoch 18/40
time = 688.47 secondes

Train loss 0.016048157881740334 micro_f1_score 0.9833988474602144 
 
time = 20.24 secondes

Val loss 0.4241260428164826 micro_f1_score 0.7542403464453266
 
----------
Epoch 19/40
time = 695.00 secondes

Train loss 0.013004147251574994 micro_f1_score 0.9866412940057089 
 
time = 20.51 secondes

Val loss 0.44363188181744245 micro_f1_score 0.752411575562701
 
----------
Epoch 20/40
time = 694.54 secondes

Train loss 0.012417138267404438 micro_f1_score 0.9875509194045762 
 
time = 20.12 secondes

Val loss 0.438053159806572 micro_f1_score 0.7532005689900426
 
----------
Epoch 21/40
time = 688.34 secondes

Train loss 0.013395651624297308 micro_f1_score 0.9861417802482296 
 
time = 19.98 secondes

Val loss 0.4393099359557277 micro_f1_score 0.7524680073126143
 
----------
Epoch 22/40
time = 690.69 secondes

Train loss 0.01060419460129426 micro_f1_score 0.98950091296409 
 
time = 20.51 secondes

Val loss 0.47247261663929363 micro_f1_score 0.7467811158798283
 
----------
Epoch 23/40
time = 689.52 secondes

Train loss 0.009934103993768699 micro_f1_score 0.9903732734675241 
 
time = 20.39 secondes

Val loss 0.47411898535783176 micro_f1_score 0.7483777937995675
 
----------
Epoch 24/40
time = 712.18 secondes

Train loss 0.009564219850086899 micro_f1_score 0.99078586658544 
 
time = 29.93 secondes

Val loss 0.4640339759529614 micro_f1_score 0.7582340574632096
 
----------
Epoch 25/40
time = 885.19 secondes

Train loss 0.008335565424023845 micro_f1_score 0.9914757591901971 
 
time = 20.17 secondes

Val loss 0.5200209441732188 micro_f1_score 0.7526652452025586
 
----------
Epoch 26/40
time = 686.69 secondes

Train loss 0.008264003020451275 micro_f1_score 0.9918241624519908 
 
time = 20.12 secondes

Val loss 0.4943874478340149 micro_f1_score 0.7487401007919365
 
----------
Epoch 27/40
time = 686.58 secondes

Train loss 0.00774543031252465 micro_f1_score 0.9923158855751675 
 
time = 20.13 secondes

Val loss 0.49613026248627023 micro_f1_score 0.7545388525780683
 
----------
Epoch 28/40
time = 782.47 secondes

Train loss 0.005496845553532874 micro_f1_score 0.9947114104173801 
 
time = 27.12 secondes

Val loss 0.5115943216398114 micro_f1_score 0.758221900975786
 
----------
Epoch 29/40
time = 735.71 secondes

Train loss 0.006136895617224205 micro_f1_score 0.9936538096142885 
 
time = 20.06 secondes

Val loss 0.5173875953330368 micro_f1_score 0.7573878146661802
 
----------
Epoch 30/40
time = 693.84 secondes

Train loss 0.004701270431476522 micro_f1_score 0.9952866048350312 
 
time = 20.07 secondes

Val loss 0.5441558177842468 micro_f1_score 0.7433116413593637
 
----------
Epoch 31/40
time = 686.32 secondes

Train loss 0.004527878009834206 micro_f1_score 0.9957783440459438 
 
time = 20.03 secondes

Val loss 0.5336140827810179 micro_f1_score 0.7494631352899068
 
----------
Epoch 32/40
time = 691.35 secondes

Train loss 0.00454626490245901 micro_f1_score 0.9964280285757714 
 
time = 20.44 secondes

Val loss 0.5360769885973852 micro_f1_score 0.760014179369018
 
----------
Epoch 33/40
time = 687.52 secondes

Train loss 0.003663634177789661 micro_f1_score 0.9965795074490728 
 
time = 20.03 secondes

Val loss 0.5472595310602032 micro_f1_score 0.755967224795155
 
----------
Epoch 34/40
time = 1190.98 secondes

Train loss 0.0025941232863309826 micro_f1_score 0.9973408296611458 
 
time = 44.47 secondes

Val loss 0.528718503527954 micro_f1_score 0.7637422642883146
 
----------
Epoch 35/40
time = 1446.97 secondes

Train loss 0.0026801873509693986 micro_f1_score 0.9970732448971834 
 
time = 44.36 secondes

Val loss 0.5348291754966876 micro_f1_score 0.7665830046611688
 
----------
Epoch 36/40
time = 1187.97 secondes

Train loss 0.0020920827248201105 micro_f1_score 0.9977596354661097 
 
time = 35.34 secondes

Val loss 0.5502930191940949 micro_f1_score 0.7561761546723954
 
----------
Epoch 37/40
time = 1148.65 secondes

Train loss 0.0018588313004993078 micro_f1_score 0.9979862456780272 
 
time = 35.50 secondes

Val loss 0.559512158886331 micro_f1_score 0.7566594672426208
 
----------
Epoch 38/40
time = 1150.19 secondes

Train loss 0.0013055992944246428 micro_f1_score 0.9988225016143123 
 
time = 35.29 secondes

Val loss 0.5592148352841861 micro_f1_score 0.7651487988526354
 
----------
Epoch 39/40
time = 1146.36 secondes

Train loss 0.0009200753935835343 micro_f1_score 0.9990882844552501 
 
time = 35.50 secondes

Val loss 0.5503614787928394 micro_f1_score 0.7716027249910363
 
----------
Epoch 40/40
time = 1146.78 secondes

Train loss 0.0007333717330053529 micro_f1_score 0.9995061728395062 
 
time = 35.39 secondes

Val loss 0.5782008317650341 micro_f1_score 0.7634024303073623
 
----------
best_f1_socre 0.7716027249910363 best_epoch 39

average train time 785.515813779831

average val time 23.791685903072356
 
time = 36.67 secondes

test_f1_score 0.7527777777777777

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_128_1
----------
Epoch 1/40
Attention type 'block_sparse' is not possible if sequence_length: 1024 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 1408 with config.block_size = 128, config.num_random_blocks = 3. Changing attention type to 'original_full'...
time = 710.55 secondes

Train loss 0.29950906666549476 micro_f1_score 0.5349794238683129 
 
time = 29.57 secondes

Val loss 0.2815650049291673 micro_f1_score 0.47777777777777775
 
----------
Epoch 2/40
time = 713.16 secondes

Train loss 0.2178080552668722 micro_f1_score 0.6625247851949769 
 
time = 29.38 secondes

Val loss 0.22500192532773877 micro_f1_score 0.6624254473161033
 
----------
Epoch 3/40
time = 712.24 secondes

Train loss 0.18171808938453862 micro_f1_score 0.7406185567010308 
 
time = 29.38 secondes

Val loss 0.20416525262789648 micro_f1_score 0.703588143525741
 
----------
Epoch 4/40
time = 712.13 secondes

Train loss 0.16449353134444167 micro_f1_score 0.7754853383153278 
 
time = 29.34 secondes

Val loss 0.19441374574528367 micro_f1_score 0.7233396584440228
 
----------
Epoch 5/40
time = 711.66 secondes

Train loss 0.15525233913716432 micro_f1_score 0.7946417796371535 
 
time = 29.47 secondes

Val loss 0.2016149286608227 micro_f1_score 0.7117887690591298
 
----------
Epoch 6/40
time = 715.15 secondes

Train loss 0.14437316278046047 micro_f1_score 0.8163151166506871 
 
time = 29.37 secondes

Val loss 0.19949738192753713 micro_f1_score 0.7208008898776419
 
----------
Epoch 7/40
time = 712.78 secondes

Train loss 0.13547621957185838 micro_f1_score 0.8311944718657452 
 
time = 29.32 secondes

Val loss 0.1891408358685306 micro_f1_score 0.7443581206067332
 
----------
Epoch 8/40
time = 712.27 secondes

Train loss 0.12492853962724004 micro_f1_score 0.8455587733566072 
 
time = 29.40 secondes

Val loss 0.2106051059042821 micro_f1_score 0.725111441307578
 
----------
Epoch 9/40
time = 712.14 secondes

Train loss 0.11326899221754289 micro_f1_score 0.8608054273655977 
 
time = 29.39 secondes

Val loss 0.20400182597461294 micro_f1_score 0.747643219724438
 
----------
Epoch 10/40
time = 712.98 secondes

Train loss 0.10363794223044638 micro_f1_score 0.8759757155247182 
 
time = 29.40 secondes

Val loss 0.20615474672102538 micro_f1_score 0.7523739956172388
 
----------
Epoch 11/40
time = 711.74 secondes

Train loss 0.09404462047575696 micro_f1_score 0.8879833875328135 
 
time = 29.33 secondes

Val loss 0.22357153758162357 micro_f1_score 0.7515550676911817
 
----------
Epoch 12/40
time = 711.00 secondes

Train loss 0.08371707144369547 micro_f1_score 0.9034453091392594 
 
time = 29.37 secondes

Val loss 0.2281590187158741 micro_f1_score 0.7545945945945947
 
----------
Epoch 13/40
time = 710.93 secondes

Train loss 0.07683055312330793 micro_f1_score 0.9121729326231484 
 
time = 29.28 secondes

Val loss 0.23144635277204825 micro_f1_score 0.7620087336244542
 
----------
Epoch 14/40
time = 714.96 secondes

Train loss 0.07094088247683537 micro_f1_score 0.9222721636701797 
 
time = 29.46 secondes

Val loss 0.2541588548509801 micro_f1_score 0.7443881245474294
 
----------
Epoch 15/40
time = 710.11 secondes

Train loss 0.06362429082913844 micro_f1_score 0.9310424710424711 
 
time = 29.32 secondes

Val loss 0.268356059051928 micro_f1_score 0.7414529914529915
 
----------
Epoch 16/40
time = 712.22 secondes

Train loss 0.056000252343663896 micro_f1_score 0.9405288887178106 
 
time = 29.36 secondes

Val loss 0.2880467437818402 micro_f1_score 0.7395135706732463
 
----------
Epoch 17/40
time = 723.05 secondes

Train loss 0.049847147897355726 micro_f1_score 0.946927481649437 
 
time = 30.94 secondes

Val loss 0.3171956629538145 micro_f1_score 0.7416201117318436
 
----------
Epoch 18/40
time = 727.50 secondes

Train loss 0.042691128473384896 micro_f1_score 0.9537257309492747 
 
time = 31.50 secondes

Val loss 0.34696625441801354 micro_f1_score 0.7277227722772278
 
----------
Epoch 19/40
time = 729.96 secondes

Train loss 0.0394218450663863 micro_f1_score 0.9580025313542745 
 
time = 30.72 secondes

Val loss 0.34942505301022136 micro_f1_score 0.7370990237099023
 
----------
Epoch 20/40
time = 726.79 secondes

Train loss 0.035525006744675 micro_f1_score 0.9629572937394765 
 
time = 32.56 secondes

Val loss 0.3470744993843016 micro_f1_score 0.736436821841092
 
----------
Epoch 21/40
time = 723.27 secondes

Train loss 0.029382888197076615 micro_f1_score 0.969124952235384 
 
time = 31.67 secondes

Val loss 0.37281018996336424 micro_f1_score 0.7375937165298108
 
----------
Epoch 22/40
time = 721.86 secondes

Train loss 0.02981383374027975 micro_f1_score 0.9700003821607368 
 
time = 31.18 secondes

Val loss 0.3554470784595755 micro_f1_score 0.7453637660485021
 
----------
Epoch 23/40
time = 726.76 secondes

Train loss 0.02665256927427542 micro_f1_score 0.9721385829172543 
 
time = 30.91 secondes

Val loss 0.3748141769014421 micro_f1_score 0.7471014492753623
 
----------
Epoch 24/40
time = 720.10 secondes

Train loss 0.022375572353878336 micro_f1_score 0.9773524477657465 
 
time = 31.12 secondes

Val loss 0.37750535323971607 micro_f1_score 0.7507183908045976
 
----------
Epoch 25/40
time = 717.76 secondes

Train loss 0.020943750340810363 micro_f1_score 0.9789116424512833 
 
time = 30.57 secondes

Val loss 0.3764031581947061 micro_f1_score 0.7580760947595119
 
----------
Epoch 26/40
time = 715.47 secondes

Train loss 0.01809346964572313 micro_f1_score 0.9806476190476191 
 
time = 30.19 secondes

Val loss 0.39285375373285325 micro_f1_score 0.7514577259475219
 
----------
Epoch 27/40
time = 716.73 secondes

Train loss 0.016199606098287092 micro_f1_score 0.9831270234241095 
 
time = 31.20 secondes

Val loss 0.4138701186805475 micro_f1_score 0.7447973713033954
 
----------
Epoch 28/40
time = 723.03 secondes

Train loss 0.01467215368319156 micro_f1_score 0.9847185701764414 
 
time = 30.96 secondes

Val loss 0.429845966765138 micro_f1_score 0.741506646971935
 
----------
Epoch 29/40
time = 728.78 secondes

Train loss 0.014713219449113895 micro_f1_score 0.9842189525043836 
 
time = 31.32 secondes

Val loss 0.41786208077043785 micro_f1_score 0.7499999999999999
 
----------
Epoch 30/40
time = 726.90 secondes

Train loss 0.011810814660540174 micro_f1_score 0.9883508451347648 
 
time = 31.62 secondes

Val loss 0.4428071528673172 micro_f1_score 0.7449392712550608
 
----------
Epoch 31/40
time = 728.94 secondes

Train loss 0.011525018282495816 micro_f1_score 0.9883212234184197 
 
time = 32.05 secondes

Val loss 0.4688356169423119 micro_f1_score 0.7466095645967165
 
----------
Epoch 32/40
time = 727.98 secondes

Train loss 0.010062738772227874 micro_f1_score 0.9893390191897654 
 
time = 30.83 secondes

Val loss 0.4884557059553803 micro_f1_score 0.7444523979957052
 
----------
Epoch 33/40
time = 723.28 secondes

Train loss 0.009210721159609232 micro_f1_score 0.9903260207190738 
 
time = 31.24 secondes

Val loss 0.4848633064109771 micro_f1_score 0.7503607503607503
 
----------
Epoch 34/40
time = 723.03 secondes

Train loss 0.00743132512612024 micro_f1_score 0.9924640328842201 
 
time = 31.51 secondes

Val loss 0.4864593172659639 micro_f1_score 0.7562610229276896
 
----------
Epoch 35/40
time = 724.86 secondes

Train loss 0.006265601047322831 micro_f1_score 0.9938346780331863 
 
time = 31.16 secondes

Val loss 0.5167621938420124 micro_f1_score 0.7441696113074204
 
----------
Epoch 36/40
time = 717.78 secondes

Train loss 0.005625582499320641 micro_f1_score 0.9942957103742014 
 
time = 30.41 secondes

Val loss 0.5132132700232209 micro_f1_score 0.7490292975644194
 
----------
Epoch 37/40
time = 722.18 secondes

Train loss 0.005168702146273734 micro_f1_score 0.9950135129991244 
 
time = 30.49 secondes

Val loss 0.5224344891602876 micro_f1_score 0.7516059957173448
 
----------
Epoch 38/40
time = 718.37 secondes

Train loss 0.004401580404606517 micro_f1_score 0.9957793071979925 
 
time = 30.95 secondes

Val loss 0.530449596340539 micro_f1_score 0.7531172069825437
 
----------
Epoch 39/40
time = 721.27 secondes

Train loss 0.004088877162205602 micro_f1_score 0.9963885192929101 
 
time = 30.56 secondes

Val loss 0.511138991987119 micro_f1_score 0.7541800071149057
 
----------
Epoch 40/40
time = 718.08 secondes

Train loss 0.003861534855709865 micro_f1_score 0.9963473099459705 
 
time = 30.49 secondes

Val loss 0.5203768071092543 micro_f1_score 0.7545945945945947
 
----------
best_f1_socre 0.7620087336244542 best_epoch 13

average train time 718.744651567936

average val time 30.406721311807633
 
time = 33.50 secondes

test_f1_score 0.7540521494009866

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_64_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 252.00 MiB (GPU 1; 79.20 GiB total capacity; 69.50 GiB already allocated; 77.31 MiB free; 70.84 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_128_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 1; 79.20 GiB total capacity; 69.71 GiB already allocated; 157.31 MiB free; 70.76 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_64_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 540.00 MiB (GPU 1; 79.20 GiB total capacity; 67.60 GiB already allocated; 491.31 MiB free; 70.44 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_128_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 1008.00 MiB (GPU 1; 79.20 GiB total capacity; 70.35 GiB already allocated; 117.31 MiB free; 70.80 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_256_1
----------
Epoch 1/40
time = 1305.93 secondes

Train loss 0.23526732816889478 micro_f1_score 0.667070952092177 
 
time = 71.47 secondes

Val loss 0.20606710187724378 micro_f1_score 0.6951456310679612
 
----------
Epoch 2/40
time = 1300.19 secondes

Train loss 0.16322092604194138 micro_f1_score 0.7800792580790128 
 
time = 71.12 secondes

Val loss 0.19584343655676137 micro_f1_score 0.7274872198191112
 
----------
Epoch 3/40
time = 1308.19 secondes

Train loss 0.14139240569695158 micro_f1_score 0.8159489375454473 
 
time = 69.98 secondes

Val loss 0.1839171119156431 micro_f1_score 0.7411089866156787
 
----------
Epoch 4/40
time = 1314.97 secondes

Train loss 0.12745516265633408 micro_f1_score 0.8398939588688946 
 
time = 67.68 secondes

Val loss 0.19194785163539355 micro_f1_score 0.7448015122873346
 
----------
Epoch 5/40
time = 1277.37 secondes

Train loss 0.1133392192977103 micro_f1_score 0.8629530683041589 
 
time = 67.63 secondes

Val loss 0.19270809104696648 micro_f1_score 0.7541478129713423
 
----------
Epoch 6/40
time = 1277.64 secondes

Train loss 0.10077206607454935 micro_f1_score 0.8812941083181349 
 
time = 67.62 secondes

Val loss 0.20964271978276675 micro_f1_score 0.7433234421364985
 
----------
Epoch 7/40
time = 1277.05 secondes

Train loss 0.08934343165239772 micro_f1_score 0.8972102750266344 
 
time = 67.66 secondes

Val loss 0.2252380006381723 micro_f1_score 0.7422979340340704
 
----------
Epoch 8/40
time = 1276.39 secondes

Train loss 0.0800280505547988 micro_f1_score 0.9090339261929012 
 
time = 67.58 secondes

Val loss 0.22816724855391707 micro_f1_score 0.7465900933237617
 
----------
Epoch 9/40
time = 1276.01 secondes

Train loss 0.07083073183558546 micro_f1_score 0.9233175614906347 
 
time = 67.64 secondes

Val loss 0.24223888286801634 micro_f1_score 0.7453551912568307
 
----------
Epoch 10/40
time = 1276.28 secondes

Train loss 0.06131914730208951 micro_f1_score 0.9348605267258844 
 
time = 67.70 secondes

Val loss 0.24595553391292446 micro_f1_score 0.749819233550253
 
----------
Epoch 11/40
time = 1275.21 secondes

Train loss 0.05496202261346552 micro_f1_score 0.9441254074189043 
 
time = 67.62 secondes

Val loss 0.2592569916951852 micro_f1_score 0.7587904360056259
 
----------
Epoch 12/40
time = 1277.17 secondes

Train loss 0.04798063223359284 micro_f1_score 0.9508070121244884 
 
time = 67.64 secondes

Val loss 0.2792269598509445 micro_f1_score 0.7428571428571428
 
----------
Epoch 13/40
time = 1277.02 secondes

Train loss 0.04162355997675174 micro_f1_score 0.9574714418030257 
 
time = 67.64 secondes

Val loss 0.28589545324689053 micro_f1_score 0.7538080056677294
 
----------
Epoch 14/40
time = 1277.36 secondes

Train loss 0.037452672597289356 micro_f1_score 0.9627204806285142 
 
time = 67.74 secondes

Val loss 0.29869845118678984 micro_f1_score 0.7520225114315864
 
----------
Epoch 15/40
time = 1278.26 secondes

Train loss 0.03344498646722452 micro_f1_score 0.9666282199154171 
 
time = 67.61 secondes

Val loss 0.30365743793425015 micro_f1_score 0.7515194851626742
 
----------
Epoch 16/40
time = 1278.44 secondes

Train loss 0.02900043921754905 micro_f1_score 0.9722413925312475 
 
time = 67.65 secondes

Val loss 0.30239056429413497 micro_f1_score 0.749819233550253
 
----------
Epoch 17/40
time = 1361.41 secondes

Train loss 0.026565353735350072 micro_f1_score 0.9736499425507469 
 
time = 77.64 secondes

Val loss 0.2982950575771879 micro_f1_score 0.7522189349112426
 
----------
Epoch 18/40
time = 1319.10 secondes

Train loss 0.023290452159844707 micro_f1_score 0.9781362281171165 
 
time = 67.66 secondes

Val loss 0.31139955303219496 micro_f1_score 0.747896084888401
 
----------
Epoch 19/40
time = 1339.52 secondes

Train loss 0.021770752125591677 micro_f1_score 0.9789525955918866 
 
time = 67.57 secondes

Val loss 0.3388934291777064 micro_f1_score 0.7425063199711087
 
----------
Epoch 20/40
time = 2373.14 secondes

Train loss 0.018168047709214326 micro_f1_score 0.9818237360623187 
 
time = 137.33 secondes

Val loss 0.3452908084040783 micro_f1_score 0.7531622696060716
 
----------
Epoch 21/40
time = 2138.02 secondes

Train loss 0.016691907804127916 micro_f1_score 0.9828929280586527 
 
time = 111.62 secondes

Val loss 0.35281780777407473 micro_f1_score 0.7499999999999999
 
----------
Epoch 22/40
time = 2045.24 secondes

Train loss 0.01550072341704288 micro_f1_score 0.9849004804392587 
 
time = 111.59 secondes

Val loss 0.36027909546601966 micro_f1_score 0.7531622696060716
 
----------
Epoch 23/40
time = 2044.20 secondes

Train loss 0.014081834941401843 micro_f1_score 0.9855017169019458 
 
time = 111.70 secondes

Val loss 0.35873269497371113 micro_f1_score 0.7527352297592997
 
----------
Epoch 24/40
time = 2043.98 secondes

Train loss 0.011986393625438968 micro_f1_score 0.9876505564872694 
 
time = 111.61 secondes

Val loss 0.3706738150022069 micro_f1_score 0.7481962481962482
 
----------
Epoch 25/40
time = 2042.97 secondes

Train loss 0.011003124770794383 micro_f1_score 0.9891006097560976 
 
time = 111.68 secondes

Val loss 0.38559199649779524 micro_f1_score 0.7423741271591328
 
----------
Epoch 26/40
time = 2057.90 secondes

Train loss 0.009496604933414969 micro_f1_score 0.990408769125371 
 
time = 111.58 secondes

Val loss 0.3877367589805947 micro_f1_score 0.7487141807494488
 
----------
Epoch 27/40
time = 2057.08 secondes

Train loss 0.009410503555193066 micro_f1_score 0.9906058646788118 
 
time = 114.95 secondes

Val loss 0.39067056994946275 micro_f1_score 0.7514367816091954
 
----------
Epoch 28/40
time = 2070.09 secondes

Train loss 0.008725046035193897 micro_f1_score 0.9907495527047091 
 
time = 113.44 secondes

Val loss 0.4168577072073202 micro_f1_score 0.7423643550125764
 
----------
Epoch 29/40
time = 2068.14 secondes

Train loss 0.008273410020374614 micro_f1_score 0.991480946223473 
 
time = 113.94 secondes

Val loss 0.40472800931969627 micro_f1_score 0.7488151658767772
 
----------
Epoch 30/40
time = 2068.44 secondes

Train loss 0.006601155448237199 micro_f1_score 0.9931900323378353 
 
time = 115.19 secondes

Val loss 0.443072646856308 micro_f1_score 0.7361963190184048
 
----------
Epoch 31/40
time = 2044.65 secondes

Train loss 0.005881692876639201 micro_f1_score 0.994298312300441 
 
time = 111.62 secondes

Val loss 0.4234748643929841 micro_f1_score 0.7532188841201717
 
----------
Epoch 32/40
time = 2041.93 secondes

Train loss 0.005325970588186924 micro_f1_score 0.9948667249705312 
 
time = 111.57 secondes

Val loss 0.42696361720073417 micro_f1_score 0.746031746031746
 
----------
Epoch 33/40
time = 2043.98 secondes

Train loss 0.004885901902751947 micro_f1_score 0.9950192007908445 
 
time = 111.57 secondes

Val loss 0.4203515849152549 micro_f1_score 0.7568740955137483
 
----------
Epoch 34/40
time = 2042.65 secondes

Train loss 0.004514102950837172 micro_f1_score 0.9956673761021587 
 
time = 111.67 secondes

Val loss 0.430829185931409 micro_f1_score 0.7521054558769681
 
----------
Epoch 35/40
time = 2043.57 secondes

Train loss 0.003514032681060426 micro_f1_score 0.9971496978679739 
 
time = 111.66 secondes

Val loss 0.42125124130092684 micro_f1_score 0.7584453323646931
 
----------
Epoch 36/40
time = 2043.45 secondes

Train loss 0.0037567147937249855 micro_f1_score 0.9961240310077519 
 
time = 111.61 secondes

Val loss 0.43685019211690934 micro_f1_score 0.7595392368610512
 
----------
Epoch 37/40
time = 2044.70 secondes

Train loss 0.0024060966644757573 micro_f1_score 0.9976425855513308 
 
time = 112.00 secondes

Val loss 0.42474633687343755 micro_f1_score 0.760144927536232
 
----------
Epoch 38/40
time = 2047.48 secondes

Train loss 0.002428176524594787 micro_f1_score 0.9977210574293529 
 
time = 112.04 secondes

Val loss 0.42897241953455034 micro_f1_score 0.7644703312704769
 
----------
Epoch 39/40
time = 2047.51 secondes

Train loss 0.0020078397929594485 micro_f1_score 0.9984048613748576 
 
time = 111.90 secondes

Val loss 0.4319130474182426 micro_f1_score 0.7655571635311145
 
----------
Epoch 40/40
time = 2045.84 secondes

Train loss 0.0011147823182368853 micro_f1_score 0.9991642607506458 
 
time = 111.63 secondes

Val loss 0.447067526764557 micro_f1_score 0.7547169811320754
 
----------
best_f1_socre 0.7655571635311145 best_epoch 39

average train time 1700.7120189964771

average val time 92.16919352412224
 
time = 114.75 secondes

test_f1_score 0.7466478475652788

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_512_1
----------
Epoch 1/40
time = 2596.52 secondes

Train loss 0.2363037453175665 micro_f1_score 0.6609075043630017 
 
time = 125.51 secondes

Val loss 0.20118295218123763 micro_f1_score 0.7119456652017578
 
----------
Epoch 2/40
time = 2626.91 secondes

Train loss 0.16322361046599376 micro_f1_score 0.7819333742080523 
 
time = 130.22 secondes

Val loss 0.18724405765533447 micro_f1_score 0.7318982387475538
 
----------
Epoch 3/40
time = 2671.89 secondes

Train loss 0.14116224167314737 micro_f1_score 0.8167205169628433 
 
time = 131.71 secondes

Val loss 0.19006939293419728 micro_f1_score 0.7398095238095238
 
----------
Epoch 4/40
time = 2674.34 secondes

Train loss 0.12389018651232257 micro_f1_score 0.8467018892090938 
 
time = 132.20 secondes

Val loss 0.19162800126388424 micro_f1_score 0.7498144023756496
 
----------
Epoch 5/40
time = 2675.20 secondes

Train loss 0.10894957195866753 micro_f1_score 0.8708384587913177 
 
time = 131.98 secondes

Val loss 0.18821529372305165 micro_f1_score 0.7573253193087904
 
----------
Epoch 6/40
time = 2685.82 secondes

Train loss 0.09750085842065714 micro_f1_score 0.8843978643464506 
 
time = 129.74 secondes

Val loss 0.19876127873287827 micro_f1_score 0.7555066079295154
 
----------
Epoch 7/40
time = 2669.80 secondes

Train loss 0.08557999025325518 micro_f1_score 0.9046797836481932 
 
time = 132.98 secondes

Val loss 0.21085514604556757 micro_f1_score 0.7538738738738738
 
----------
Epoch 8/40
time = 2670.73 secondes

Train loss 0.07508876615933872 micro_f1_score 0.9176139692956756 
 
time = 134.20 secondes

Val loss 0.21547672628867823 micro_f1_score 0.7574007220216606
 
----------
Epoch 9/40
time = 2693.05 secondes

Train loss 0.06572651372853297 micro_f1_score 0.9307261701135127 
 
time = 132.71 secondes

Val loss 0.23174037210276868 micro_f1_score 0.7521551724137931
 
----------
Epoch 10/40
time = 2686.57 secondes

Train loss 0.05739611196877049 micro_f1_score 0.9410485436893203 
 
time = 130.74 secondes

Val loss 0.23930167724363138 micro_f1_score 0.7579710144927536
 
----------
Epoch 11/40
time = 2658.24 secondes

Train loss 0.051392058314973704 micro_f1_score 0.9476982591876209 
 
time = 129.73 secondes

Val loss 0.24756714054307 micro_f1_score 0.7532005689900426
 
----------
Epoch 12/40
time = 2630.02 secondes

Train loss 0.04404652084534367 micro_f1_score 0.9568799568799569 
 
time = 128.77 secondes

Val loss 0.25876277214923843 micro_f1_score 0.7611453425154041
 
----------
Epoch 13/40
time = 2694.54 secondes

Train loss 0.038880950386577226 micro_f1_score 0.9612694997310383 
 
time = 135.34 secondes

Val loss 0.28868134270925994 micro_f1_score 0.7424080028581636
 
----------
Epoch 14/40
time = 2986.37 secondes

Train loss 0.03350986240217714 micro_f1_score 0.9667919318966178 
 
time = 154.59 secondes

Val loss 0.28781587386229 micro_f1_score 0.7551519385260217
 
----------
Epoch 15/40
time = 3137.62 secondes

Train loss 0.030592218914639775 micro_f1_score 0.9693044644567926 
 
time = 156.11 secondes

Val loss 0.3115898751821674 micro_f1_score 0.7437609841827768
 
----------
Epoch 16/40
time = 2734.53 secondes

Train loss 0.026959628259620488 micro_f1_score 0.9731571893547769 
 
time = 153.64 secondes

Val loss 0.3042312851939045 micro_f1_score 0.7540069686411149
 
----------
Epoch 17/40
time = 3111.62 secondes

Train loss 0.02407024596505253 micro_f1_score 0.9756228029955677 
 
time = 155.29 secondes

Val loss 0.3223592436215917 micro_f1_score 0.7536945812807883
 
----------
Epoch 18/40
time = 3128.38 secondes

Train loss 0.0221588852250034 micro_f1_score 0.97807705551517 
 
time = 156.46 secondes

Val loss 0.32683169035637966 micro_f1_score 0.7543424317617867
 
----------
Epoch 19/40
time = 3137.41 secondes

Train loss 0.018490880750421736 micro_f1_score 0.9818957960132636 
 
time = 156.44 secondes

Val loss 0.3200327534167493 micro_f1_score 0.7599001070281841
 
----------
Epoch 20/40
time = 3139.26 secondes

Train loss 0.017043585350017508 micro_f1_score 0.9829434883809669 
 
time = 157.75 secondes

Val loss 0.33772655539825314 micro_f1_score 0.7614942528735631
 
----------
Epoch 21/40
time = 2870.26 secondes

Train loss 0.016211189888050644 micro_f1_score 0.9839417172063927 
 
time = 127.85 secondes

Val loss 0.3573837423178016 micro_f1_score 0.7510948905109489
 
----------
Epoch 22/40
time = 2622.35 secondes

Train loss 0.015120665372248638 micro_f1_score 0.9851634310995843 
 
time = 128.91 secondes

Val loss 0.3609097073556947 micro_f1_score 0.7553743513713861
 
----------
Epoch 23/40
time = 2623.46 secondes

Train loss 0.013205127187465478 micro_f1_score 0.9858715107201341 
 
time = 128.42 secondes

Val loss 0.35013758452212224 micro_f1_score 0.7660668380462724
 
----------
Epoch 24/40
time = 2627.11 secondes

Train loss 0.011226232206739427 micro_f1_score 0.9892186369004534 
 
time = 128.31 secondes

Val loss 0.34835617977087613 micro_f1_score 0.7710583153347732
 
----------
Epoch 25/40
time = 2609.08 secondes

Train loss 0.01098339875348789 micro_f1_score 0.9891391334171716 
 
time = 126.41 secondes

Val loss 0.37992256049249995 micro_f1_score 0.7523533671252715
 
----------
Epoch 26/40
time = 2607.62 secondes

Train loss 0.009377040598816281 micro_f1_score 0.9912805087004531 
 
time = 126.51 secondes

Val loss 0.38272341230853657 micro_f1_score 0.7528653295128941
 
----------
Epoch 27/40
time = 2609.07 secondes

Train loss 0.008704520246330732 micro_f1_score 0.9909039010466222 
 
time = 126.51 secondes

Val loss 0.3935484005535235 micro_f1_score 0.7554915376305366
 
----------
Epoch 28/40
time = 2650.60 secondes

Train loss 0.00852271299533911 micro_f1_score 0.9922740247383445 
 
time = 133.01 secondes

Val loss 0.3976423444806552 micro_f1_score 0.7549715909090909
 
----------
Epoch 29/40
time = 2667.60 secondes

Train loss 0.008055082638262927 micro_f1_score 0.9923855935429834 
 
time = 127.67 secondes

Val loss 0.39448484497480707 micro_f1_score 0.7574437182280319
 
----------
Epoch 30/40
time = 2614.76 secondes

Train loss 0.006925393286400248 micro_f1_score 0.9931113225499524 
 
time = 127.27 secondes

Val loss 0.3979289013831342 micro_f1_score 0.7570703408266861
 
----------
Epoch 31/40
time = 2609.73 secondes

Train loss 0.006335357465345501 micro_f1_score 0.9941875925996277 
 
time = 127.82 secondes

Val loss 0.4191802744982672 micro_f1_score 0.7542281396185677
 
----------
Epoch 32/40
time = 2614.17 secondes

Train loss 0.006458582652513534 micro_f1_score 0.9940711462450592 
 
time = 128.52 secondes

Val loss 0.40984233737480447 micro_f1_score 0.7560087399854334
 
----------
Epoch 33/40
time = 2618.71 secondes

Train loss 0.004964846811364279 micro_f1_score 0.9953219488076676 
 
time = 129.14 secondes

Val loss 0.41398610528863844 micro_f1_score 0.7583723442563918
 
----------
Epoch 34/40
time = 2613.14 secondes

Train loss 0.004114989988201168 micro_f1_score 0.9964632059326869 
 
time = 127.74 secondes

Val loss 0.4246144732002352 micro_f1_score 0.7562296858071506
 
----------
Epoch 35/40
time = 2614.26 secondes

Train loss 0.0037546916877856658 micro_f1_score 0.9964648192496294 
 
time = 127.64 secondes

Val loss 0.43190916383364164 micro_f1_score 0.7553342816500711
 
----------
Epoch 36/40
time = 2614.20 secondes

Train loss 0.0036155445336155502 micro_f1_score 0.9970353477765108 
 
time = 127.01 secondes

Val loss 0.438643853928222 micro_f1_score 0.7641643059490085
 
----------
Epoch 37/40
time = 2606.99 secondes

Train loss 0.002522118954751735 micro_f1_score 0.9977198449494565 
 
time = 126.27 secondes

Val loss 0.4388839613463058 micro_f1_score 0.7625394598386532
 
----------
Epoch 38/40
time = 2602.99 secondes

Train loss 0.001840759811583101 micro_f1_score 0.9988221436984688 
 
time = 126.80 secondes

Val loss 0.43739768227592846 micro_f1_score 0.7616011335458731
 
----------
Epoch 39/40
time = 2599.41 secondes

Train loss 0.0020228839646537185 micro_f1_score 0.9982902085945514 
 
time = 125.93 secondes

Val loss 0.4402136388616484 micro_f1_score 0.7615658362989324
 
----------
Epoch 40/40
time = 2607.24 secondes

Train loss 0.0014100572415315525 micro_f1_score 0.9988221436984688 
 
time = 126.66 secondes

Val loss 0.4517062783974116 micro_f1_score 0.7571273908336341
 
----------
best_f1_socre 0.7710583153347732 best_epoch 24

average train time 2715.2891274273397

average val time 133.76227564811705
 
time = 131.63 secondes

test_f1_score 0.7650771388499299

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_256_1
----------
Epoch 1/40
time = 3098.32 secondes

Train loss 0.22437530157936586 micro_f1_score 0.693574889403834 
 
time = 138.16 secondes

Val loss 0.19459027125210057 micro_f1_score 0.7161904761904763
 
----------
Epoch 2/40
time = 3101.10 secondes

Train loss 0.15138667444992174 micro_f1_score 0.8011025090186858 
 
time = 138.70 secondes

Val loss 0.17447984914799206 micro_f1_score 0.7628012048192772
 
----------
Epoch 3/40
time = 3096.38 secondes

Train loss 0.12781350478060075 micro_f1_score 0.8414100421739475 
 
time = 139.63 secondes

Val loss 0.16992495441045918 micro_f1_score 0.7709580838323352
 
----------
Epoch 4/40
time = 3185.26 secondes

Train loss 0.11379080342447705 micro_f1_score 0.8607514748386585 
 
time = 152.47 secondes

Val loss 0.17142946957076183 micro_f1_score 0.770153730783652
 
----------
Epoch 5/40
time = 3098.45 secondes

Train loss 0.10096520972144496 micro_f1_score 0.8807123524104656 
 
time = 141.97 secondes

Val loss 0.1859366497421851 micro_f1_score 0.7656423546834507
 
----------
Epoch 6/40
time = 3105.53 secondes

Train loss 0.08942859775747533 micro_f1_score 0.8963431399505086 
 
time = 139.22 secondes

Val loss 0.19195797569194778 micro_f1_score 0.7706489675516224
 
----------
Epoch 7/40
time = 3126.09 secondes

Train loss 0.07892535471164429 micro_f1_score 0.9125097732603596 
 
time = 141.72 secondes

Val loss 0.19676498201538306 micro_f1_score 0.7749453750910416
 
----------
Epoch 8/40
time = 3136.38 secondes

Train loss 0.07000643235656458 micro_f1_score 0.9231726767028435 
 
time = 142.08 secondes

Val loss 0.20814208191682082 micro_f1_score 0.7600297176820209
 
----------
Epoch 9/40
time = 3079.62 secondes

Train loss 0.0630883288074721 micro_f1_score 0.9325100963031997 
 
time = 137.03 secondes

Val loss 0.2115053177246305 micro_f1_score 0.777056277056277
 
----------
Epoch 10/40
time = 3070.75 secondes

Train loss 0.054187630903110046 micro_f1_score 0.94393751933189 
 
time = 137.00 secondes

Val loss 0.20878798667280402 micro_f1_score 0.7893972403776326
 
----------
Epoch 11/40
time = 3072.62 secondes

Train loss 0.046540183181295526 micro_f1_score 0.9530382479950649 
 
time = 137.07 secondes

Val loss 0.2355352391595723 micro_f1_score 0.7769284225156359
 
----------
Epoch 12/40
time = 3072.29 secondes

Train loss 0.04259873867865551 micro_f1_score 0.9572491422844147 
 
time = 137.09 secondes

Val loss 0.24922891315378126 micro_f1_score 0.7653454928095405
 
----------
Epoch 13/40
time = 3073.48 secondes

Train loss 0.037720280078969694 micro_f1_score 0.9618179720844389 
 
time = 137.01 secondes

Val loss 0.25070803777360523 micro_f1_score 0.770693906305037
 
----------
Epoch 14/40
time = 3072.41 secondes

Train loss 0.0333446326543921 micro_f1_score 0.9667458432304038 
 
time = 137.01 secondes

Val loss 0.2583584237660541 micro_f1_score 0.7771469127040453
 
----------
Epoch 15/40
time = 3072.31 secondes

Train loss 0.029444178840069956 micro_f1_score 0.9704541099827553 
 
time = 137.16 secondes

Val loss 0.2707461947422536 micro_f1_score 0.7758931793576327
 
----------
Epoch 16/40
time = 3070.65 secondes

Train loss 0.02680554166771807 micro_f1_score 0.9732173646499865 
 
time = 137.13 secondes

Val loss 0.28557477366240297 micro_f1_score 0.7711442786069651
 
----------
Epoch 17/40
time = 3072.41 secondes

Train loss 0.022970034025173197 micro_f1_score 0.9774292272379494 
 
time = 136.79 secondes

Val loss 0.28882306569912397 micro_f1_score 0.7713675213675214
 
----------
Epoch 18/40
time = 3071.25 secondes

Train loss 0.02215137508485953 micro_f1_score 0.9775298073983492 
 
time = 136.91 secondes

Val loss 0.29643311449250237 micro_f1_score 0.7812610073969708
 
----------
Epoch 19/40
time = 3070.34 secondes

Train loss 0.019389354355050133 micro_f1_score 0.9801031124689707 
 
time = 136.94 secondes

Val loss 0.31313273738153646 micro_f1_score 0.7685282753775905
 
----------
Epoch 20/40
time = 3069.08 secondes

Train loss 0.01718033181591513 micro_f1_score 0.9830159154230755 
 
time = 136.92 secondes

Val loss 0.3131682309703749 micro_f1_score 0.7725968436154951
 
----------
Epoch 21/40
time = 3071.07 secondes

Train loss 0.015696184647579988 micro_f1_score 0.9841693686820522 
 
time = 137.05 secondes

Val loss 0.3235643848654677 micro_f1_score 0.7676840215439856
 
----------
Epoch 22/40
time = 3071.00 secondes

Train loss 0.015230908711715643 micro_f1_score 0.9848594637885665 
 
time = 136.97 secondes

Val loss 0.317523899381278 micro_f1_score 0.7692307692307693
 
----------
Epoch 23/40
time = 3071.14 secondes

Train loss 0.013039372295302732 micro_f1_score 0.9859916254282453 
 
time = 137.03 secondes

Val loss 0.32425092270628353 micro_f1_score 0.7746427262733604
 
----------
Epoch 24/40
time = 3073.18 secondes

Train loss 0.012011284779087087 micro_f1_score 0.9875404839016956 
 
time = 136.87 secondes

Val loss 0.33735126159230217 micro_f1_score 0.7733531819873464
 
----------
Epoch 25/40
time = 3072.09 secondes

Train loss 0.01122890977309031 micro_f1_score 0.987938971959061 
 
time = 137.10 secondes

Val loss 0.34500192991289935 micro_f1_score 0.7708104143747709
 
----------
Epoch 26/40
time = 3071.15 secondes

Train loss 0.00885457143010278 micro_f1_score 0.9905160921729195 
 
time = 136.94 secondes

Val loss 0.3669190301758344 micro_f1_score 0.7729789590254708
 
----------
Epoch 27/40
time = 3070.68 secondes

Train loss 0.009841618043496923 micro_f1_score 0.9901691815272062 
 
time = 136.93 secondes

Val loss 0.3537699238733068 micro_f1_score 0.7814227792112054
 
----------
Epoch 28/40
time = 3071.16 secondes

Train loss 0.008059482857205293 micro_f1_score 0.9913573196268799 
 
time = 137.14 secondes

Val loss 0.37105664580327563 micro_f1_score 0.7715549005158437
 
----------
Epoch 29/40
time = 3070.74 secondes

Train loss 0.007692236929155492 micro_f1_score 0.992436048500513 
 
time = 136.91 secondes

Val loss 0.369153051347029 micro_f1_score 0.7771679473106476
 
----------
Epoch 30/40
time = 3072.27 secondes

Train loss 0.0073774832848439555 micro_f1_score 0.9928128683880291 
 
time = 136.67 secondes

Val loss 0.36255303428309865 micro_f1_score 0.7807211184694629
 
----------
Epoch 31/40
time = 3068.66 secondes

Train loss 0.005528608528251638 micro_f1_score 0.9947504564820451 
 
time = 136.73 secondes

Val loss 0.38422782047361626 micro_f1_score 0.773577981651376
 
----------
Epoch 32/40
time = 3068.63 secondes

Train loss 0.005655473345475846 micro_f1_score 0.9942939744370054 
 
time = 136.48 secondes

Val loss 0.3885471451966489 micro_f1_score 0.777254617892068
 
----------
Epoch 33/40
time = 3067.51 secondes

Train loss 0.004588250970721732 micro_f1_score 0.9953608639440262 
 
time = 136.45 secondes

Val loss 0.40141951091221123 micro_f1_score 0.7684587813620072
 
----------
Epoch 34/40
time = 3067.95 secondes

Train loss 0.004695274832323907 micro_f1_score 0.9953201689304874 
 
time = 137.03 secondes

Val loss 0.4059839411715015 micro_f1_score 0.7672819399203765
 
----------
Epoch 35/40
time = 3069.50 secondes

Train loss 0.003961529260558162 micro_f1_score 0.9963506424389873 
 
time = 136.98 secondes

Val loss 0.39322246232482255 micro_f1_score 0.7757009345794392
 
----------
Epoch 36/40
time = 3068.93 secondes

Train loss 0.0034413087383114004 micro_f1_score 0.9967310323855862 
 
time = 136.93 secondes

Val loss 0.3895134120324596 micro_f1_score 0.7837445573294629
 
----------
Epoch 37/40
time = 3069.38 secondes

Train loss 0.002817877054084428 micro_f1_score 0.9970355731225297 
 
time = 136.94 secondes

Val loss 0.3940821227724435 micro_f1_score 0.7761521580102414
 
----------
Epoch 38/40
time = 3068.65 secondes

Train loss 0.0018880376339193144 micro_f1_score 0.9980617945502223 
 
time = 136.74 secondes

Val loss 0.4023083136340634 micro_f1_score 0.777209642074507
 
----------
Epoch 39/40
time = 3069.44 secondes

Train loss 0.0019097286858727005 micro_f1_score 0.9982131315819489 
 
time = 136.97 secondes

Val loss 0.39907441379838304 micro_f1_score 0.7774566473988439
 
----------
Epoch 40/40
time = 3070.73 secondes

Train loss 0.0015374300738511847 micro_f1_score 0.9983277591973243 
 
time = 137.22 secondes

Val loss 0.40680825917935765 micro_f1_score 0.7775781530722242
 
----------
best_f1_socre 0.7893972403776326 best_epoch 10

average train time 3080.464173412323

average val time 137.90237558484077
 
time = 140.93 secondes

test_f1_score 0.7717813051146384

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_512_1
----------
Epoch 1/40
time = 1588.13 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Train loss 0.21652178957320012 micro_f1_score 0.7079875953398711 
 
time = 69.28 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss 0.1792016061114483 micro_f1_score 0.7539432176656152
 
----------
Epoch 2/40
time = 1597.87 secondes

Train loss 0.14943235187313042 micro_f1_score 0.8036770065602981 
 
time = 69.05 secondes

Val loss 0.17164064455227773 micro_f1_score 0.7655068078668683
 
----------
Epoch 3/40
time = 1609.58 secondes

Train loss 0.127059248051195 micro_f1_score 0.8416683296747155 
 
time = 63.03 secondes

Val loss 0.17023088831882008 micro_f1_score 0.7758167480285393
 
----------
Epoch 4/40
time = 1606.43 secondes

Train loss 0.10987334326748643 micro_f1_score 0.868307375262553 
 
time = 60.60 secondes

Val loss 0.17482994954849854 micro_f1_score 0.7772828507795101
 
----------
Epoch 5/40
time = 1606.56 secondes

Train loss 0.09684010140664942 micro_f1_score 0.8867382666509305 
 
time = 68.59 secondes

Val loss 0.18358798074673432 micro_f1_score 0.7782608695652173
 
----------
Epoch 6/40
time = 1612.21 secondes

Train loss 0.08537730962742825 micro_f1_score 0.9048794826572604 
 
time = 64.04 secondes

Val loss 0.18638427280744568 micro_f1_score 0.7868259138617444
 
----------
Epoch 7/40
time = 1589.52 secondes

Train loss 0.07507526977742845 micro_f1_score 0.918842955849631 
 
time = 66.09 secondes

Val loss 0.19506619620274324 micro_f1_score 0.7868038311457963
 
----------
Epoch 8/40
time = 1584.09 secondes

Train loss 0.065761952179375 micro_f1_score 0.9302379750766722 
 
time = 64.80 secondes

Val loss 0.21078215612739812 micro_f1_score 0.7843971631205674
 
----------
Epoch 9/40
time = 1591.30 secondes

Train loss 0.057811619878352226 micro_f1_score 0.9400579710144928 
 
time = 65.69 secondes

Val loss 0.21841859573223552 micro_f1_score 0.7836134453781511
 
----------
Epoch 10/40
time = 1588.47 secondes

Train loss 0.051795938777997416 micro_f1_score 0.9456458663778096 
 
time = 63.17 secondes

Val loss 0.21333756383325234 micro_f1_score 0.7914893617021278
 
----------
Epoch 11/40
time = 1589.06 secondes

Train loss 0.04391199799657271 micro_f1_score 0.9564113415150234 
 
time = 66.23 secondes

Val loss 0.23037435260952496 micro_f1_score 0.7853366231935142
 
----------
Epoch 12/40
time = 1588.30 secondes

Train loss 0.03992050848775417 micro_f1_score 0.9600491702519975 
 
time = 65.74 secondes

Val loss 0.2418427394061792 micro_f1_score 0.7797091167080524
 
----------
Epoch 13/40
time = 1591.03 secondes

Train loss 0.03414510232438375 micro_f1_score 0.9655463474524247 
 
time = 64.13 secondes

Val loss 0.25107724818049887 micro_f1_score 0.7880553532410779
 
----------
Epoch 14/40
time = 1589.34 secondes

Train loss 0.03036911380057735 micro_f1_score 0.969546063972419 
 
time = 65.75 secondes

Val loss 0.26100102066993713 micro_f1_score 0.7831623629288998
 
----------
Epoch 15/40
time = 1592.83 secondes

Train loss 0.027154075360106018 micro_f1_score 0.973420145538108 
 
time = 64.83 secondes

Val loss 0.2705005068698379 micro_f1_score 0.7825167430384209
 
----------
Epoch 16/40
time = 1589.39 secondes

Train loss 0.024054616807460752 micro_f1_score 0.9774890120389833 
 
time = 61.91 secondes

Val loss 0.28479454037351687 micro_f1_score 0.7823298895618099
 
----------
Epoch 17/40
time = 1607.20 secondes

Train loss 0.021788670360488264 micro_f1_score 0.9776604697421772 
 
time = 64.11 secondes

Val loss 0.27366666004183837 micro_f1_score 0.7971066907775769
 
----------
Epoch 18/40
time = 1569.45 secondes

Train loss 0.01959632414273565 micro_f1_score 0.9811810512654121 
 
time = 59.49 secondes

Val loss 0.30963670657794984 micro_f1_score 0.7780918727915195
 
----------
Epoch 19/40
time = 1566.82 secondes

Train loss 0.016753919301386755 micro_f1_score 0.9837317650653021 
 
time = 59.28 secondes

Val loss 0.29163609909229593 micro_f1_score 0.7900612171407994
 
----------
Epoch 20/40
time = 1568.34 secondes

Train loss 0.01542030404692071 micro_f1_score 0.9841548623572983 
 
time = 63.70 secondes

Val loss 0.3154261489627791 micro_f1_score 0.7776591960156527
 
----------
Epoch 21/40
time = 1564.99 secondes

Train loss 0.01477935866642584 micro_f1_score 0.9851600350970893 
 
time = 58.11 secondes

Val loss 0.31272833427933394 micro_f1_score 0.7899280575539569
 
----------
Epoch 22/40
time = 1571.22 secondes

Train loss 0.012921451040656885 micro_f1_score 0.9871785087384569 
 
time = 54.89 secondes

Val loss 0.32730391707088125 micro_f1_score 0.7834167262330237
 
----------
Epoch 23/40
time = 1562.14 secondes

Train loss 0.012577359191365415 micro_f1_score 0.9874690535136166 
 
time = 53.29 secondes

Val loss 0.3433397800096723 micro_f1_score 0.7737540337038366
 
----------
Epoch 24/40
time = 1557.71 secondes

Train loss 0.011644646139403277 micro_f1_score 0.9883123310617886 
 
time = 53.75 secondes

Val loss 0.3355106895820039 micro_f1_score 0.7827648114901257
 
----------
Epoch 25/40
time = 1556.70 secondes

Train loss 0.010605679136367414 micro_f1_score 0.9889113287352818 
 
time = 53.38 secondes

Val loss 0.34323428015484186 micro_f1_score 0.7849656667871341
 
----------
Epoch 26/40
time = 1565.99 secondes

Train loss 0.008984602222492843 micro_f1_score 0.9901714285714286 
 
time = 58.53 secondes

Val loss 0.33978578930751224 micro_f1_score 0.7858960378044348
 
----------
Epoch 27/40
time = 1561.73 secondes

Train loss 0.009286113891570707 micro_f1_score 0.9910972454725308 
 
time = 58.90 secondes

Val loss 0.344313096987908 micro_f1_score 0.7867298578199052
 
----------
Epoch 28/40
time = 1561.52 secondes

Train loss 0.007091936601094088 micro_f1_score 0.992852798053528 
 
time = 57.55 secondes

Val loss 0.3600048790456819 micro_f1_score 0.781354051054384
 
----------
Epoch 29/40
time = 1561.93 secondes

Train loss 0.007194435171694348 micro_f1_score 0.9931512061486949 
 
time = 57.59 secondes

Val loss 0.34449823203756186 micro_f1_score 0.7847067557535263
 
----------
Epoch 30/40
time = 1563.55 secondes

Train loss 0.006175435304474967 micro_f1_score 0.9937671024627546 
 
time = 59.59 secondes

Val loss 0.3596931995182741 micro_f1_score 0.7824207492795389
 
----------
Epoch 31/40
time = 1573.23 secondes

Train loss 0.005601588436816651 micro_f1_score 0.9941858255747673 
 
time = 60.43 secondes

Val loss 0.37551088105948244 micro_f1_score 0.7782214156079855
 
----------
Epoch 32/40
time = 1575.43 secondes

Train loss 0.006196620106460581 micro_f1_score 0.9938384299406663 
 
time = 59.11 secondes

Val loss 0.3697654155434155 micro_f1_score 0.7839884601514605
 
----------
Epoch 33/40
time = 1583.40 secondes

Train loss 0.004948411725090737 micro_f1_score 0.9953994144709327 
 
time = 64.15 secondes

Val loss 0.3792766794318058 micro_f1_score 0.7825454545454545
 
----------
Epoch 34/40
time = 1588.62 secondes

Train loss 0.004026966173488118 micro_f1_score 0.9960832034072327 
 
time = 68.84 secondes

Val loss 0.3786445008438142 micro_f1_score 0.7832973362131029
 
----------
Epoch 35/40
time = 1611.96 secondes

Train loss 0.0034979459525939175 micro_f1_score 0.9970761344218722 
 
time = 68.11 secondes

Val loss 0.37924328556314846 micro_f1_score 0.7868734222863324
 
----------
Epoch 36/40
time = 1610.53 secondes

Train loss 0.003089022154043804 micro_f1_score 0.9972259167775034 
 
time = 68.44 secondes

Val loss 0.3851352741972345 micro_f1_score 0.7893428063943161
 
----------
Epoch 37/40
time = 1571.97 secondes

Train loss 0.00313660311755181 micro_f1_score 0.9976066557763174 
 
time = 56.05 secondes

Val loss 0.3925606306703364 micro_f1_score 0.7828264758497318
 
----------
Epoch 38/40
time = 1558.89 secondes

Train loss 0.0020136785339960003 micro_f1_score 0.9983291562238932 
 
time = 58.29 secondes

Val loss 0.3866258542679372 micro_f1_score 0.7861566484517304
 
----------
Epoch 39/40
time = 1568.68 secondes

Train loss 0.0021372952428685716 micro_f1_score 0.9981754599361411 
 
time = 58.74 secondes

Val loss 0.39338509115527887 micro_f1_score 0.7841726618705036
 
----------
Epoch 40/40
time = 1574.80 secondes

Train loss 0.001482628532874121 micro_f1_score 0.998594385138472 
 
time = 59.44 secondes

Val loss 0.3978093659780065 micro_f1_score 0.7871572871572871
 
----------
best_f1_socre 0.7971066907775769 best_epoch 17

average train time 1581.772994363308

average val time 61.91711419224739
 
time = 62.92 secondes

test_f1_score 0.7837932238910233

----------

Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_256_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 774.00 MiB (GPU 1; 79.20 GiB total capacity; 61.26 GiB already allocated; 111.31 MiB free; 64.08 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_512_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 1.51 GiB (GPU 1; 79.20 GiB total capacity; 60.11 GiB already allocated; 907.31 MiB free; 63.31 GiB reserved in total by PyTorch)
datasets imported
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_64_2
----------
Epoch 1/40
time = 1576.42 secondes

Train loss 0.26038070746638753 micro_f1_score 0.6366005565616856 
 
time = 49.64 secondes

Val loss 0.20460394354628736 micro_f1_score 0.6943883730318934
 
----------
Epoch 2/40
time = 1578.19 secondes

Train loss 0.16927425500240412 micro_f1_score 0.7702452091489799 
 
time = 49.92 secondes

Val loss 0.19439527771023454 micro_f1_score 0.7180685358255451
 
----------
Epoch 3/40
time = 1577.74 secondes

Train loss 0.14237836865825695 micro_f1_score 0.8157018754566858 
 
time = 49.97 secondes

Val loss 0.1912215680616801 micro_f1_score 0.741074783915821
 
----------
Epoch 4/40
time = 1578.67 secondes

Train loss 0.12527214882408713 micro_f1_score 0.8427354548376728 
 
time = 49.94 secondes

Val loss 0.20454249506602523 micro_f1_score 0.7413333333333334
 
----------
Epoch 5/40
time = 1577.76 secondes

Train loss 0.10887373186614331 micro_f1_score 0.866865671641791 
 
time = 49.92 secondes

Val loss 0.1983530699718194 micro_f1_score 0.7536231884057971
 
----------
Epoch 6/40
time = 1578.00 secondes

Train loss 0.09539877698412752 micro_f1_score 0.8881401617250673 
 
time = 50.10 secondes

Val loss 0.2034358646048874 micro_f1_score 0.7760088855979268
 
----------
Epoch 7/40
time = 1576.06 secondes

Train loss 0.08153067190485361 micro_f1_score 0.9066698183823818 
 
time = 50.02 secondes

Val loss 0.23338517546653748 micro_f1_score 0.7522092612230471
 
----------
Epoch 8/40
time = 1576.83 secondes

Train loss 0.06983245846286819 micro_f1_score 0.9215670941507663 
 
time = 50.31 secondes

Val loss 0.25634749905496346 micro_f1_score 0.756946887091101
 
----------
Epoch 9/40
time = 1576.86 secondes

Train loss 0.057329153282953814 micro_f1_score 0.9393209540828218 
 
time = 49.66 secondes

Val loss 0.26485018517638814 micro_f1_score 0.7582496413199427
 
----------
Epoch 10/40
time = 1577.90 secondes

Train loss 0.04797383648330799 micro_f1_score 0.9478452781972353 
 
time = 50.03 secondes

Val loss 0.28787558953293035 micro_f1_score 0.7506315409599422
 
----------
Epoch 11/40
time = 1576.13 secondes

Train loss 0.04039002981766857 micro_f1_score 0.957849677991593 
 
time = 50.03 secondes

Val loss 0.3139912257062607 micro_f1_score 0.7593316743690011
 
----------
Epoch 12/40
time = 1575.40 secondes

Train loss 0.033467252218995144 micro_f1_score 0.96447950740812 
 
time = 49.44 secondes

Val loss 0.34736340981526453 micro_f1_score 0.7445152158527955
 
----------
Epoch 13/40
time = 1573.83 secondes

Train loss 0.027529553350675768 micro_f1_score 0.9699646643109541 
 
time = 50.02 secondes

Val loss 0.3625737482407054 micro_f1_score 0.7423117709437963
 
----------
Epoch 14/40
time = 1573.49 secondes

Train loss 0.024987904457396383 micro_f1_score 0.9726830389640244 
 
time = 49.62 secondes

Val loss 0.364262125531181 micro_f1_score 0.7505376344086021
 
----------
Epoch 15/40
time = 1589.49 secondes

Train loss 0.021720667110613403 micro_f1_score 0.9775143403441684 
 
time = 50.04 secondes

Val loss 0.3730852310774756 micro_f1_score 0.7566787003610108
 
----------
Epoch 16/40
time = 1580.64 secondes

Train loss 0.019957136596460735 micro_f1_score 0.9777947348340328 
 
time = 49.44 secondes

Val loss 0.37988627592071156 micro_f1_score 0.7614475627769572
 
----------
Epoch 17/40
time = 1578.41 secondes

Train loss 0.016347896607633604 micro_f1_score 0.9826422004348987 
 
time = 49.27 secondes

Val loss 0.4164870019818916 micro_f1_score 0.7497322384862548
 
----------
Epoch 18/40
time = 1578.87 secondes

Train loss 0.01563267008703504 micro_f1_score 0.9844464775846294 
 
time = 49.87 secondes

Val loss 0.42455766081321433 micro_f1_score 0.7524752475247525
 
----------
Epoch 19/40
time = 1579.62 secondes

Train loss 0.013699901598004357 micro_f1_score 0.9853945010105631 
 
time = 49.71 secondes

Val loss 0.45102342059377765 micro_f1_score 0.749098774333093
 
----------
Epoch 20/40
time = 1580.63 secondes

Train loss 0.013213090730612356 micro_f1_score 0.9870525514089871 
 
time = 49.77 secondes

Val loss 0.4765105760488354 micro_f1_score 0.7531531531531532
 
----------
Epoch 21/40
time = 1580.83 secondes

Train loss 0.013332737535227778 micro_f1_score 0.9862252663622527 
 
time = 49.80 secondes

Val loss 0.4708457997099298 micro_f1_score 0.7529495888451913
 
----------
Epoch 22/40
time = 1581.84 secondes

Train loss 0.011269021402338628 micro_f1_score 0.9887401095556907 
 
time = 50.16 secondes

Val loss 0.44212440706667355 micro_f1_score 0.759027266028003
 
----------
Epoch 23/40
time = 1587.34 secondes

Train loss 0.009374313356657933 micro_f1_score 0.9903260207190738 
 
time = 50.21 secondes

Val loss 0.4777905653978958 micro_f1_score 0.7521676300578035
 
----------
Epoch 24/40
time = 1588.19 secondes

Train loss 0.008620446323556616 micro_f1_score 0.990900437845041 
 
time = 50.76 secondes

Val loss 0.502041892194357 micro_f1_score 0.7426192278576836
 
----------
Epoch 25/40
time = 1590.36 secondes

Train loss 0.007915088412092855 micro_f1_score 0.992197906755471 
 
time = 50.29 secondes

Val loss 0.5115535891935473 micro_f1_score 0.7536646406864497
 
----------
Epoch 26/40
time = 1587.55 secondes

Train loss 0.007094311734328732 micro_f1_score 0.9920882464815519 
 
time = 50.26 secondes

Val loss 0.5426429536987524 micro_f1_score 0.7456626061277224
 
----------
Epoch 27/40
time = 1589.39 secondes

Train loss 0.007541407367574032 micro_f1_score 0.993122316373447 
 
time = 50.20 secondes

Val loss 0.5173759447013746 micro_f1_score 0.747844827586207
 
----------
Epoch 28/40
time = 1587.74 secondes

Train loss 0.006650118742110355 micro_f1_score 0.9932697060724741 
 
time = 50.04 secondes

Val loss 0.5244463012843835 micro_f1_score 0.7526881720430108
 
----------
Epoch 29/40
time = 1589.46 secondes

Train loss 0.006210327441655315 micro_f1_score 0.9941116134179235 
 
time = 50.37 secondes

Val loss 0.5161806840877063 micro_f1_score 0.7560795873249814
 
----------
Epoch 30/40
time = 1588.02 secondes

Train loss 0.00486953267774863 micro_f1_score 0.9950173063025369 
 
time = 50.23 secondes

Val loss 0.5276941809742177 micro_f1_score 0.7572463768115943
 
----------
Epoch 31/40
time = 1583.76 secondes

Train loss 0.004720384724383654 micro_f1_score 0.9951716534235638 
 
time = 49.64 secondes

Val loss 0.5212514533615503 micro_f1_score 0.7571324067300659
 
----------
Epoch 32/40
time = 1578.01 secondes

Train loss 0.003074280731476671 micro_f1_score 0.9966542468253364 
 
time = 49.63 secondes

Val loss 0.5366463388820164 micro_f1_score 0.7600585223116314
 
----------
Epoch 33/40
time = 1578.57 secondes

Train loss 0.003379380191764286 micro_f1_score 0.9965439975694049 
 
time = 49.87 secondes

Val loss 0.5480495029297031 micro_f1_score 0.7494584837545126
 
----------
Epoch 34/40
time = 1577.74 secondes

Train loss 0.0028147964413644355 micro_f1_score 0.9975683890577508 
 
time = 49.45 secondes

Val loss 0.5602516236363865 micro_f1_score 0.7583988563259471
 
----------
Epoch 35/40
time = 1589.13 secondes

Train loss 0.0030326929034451603 micro_f1_score 0.9969981380856482 
 
time = 52.61 secondes

Val loss 0.555387014248332 micro_f1_score 0.7607843137254902
 
----------
Epoch 36/40
time = 1615.05 secondes

Train loss 0.0015623480659857567 micro_f1_score 0.9983281404362034 
 
time = 52.38 secondes

Val loss 0.5614484549790132 micro_f1_score 0.7537906137184116
 
----------
Epoch 37/40
time = 1626.49 secondes

Train loss 0.001689074352681338 micro_f1_score 0.9982132674396502 
 
time = 54.57 secondes

Val loss 0.5891592158157317 micro_f1_score 0.7522538766678687
 
----------
Epoch 38/40
time = 1626.67 secondes

Train loss 0.0012699913046859152 micro_f1_score 0.9986700611771858 
 
time = 53.28 secondes

Val loss 0.5801729597761983 micro_f1_score 0.756193895870736
 
----------
Epoch 39/40
time = 1625.81 secondes

Train loss 0.0010255267940543114 micro_f1_score 0.9990120079039368 
 
time = 53.50 secondes

Val loss 0.5694238909932433 micro_f1_score 0.7585957292797683
 
----------
Epoch 40/40
time = 1636.50 secondes

Train loss 0.0010268989684592016 micro_f1_score 0.998861047835991 
 
time = 54.80 secondes

Val loss 0.6083341659825356 micro_f1_score 0.751954513148543
 
----------
best_f1_socre 0.7760088855979268 best_epoch 6

average train time 1586.735289078951

average val time 50.46952927708626
 
time = 56.98 secondes

test_f1_score 0.7576866764275255

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_128_2
----------
Epoch 1/40
Attention type 'block_sparse' is not possible if sequence_length: 1024 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 1408 with config.block_size = 128, config.num_random_blocks = 3. Changing attention type to 'original_full'...
time = 950.63 secondes

Train loss 0.26179685393969215 micro_f1_score 0.6394450988090564 
 
time = 43.50 secondes

Val loss 0.21996718913805288 micro_f1_score 0.6754491017964072
 
----------
Epoch 2/40
time = 937.87 secondes

Train loss 0.17010342108236776 micro_f1_score 0.770895031260283 
 
time = 44.14 secondes

Val loss 0.193120219668404 micro_f1_score 0.7232212666145426
 
----------
Epoch 3/40
time = 929.51 secondes

Train loss 0.14608601894363896 micro_f1_score 0.8085123664866183 
 
time = 44.96 secondes

Val loss 0.18506804719323017 micro_f1_score 0.7483044461190657
 
----------
Epoch 4/40
time = 932.18 secondes

Train loss 0.12661143329732857 micro_f1_score 0.8409621108519024 
 
time = 44.97 secondes

Val loss 0.20087776223167045 micro_f1_score 0.7480344440284538
 
----------
Epoch 5/40
time = 931.58 secondes

Train loss 0.11209053971074723 micro_f1_score 0.8629639966508513 
 
time = 43.15 secondes

Val loss 0.19696888327598572 micro_f1_score 0.7518796992481204
 
----------
Epoch 6/40
time = 930.83 secondes

Train loss 0.09915138116307758 micro_f1_score 0.8821850471735512 
 
time = 43.74 secondes

Val loss 0.2044030177544375 micro_f1_score 0.7529751172015867
 
----------
Epoch 7/40
time = 932.36 secondes

Train loss 0.08506376995696678 micro_f1_score 0.9019069292945147 
 
time = 43.18 secondes

Val loss 0.23962537467968267 micro_f1_score 0.7465980139757263
 
----------
Epoch 8/40
time = 933.27 secondes

Train loss 0.07528765561092679 micro_f1_score 0.9157276995305165 
 
time = 41.05 secondes

Val loss 0.2513451566461657 micro_f1_score 0.7492774566473989
 
----------
Epoch 9/40
time = 930.32 secondes

Train loss 0.06303759605439195 micro_f1_score 0.9299863786728935 
 
time = 42.35 secondes

Val loss 0.25624012421877657 micro_f1_score 0.756717501815541
 
----------
Epoch 10/40
time = 934.45 secondes

Train loss 0.05505454774132116 micro_f1_score 0.9397954439795443 
 
time = 42.43 secondes

Val loss 0.2694669994907301 micro_f1_score 0.7565217391304347
 
----------
Epoch 11/40
time = 934.70 secondes

Train loss 0.045864919192988324 micro_f1_score 0.9492468134414832 
 
time = 44.62 secondes

Val loss 0.2784085634057639 micro_f1_score 0.7633262260127933
 
----------
Epoch 12/40
time = 933.87 secondes

Train loss 0.03971794043858494 micro_f1_score 0.9573649376635371 
 
time = 43.52 secondes

Val loss 0.29676633657979185 micro_f1_score 0.7595026642984014
 
----------
Epoch 13/40
time = 933.32 secondes

Train loss 0.0337583606150253 micro_f1_score 0.9641498559077809 
 
time = 44.66 secondes

Val loss 0.3073504805320599 micro_f1_score 0.758303886925795
 
----------
Epoch 14/40
time = 922.95 secondes

Train loss 0.02943284388386166 micro_f1_score 0.9680806222937504 
 
time = 42.53 secondes

Val loss 0.3389123297128521 micro_f1_score 0.7584269662921349
 
----------
Epoch 15/40
time = 934.62 secondes

Train loss 0.02578793782536318 micro_f1_score 0.9723794950267789 
 
time = 44.19 secondes

Val loss 0.3647378619577064 micro_f1_score 0.7509659290481209
 
----------
Epoch 16/40
time = 932.73 secondes

Train loss 0.022260525997629524 micro_f1_score 0.9763616891064871 
 
time = 43.37 secondes

Val loss 0.384125535116821 micro_f1_score 0.7545454545454546
 
----------
Epoch 17/40
time = 936.93 secondes

Train loss 0.019805732624278077 micro_f1_score 0.9789899915959965 
 
time = 43.12 secondes

Val loss 0.3818546802294059 micro_f1_score 0.7635850388143967
 
----------
Epoch 18/40
time = 932.91 secondes

Train loss 0.01788702533999353 micro_f1_score 0.9810946033685978 
 
time = 46.23 secondes

Val loss 0.3938412998543411 micro_f1_score 0.7560014331780724
 
----------
Epoch 19/40
time = 946.23 secondes

Train loss 0.015521142840031995 micro_f1_score 0.9832990162434225 
 
time = 43.91 secondes

Val loss 0.4058228860624501 micro_f1_score 0.7625951431678144
 
----------
Epoch 20/40
time = 943.60 secondes

Train loss 0.013460724142082204 micro_f1_score 0.9855061408192843 
 
time = 45.47 secondes

Val loss 0.4262582367805184 micro_f1_score 0.7627302275189599
 
----------
Epoch 21/40
time = 941.54 secondes

Train loss 0.01356884398689569 micro_f1_score 0.9854045196448306 
 
time = 45.10 secondes

Val loss 0.4199562786055393 micro_f1_score 0.7656082768462361
 
----------
Epoch 22/40
time = 945.57 secondes

Train loss 0.011479865088308994 micro_f1_score 0.9879362179853102 
 
time = 45.48 secondes

Val loss 0.44512106893492526 micro_f1_score 0.7629764065335755
 
----------
Epoch 23/40
time = 944.07 secondes

Train loss 0.010031681631451581 micro_f1_score 0.9886264216972878 
 
time = 45.55 secondes

Val loss 0.44462649634138485 micro_f1_score 0.7706685837526959
 
----------
Epoch 24/40
time = 945.13 secondes

Train loss 0.009669454289391283 micro_f1_score 0.9901789113056718 
 
time = 42.11 secondes

Val loss 0.4590437522188562 micro_f1_score 0.7620764239365536
 
----------
Epoch 25/40
time = 942.89 secondes

Train loss 0.008702143542180787 micro_f1_score 0.9911800486618005 
 
time = 46.13 secondes

Val loss 0.47890863347737517 micro_f1_score 0.7547169811320754
 
----------
Epoch 26/40
time = 952.33 secondes

Train loss 0.008949226538148192 micro_f1_score 0.9912237376999354 
 
time = 49.75 secondes

Val loss 0.4777347697097747 micro_f1_score 0.7643593519882179
 
----------
Epoch 27/40
time = 1096.29 secondes

Train loss 0.008164200345463295 micro_f1_score 0.9911713220184184 
 
time = 50.78 secondes

Val loss 0.4730953685328609 micro_f1_score 0.7654676258992806
 
----------
Epoch 28/40
time = 1093.14 secondes

Train loss 0.007815928253484681 micro_f1_score 0.9916381603952871 
 
time = 51.09 secondes

Val loss 0.496630924158409 micro_f1_score 0.7552498189717596
 
----------
Epoch 29/40
time = 1104.15 secondes

Train loss 0.00625238311402935 micro_f1_score 0.9932324538057942 
 
time = 49.99 secondes

Val loss 0.5036537273496878 micro_f1_score 0.7574007220216606
 
----------
Epoch 30/40
time = 1101.63 secondes

Train loss 0.005036103368059287 micro_f1_score 0.9948686761184387 
 
time = 50.92 secondes

Val loss 0.5144022038725556 micro_f1_score 0.7587230883444692
 
----------
Epoch 31/40
time = 1106.65 secondes

Train loss 0.004116419143751914 micro_f1_score 0.9956288722490402 
 
time = 51.19 secondes

Val loss 0.5117249708683764 micro_f1_score 0.7650471356055112
 
----------
Epoch 32/40
time = 1254.19 secondes

Train loss 0.003952442408948267 micro_f1_score 0.9961240310077519 
 
time = 62.85 secondes

Val loss 0.5477109099509286 micro_f1_score 0.7619728377412437
 
----------
Epoch 33/40
time = 1370.73 secondes

Train loss 0.004240200211182758 micro_f1_score 0.9960080599171198 
 
time = 63.20 secondes

Val loss 0.5617013175467975 micro_f1_score 0.7549295774647887
 
----------
Epoch 34/40
time = 1364.16 secondes

Train loss 0.0032420561054276003 micro_f1_score 0.9967703940119306 
 
time = 61.51 secondes

Val loss 0.5589589574297921 micro_f1_score 0.759377211606511
 
----------
Epoch 35/40
time = 1368.37 secondes

Train loss 0.0030509768465177525 micro_f1_score 0.9969983661993237 
 
time = 62.00 secondes

Val loss 0.5479679847838449 micro_f1_score 0.7635590216235377
 
----------
Epoch 36/40
time = 1363.61 secondes

Train loss 0.0026834117329353584 micro_f1_score 0.9974548907882241 
 
time = 62.23 secondes

Val loss 0.5668988594266234 micro_f1_score 0.7629370629370629
 
----------
Epoch 37/40
time = 1364.12 secondes

Train loss 0.0018271719460891938 micro_f1_score 0.9980249164387724 
 
time = 62.06 secondes

Val loss 0.5690871670109326 micro_f1_score 0.7565649396735273
 
----------
Epoch 38/40
time = 1364.57 secondes

Train loss 0.0014352112607731533 micro_f1_score 0.9984429000037979 
 
time = 61.84 secondes

Val loss 0.549972900601684 micro_f1_score 0.7584453323646931
 
----------
Epoch 39/40
time = 1365.35 secondes

Train loss 0.001229423064080595 micro_f1_score 0.9988223226835846 
 
time = 61.92 secondes

Val loss 0.5606096485843424 micro_f1_score 0.7630434782608696
 
----------
Epoch 40/40
time = 1362.76 secondes

Train loss 0.0009978695606830796 micro_f1_score 0.9991645781119466 
 
time = 61.36 secondes

Val loss 0.5960588276874824 micro_f1_score 0.7558221594918842
 
----------
best_f1_socre 0.7706685837526959 best_epoch 23

average train time 1051.151881825924

average val time 49.05411611795425
 
time = 62.94 secondes

test_f1_score 0.7384937238493723

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_64_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 252.00 MiB (GPU 1; 79.20 GiB total capacity; 50.81 GiB already allocated; 147.31 MiB free; 51.76 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_128_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 79.20 GiB total capacity; 49.75 GiB already allocated; 151.31 MiB free; 51.75 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_64_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 540.00 MiB (GPU 1; 79.20 GiB total capacity; 47.64 GiB already allocated; 141.31 MiB free; 51.76 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_128_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 2.11 GiB (GPU 1; 79.20 GiB total capacity; 48.07 GiB already allocated; 1.88 GiB free; 50.02 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_256_2
----------
Epoch 1/40
time = 3366.15 secondes

Train loss 0.2355213862885763 micro_f1_score 0.6676875957120981 
 
time = 188.25 secondes

Val loss 0.199663301838226 micro_f1_score 0.7095761381475668
 
----------
Epoch 2/40
time = 3352.34 secondes

Train loss 0.16332580119639903 micro_f1_score 0.7807967313585291 
 
time = 184.88 secondes

Val loss 0.20299138446323206 micro_f1_score 0.7117834394904459
 
----------
Epoch 3/40
time = 3356.10 secondes

Train loss 0.14244681730665065 micro_f1_score 0.813637466634312 
 
time = 188.66 secondes

Val loss 0.18614329153397044 micro_f1_score 0.7502885725278953
 
----------
Epoch 4/40
time = 3353.84 secondes

Train loss 0.12469780951201379 micro_f1_score 0.8418054162487463 
 
time = 187.28 secondes

Val loss 0.1941724093478234 micro_f1_score 0.7459016393442622
 
----------
Epoch 5/40
time = 3347.23 secondes

Train loss 0.10916878348635929 micro_f1_score 0.8671834625322997 
 
time = 187.87 secondes

Val loss 0.19094229929271292 micro_f1_score 0.760917838638046
 
----------
Epoch 6/40
time = 3349.92 secondes

Train loss 0.09559730613218234 micro_f1_score 0.8898807172762461 
 
time = 187.56 secondes

Val loss 0.20900883427897438 micro_f1_score 0.7513691128148959
 
----------
Epoch 7/40
time = 3354.09 secondes

Train loss 0.08467405498078143 micro_f1_score 0.9037955803273541 
 
time = 192.85 secondes

Val loss 0.20423128993296233 micro_f1_score 0.7595029239766081
 
----------
Epoch 8/40
time = 3849.63 secondes

Train loss 0.0739824864845555 micro_f1_score 0.9185092585358232 
 
time = 214.65 secondes

Val loss 0.21923707205741133 micro_f1_score 0.756717501815541
 
----------
Epoch 9/40
time = 3861.30 secondes

Train loss 0.06632843512773245 micro_f1_score 0.927962638645651 
 
time = 216.11 secondes

Val loss 0.23074837212191254 micro_f1_score 0.7560262965668372
 
----------
Epoch 10/40
time = 3454.60 secondes

Train loss 0.058173990419117715 micro_f1_score 0.9396400948346225 
 
time = 192.16 secondes

Val loss 0.23539750974197857 micro_f1_score 0.7583212735166425
 
----------
Epoch 11/40
time = 3377.27 secondes

Train loss 0.04953929058964121 micro_f1_score 0.9507638754592923 
 
time = 188.92 secondes

Val loss 0.251059106505308 micro_f1_score 0.7591865858009277
 
----------
Epoch 12/40
time = 3386.51 secondes

Train loss 0.043337497970525614 micro_f1_score 0.9563300197269176 
 
time = 192.10 secondes

Val loss 0.25818360914460947 micro_f1_score 0.7584715212689258
 
----------
Epoch 13/40
time = 3385.32 secondes

Train loss 0.0396034224646854 micro_f1_score 0.9615844025738836 
 
time = 188.83 secondes

Val loss 0.2756466811797658 micro_f1_score 0.7568725455194574
 
----------
Epoch 14/40
time = 3370.89 secondes

Train loss 0.035715704034544056 micro_f1_score 0.9626520868627753 
 
time = 186.07 secondes

Val loss 0.27587152296890977 micro_f1_score 0.7588627588627589
 
----------
Epoch 15/40
time = 3359.79 secondes

Train loss 0.0321209333042052 micro_f1_score 0.9678510998307952 
 
time = 188.72 secondes

Val loss 0.28434522388899913 micro_f1_score 0.7558940877765687
 
----------
Epoch 16/40
time = 3357.47 secondes

Train loss 0.02721370166431911 micro_f1_score 0.9732755645872474 
 
time = 192.21 secondes

Val loss 0.2892772727569596 micro_f1_score 0.7587198849334771
 
----------
Epoch 17/40
time = 3355.30 secondes

Train loss 0.025096151073094087 micro_f1_score 0.9752991715250076 
 
time = 188.76 secondes

Val loss 0.2979155115661074 micro_f1_score 0.7538738738738738
 
----------
Epoch 18/40
time = 3349.39 secondes

Train loss 0.02177204641372989 micro_f1_score 0.9776282075796398 
 
time = 190.20 secondes

Val loss 0.3052512050652113 micro_f1_score 0.7636621717530163
 
----------
Epoch 19/40
time = 3354.88 secondes

Train loss 0.020131457619566202 micro_f1_score 0.980260394792104 
 
time = 190.16 secondes

Val loss 0.3192869202523935 micro_f1_score 0.7567368032484313
 
----------
Epoch 20/40
time = 3359.34 secondes

Train loss 0.017648104354969923 micro_f1_score 0.9827750830691669 
 
time = 190.23 secondes

Val loss 0.32489564343065513 micro_f1_score 0.7626514611546688
 
----------
Epoch 21/40
time = 3374.02 secondes

Train loss 0.015441541743374572 micro_f1_score 0.9838919001450491 
 
time = 191.60 secondes

Val loss 0.3421736545250064 micro_f1_score 0.7609308885754583
 
----------
Epoch 22/40
time = 2974.37 secondes

Train loss 0.015073118352190685 micro_f1_score 0.9843583091713719 
 
time = 164.93 secondes

Val loss 0.33637941311128805 micro_f1_score 0.7634408602150538
 
----------
Epoch 23/40
time = 2937.81 secondes

Train loss 0.012749072736567557 micro_f1_score 0.9876091349269892 
 
time = 167.71 secondes

Val loss 0.34022119616875884 micro_f1_score 0.763420955201777
 
----------
Epoch 24/40
time = 2945.94 secondes

Train loss 0.012419750811720922 micro_f1_score 0.9877041379572881 
 
time = 165.54 secondes

Val loss 0.3544712423301134 micro_f1_score 0.7574007220216606
 
----------
Epoch 25/40
time = 2946.95 secondes

Train loss 0.011876993979115525 micro_f1_score 0.9878289974136619 
 
time = 165.03 secondes

Val loss 0.3550220632650813 micro_f1_score 0.7599856063332134
 
----------
Epoch 26/40
time = 2939.67 secondes

Train loss 0.010316305364946717 micro_f1_score 0.9894159750247467 
 
time = 166.99 secondes

Val loss 0.3684467563375098 micro_f1_score 0.7590404582885787
 
----------
Epoch 27/40
time = 2934.20 secondes

Train loss 0.00882416262620691 micro_f1_score 0.9913619239697098 
 
time = 164.90 secondes

Val loss 0.37586964740127815 micro_f1_score 0.7663685152057246
 
----------
Epoch 28/40
time = 2938.38 secondes

Train loss 0.007702634368051968 micro_f1_score 0.9926540554942337 
 
time = 164.17 secondes

Val loss 0.39077698500429997 micro_f1_score 0.7603539823008849
 
----------
Epoch 29/40
time = 2949.26 secondes

Train loss 0.008222417187770023 micro_f1_score 0.9915115526626318 
 
time = 166.90 secondes

Val loss 0.39059480927029594 micro_f1_score 0.7557803468208093
 
----------
Epoch 30/40
time = 2943.77 secondes

Train loss 0.007216345058672281 micro_f1_score 0.9922038410344172 
 
time = 167.51 secondes

Val loss 0.3926751139711161 micro_f1_score 0.7639885222381635
 
----------
Epoch 31/40
time = 2962.14 secondes

Train loss 0.005801925940993913 micro_f1_score 0.9941449319443388 
 
time = 167.71 secondes

Val loss 0.38969952466546515 micro_f1_score 0.7656813266041816
 
----------
Epoch 32/40
time = 2948.72 secondes

Train loss 0.005726428113619247 micro_f1_score 0.994226240218795 
 
time = 166.26 secondes

Val loss 0.41174171596276954 micro_f1_score 0.7588961510530138
 
----------
Epoch 33/40
time = 2944.11 secondes

Train loss 0.00498996888278713 micro_f1_score 0.9951705517739666 
 
time = 167.58 secondes

Val loss 0.4019321835920459 micro_f1_score 0.7671331180480805
 
----------
Epoch 34/40
time = 2942.00 secondes

Train loss 0.0046609491835215115 micro_f1_score 0.9956686930091184 
 
time = 165.85 secondes

Val loss 0.4126453101634979 micro_f1_score 0.7609475951184493
 
----------
Epoch 35/40
time = 2939.73 secondes

Train loss 0.0032918014785962867 micro_f1_score 0.9970362489550878 
 
time = 165.36 secondes

Val loss 0.4199564061203941 micro_f1_score 0.7568940493468795
 
----------
Epoch 36/40
time = 2912.40 secondes

Train loss 0.002809842319966101 micro_f1_score 0.9971122425716239 
 
time = 163.28 secondes

Val loss 0.41495755998814693 micro_f1_score 0.7645327446651949
 
----------
Epoch 37/40
time = 2902.29 secondes

Train loss 0.0023473704371418853 micro_f1_score 0.9979481723535222 
 
time = 163.02 secondes

Val loss 0.42667779741717166 micro_f1_score 0.7608461814270348
 
----------
Epoch 38/40
time = 2907.58 secondes

Train loss 0.002189637856691161 micro_f1_score 0.9979104137380798 
 
time = 163.40 secondes

Val loss 0.41780654670762235 micro_f1_score 0.7622478386167147
 
----------
Epoch 39/40
time = 2916.34 secondes

Train loss 0.0015073836235387716 micro_f1_score 0.9984798966329711 
 
time = 163.19 secondes

Val loss 0.4255054343919285 micro_f1_score 0.7647268832559801
 
----------
Epoch 40/40
time = 2915.49 secondes

Train loss 0.0016046155108490787 micro_f1_score 0.9987084029782708 
 
time = 163.67 secondes

Val loss 0.4283312190262998 micro_f1_score 0.765697883028346
 
----------
best_f1_socre 0.7671331180480805 best_epoch 33

average train time 3186.9135423600674

average val time 179.27574899196625
 
time = 171.38 secondes

test_f1_score 0.7497389488339714

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_512_2
----------
Epoch 1/40
time = 911.81 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Train loss 0.2314113285627451 micro_f1_score 0.6730620985010707 
 
time = 55.66 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss 0.19637128112257504 micro_f1_score 0.7173489278752437
 
----------
Epoch 2/40
time = 909.52 secondes

Train loss 0.16146558119101567 micro_f1_score 0.7824269497151757 
 
time = 53.87 secondes

Val loss 0.1897351332619542 micro_f1_score 0.735248447204969
 
----------
Epoch 3/40
time = 906.91 secondes

Train loss 0.1393115006252989 micro_f1_score 0.8194371330228792 
 
time = 52.48 secondes

Val loss 0.18486997961509424 micro_f1_score 0.7483672685363042
 
----------
Epoch 4/40
time = 906.99 secondes

Train loss 0.12229456256672337 micro_f1_score 0.8478470437017995 
 
time = 53.26 secondes

Val loss 0.183314141557842 micro_f1_score 0.756797583081571
 
----------
Epoch 5/40
time = 900.89 secondes

Train loss 0.10926750327989056 micro_f1_score 0.8663554012530429 
 
time = 48.25 secondes

Val loss 0.20223503901821668 micro_f1_score 0.7476085356880058
 
----------
Epoch 6/40
time = 850.74 secondes

Train loss 0.096152107375632 micro_f1_score 0.8893908770261166 
 
time = 47.98 secondes

Val loss 0.20311653479689457 micro_f1_score 0.755192878338279
 
----------
Epoch 7/40
time = 846.09 secondes

Train loss 0.08485767259723968 micro_f1_score 0.9057762727881246 
 
time = 45.61 secondes

Val loss 0.20871407758505617 micro_f1_score 0.7561064527889172
 
----------
Epoch 8/40
time = 845.69 secondes

Train loss 0.07416610790685088 micro_f1_score 0.9198619932564888 
 
time = 46.97 secondes

Val loss 0.21847889328100642 micro_f1_score 0.7560975609756098
 
----------
Epoch 9/40
time = 843.12 secondes

Train loss 0.06517588966651945 micro_f1_score 0.9290705390951322 
 
time = 45.75 secondes

Val loss 0.22703080722054497 micro_f1_score 0.7613997879109224
 
----------
Epoch 10/40
time = 844.16 secondes

Train loss 0.057759327011031876 micro_f1_score 0.9394965176452278 
 
time = 45.46 secondes

Val loss 0.23879154392930327 micro_f1_score 0.7541456380677721
 
----------
Epoch 11/40
time = 844.12 secondes

Train loss 0.05034879692494534 micro_f1_score 0.9476007900546067 
 
time = 45.15 secondes

Val loss 0.24925912617415677 micro_f1_score 0.756640344580043
 
----------
Epoch 12/40
time = 845.65 secondes

Train loss 0.045191589870850916 micro_f1_score 0.9554928270368509 
 
time = 45.07 secondes

Val loss 0.26651649720600395 micro_f1_score 0.7567567567567568
 
----------
Epoch 13/40
time = 845.21 secondes

Train loss 0.040008357020413825 micro_f1_score 0.9601141799105076 
 
time = 46.21 secondes

Val loss 0.28437895449947137 micro_f1_score 0.752808988764045
 
----------
Epoch 14/40
time = 846.70 secondes

Train loss 0.034283057609489104 micro_f1_score 0.9649994224327134 
 
time = 45.39 secondes

Val loss 0.28941885992640354 micro_f1_score 0.7533664068036854
 
----------
Epoch 15/40
time = 852.31 secondes

Train loss 0.029671277492810545 micro_f1_score 0.9702605087220473 
 
time = 43.51 secondes

Val loss 0.31495216830832057 micro_f1_score 0.7491166077738516
 
----------
Epoch 16/40
time = 857.75 secondes

Train loss 0.02768835957048813 micro_f1_score 0.9727203065134099 
 
time = 39.64 secondes

Val loss 0.30399592782630297 micro_f1_score 0.7569296375266525
 
----------
Epoch 17/40
time = 840.69 secondes

Train loss 0.02404075851569556 micro_f1_score 0.9780434532704909 
 
time = 40.26 secondes

Val loss 0.3114149583900561 micro_f1_score 0.7518531591951995
 
----------
Epoch 18/40
time = 851.78 secondes

Train loss 0.020203010157182778 micro_f1_score 0.9800473969879978 
 
time = 39.18 secondes

Val loss 0.31276018880918377 micro_f1_score 0.7649769585253456
 
----------
Epoch 19/40
time = 857.41 secondes

Train loss 0.018598732364737337 micro_f1_score 0.9810441030344721 
 
time = 39.06 secondes

Val loss 0.34384334490436025 micro_f1_score 0.7553080403759137
 
----------
Epoch 20/40
time = 850.48 secondes

Train loss 0.01709394310401786 micro_f1_score 0.9836653690557973 
 
time = 40.22 secondes

Val loss 0.3314106196897929 micro_f1_score 0.7627541919372101
 
----------
Epoch 21/40
time = 852.19 secondes

Train loss 0.016005543618502833 micro_f1_score 0.9844769060604905 
 
time = 39.78 secondes

Val loss 0.338510555322053 micro_f1_score 0.7531380753138075
 
----------
Epoch 22/40
time = 967.69 secondes

Train loss 0.013590196963150038 micro_f1_score 0.9869198794950997 
 
time = 56.22 secondes

Val loss 0.3421919168751748 micro_f1_score 0.7545164718384697
 
----------
Epoch 23/40
time = 1144.53 secondes

Train loss 0.012821088460020235 micro_f1_score 0.9872327451503488 
 
time = 53.40 secondes

Val loss 0.35929612070322037 micro_f1_score 0.7557440791799223
 
----------
Epoch 24/40
time = 1118.11 secondes

Train loss 0.011240063841939225 micro_f1_score 0.988956587966489 
 
time = 52.60 secondes

Val loss 0.35755065181216256 micro_f1_score 0.76013143483023
 
----------
Epoch 25/40
time = 1098.63 secondes

Train loss 0.01114971848240915 micro_f1_score 0.9889206167904055 
 
time = 51.52 secondes

Val loss 0.3722310085765651 micro_f1_score 0.748102638236357
 
----------
Epoch 26/40
time = 1105.26 secondes

Train loss 0.009360603893648725 micro_f1_score 0.9910156844830211 
 
time = 55.48 secondes

Val loss 0.38454402677836963 micro_f1_score 0.7567567567567568
 
----------
Epoch 27/40
time = 1094.41 secondes

Train loss 0.010215023001849961 micro_f1_score 0.989378307381886 
 
time = 57.33 secondes

Val loss 0.38422080581305457 micro_f1_score 0.758273381294964
 
----------
Epoch 28/40
time = 1120.37 secondes

Train loss 0.009088335950860988 micro_f1_score 0.99120100559936 
 
time = 53.87 secondes

Val loss 0.4066127343255965 micro_f1_score 0.7524822695035461
 
----------
Epoch 29/40
time = 1042.18 secondes

Train loss 0.007917056422468762 micro_f1_score 0.9922793138858251 
 
time = 44.38 secondes

Val loss 0.40450326084602073 micro_f1_score 0.7541899441340782
 
----------
Epoch 30/40
time = 987.47 secondes

Train loss 0.006802600750705932 micro_f1_score 0.9936915710268298 
 
time = 44.32 secondes

Val loss 0.3965540059277269 micro_f1_score 0.7624151482672383
 
----------
Epoch 31/40
time = 980.92 secondes

Train loss 0.005940027815000855 micro_f1_score 0.9941017542524448 
 
time = 45.93 secondes

Val loss 0.3889442644402629 micro_f1_score 0.7614579574160951
 
----------
Epoch 32/40
time = 982.33 secondes

Train loss 0.005253698308254709 micro_f1_score 0.9946776155717761 
 
time = 44.45 secondes

Val loss 0.40088378821240095 micro_f1_score 0.7604590892262125
 
----------
Epoch 33/40
time = 983.11 secondes

Train loss 0.005128928360493585 micro_f1_score 0.994715029846774 
 
time = 45.06 secondes

Val loss 0.4125991190066103 micro_f1_score 0.7579617834394904
 
----------
Epoch 34/40
time = 985.94 secondes

Train loss 0.003815354908553533 micro_f1_score 0.9960474308300394 
 
time = 43.78 secondes

Val loss 0.43314101302721464 micro_f1_score 0.7450564971751411
 
----------
Epoch 35/40
time = 980.73 secondes

Train loss 0.003397152926743991 micro_f1_score 0.9966547555690718 
 
time = 44.07 secondes

Val loss 0.4132840952179471 micro_f1_score 0.7578649699540474
 
----------
Epoch 36/40
time = 956.24 secondes

Train loss 0.0031637991189855455 micro_f1_score 0.9969981380856482 
 
time = 38.50 secondes

Val loss 0.4259606182086663 micro_f1_score 0.7531083481349912
 
----------
Epoch 37/40
time = 852.50 secondes

Train loss 0.0030918667536818313 micro_f1_score 0.9970351223962293 
 
time = 38.95 secondes

Val loss 0.4350962276097204 micro_f1_score 0.7556980056980056
 
----------
Epoch 38/40
time = 867.88 secondes

Train loss 0.0021166184218207097 micro_f1_score 0.9980237154150198 
 
time = 38.22 secondes

Val loss 0.43519973217463886 micro_f1_score 0.756512678013199
 
----------
Epoch 39/40
time = 1068.01 secondes

Train loss 0.0014047965025154325 micro_f1_score 0.9987082066869301 
 
time = 52.29 secondes

Val loss 0.43213248033015456 micro_f1_score 0.7596460176991151
 
----------
Epoch 40/40
time = 1115.33 secondes

Train loss 0.0015392826250840823 micro_f1_score 0.9987462482428479 
 
time = 54.87 secondes

Val loss 0.4251712583860413 micro_f1_score 0.7622775800711744
 
----------
best_f1_socre 0.7649769585253456 best_epoch 18

average train time 939.045694321394

average val time 46.84961758255959
 
time = 60.88 secondes

test_f1_score 0.7385146804835925

----------

Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_256_2
----------
Epoch 1/40
time = 867.78 secondes

Train loss 0.21772205471187026 micro_f1_score 0.6973866389754241 
 
time = 30.15 secondes

Val loss 0.18171963743010505 micro_f1_score 0.7465940054495913
 
----------
Epoch 2/40
time = 870.21 secondes

Train loss 0.14972492432943335 micro_f1_score 0.8033032425211513 
 
time = 30.18 secondes

Val loss 0.18427569809995714 micro_f1_score 0.7435223432219301
 
----------
Epoch 3/40
time = 869.49 secondes

Train loss 0.12856896661624714 micro_f1_score 0.8380723855228953 
 
time = 29.85 secondes

Val loss 0.17060717134202114 micro_f1_score 0.7758749069247952
 
----------
Epoch 4/40
time = 868.21 secondes

Train loss 0.1113124052700293 micro_f1_score 0.8647342995169082 
 
time = 31.66 secondes

Val loss 0.16979001376961098 micro_f1_score 0.7786088257292445
 
----------
Epoch 5/40
time = 867.33 secondes

Train loss 0.09742518263860597 micro_f1_score 0.888319268081079 
 
time = 29.66 secondes

Val loss 0.18017622797948415 micro_f1_score 0.7800369685767098
 
----------
Epoch 6/40
time = 870.60 secondes

Train loss 0.08742315997411539 micro_f1_score 0.9000902916813881 
 
time = 29.91 secondes

Val loss 0.188859617978823 micro_f1_score 0.7741702741702742
 
----------
Epoch 7/40
time = 869.00 secondes

Train loss 0.07623199649104798 micro_f1_score 0.9162723397598842 
 
time = 29.92 secondes

Val loss 0.20351144024094597 micro_f1_score 0.7701729106628242
 
----------
Epoch 8/40
time = 867.28 secondes

Train loss 0.06587926322627846 micro_f1_score 0.9289945440374123 
 
time = 29.89 secondes

Val loss 0.21670322059119335 micro_f1_score 0.7757274662881476
 
----------
Epoch 9/40
time = 866.81 secondes

Train loss 0.05649473694245423 micro_f1_score 0.9411764705882353 
 
time = 30.79 secondes

Val loss 0.21465428747603152 micro_f1_score 0.7869318181818182
 
----------
Epoch 10/40
time = 867.49 secondes

Train loss 0.050994494966826995 micro_f1_score 0.9480775932164013 
 
time = 29.81 secondes

Val loss 0.21859457465957421 micro_f1_score 0.7813057438458795
 
----------
Epoch 11/40
time = 870.43 secondes

Train loss 0.04524884399211219 micro_f1_score 0.9539224037791373 
 
time = 29.84 secondes

Val loss 0.23416697746906123 micro_f1_score 0.778056651129437
 
----------
Epoch 12/40
time = 867.08 secondes

Train loss 0.03971052280132164 micro_f1_score 0.9603639728562616 
 
time = 31.07 secondes

Val loss 0.2420643174135294 micro_f1_score 0.7869085734614015
 
----------
Epoch 13/40
time = 866.86 secondes

Train loss 0.03532690984314425 micro_f1_score 0.9642087438423645 
 
time = 29.58 secondes

Val loss 0.2555764043917421 micro_f1_score 0.7752048450302815
 
----------
Epoch 14/40
time = 877.89 secondes

Train loss 0.03081960804668163 micro_f1_score 0.9689818195795058 
 
time = 31.82 secondes

Val loss 0.2641264575793118 micro_f1_score 0.7780530973451327
 
----------
Epoch 15/40
time = 879.38 secondes

Train loss 0.027849100955115915 micro_f1_score 0.9727052861914085 
 
time = 32.82 secondes

Val loss 0.2664614683780514 micro_f1_score 0.7868147617341453
 
----------
Epoch 16/40
time = 876.70 secondes

Train loss 0.024807513191423436 micro_f1_score 0.9760036740786101 
 
time = 31.86 secondes

Val loss 0.28565406664961673 micro_f1_score 0.7754816112084063
 
----------
Epoch 17/40
time = 880.16 secondes

Train loss 0.02039647864729485 micro_f1_score 0.9794232387363268 
 
time = 31.75 secondes

Val loss 0.28671950285063413 micro_f1_score 0.7861923212398731
 
----------
Epoch 18/40
time = 879.43 secondes

Train loss 0.019596208137856494 micro_f1_score 0.9806515754053228 
 
time = 31.07 secondes

Val loss 0.28335499641348105 micro_f1_score 0.7888848791050164
 
----------
Epoch 19/40
time = 877.22 secondes

Train loss 0.018483346778540747 micro_f1_score 0.9821708089947695 
 
time = 30.67 secondes

Val loss 0.3008203067984737 micro_f1_score 0.7826398852223816
 
----------
Epoch 20/40
time = 876.80 secondes

Train loss 0.01654667365862904 micro_f1_score 0.9838469469584146 
 
time = 33.37 secondes

Val loss 0.2993594731708042 micro_f1_score 0.7882479398065209
 
----------
Epoch 21/40
time = 878.96 secondes

Train loss 0.01499418314957471 micro_f1_score 0.9848103198229142 
 
time = 30.79 secondes

Val loss 0.3180460341152598 micro_f1_score 0.7811400422237861
 
----------
Epoch 22/40
time = 878.97 secondes

Train loss 0.013123985185646577 micro_f1_score 0.986877241168841 
 
time = 30.60 secondes

Val loss 0.3139876002659563 micro_f1_score 0.780752532561505
 
----------
Epoch 23/40
time = 876.94 secondes

Train loss 0.012761175603320537 micro_f1_score 0.986388073359515 
 
time = 31.03 secondes

Val loss 0.32392441957700446 micro_f1_score 0.7827348567283279
 
----------
Epoch 24/40
time = 876.88 secondes

Train loss 0.010229138940256005 micro_f1_score 0.9900560064007314 
 
time = 33.39 secondes

Val loss 0.31661946619631814 micro_f1_score 0.790316573556797
 
----------
Epoch 25/40
time = 878.01 secondes

Train loss 0.009712558877806447 micro_f1_score 0.9902956958556912 
 
time = 31.39 secondes

Val loss 0.34132199233672655 micro_f1_score 0.7761304670126019
 
----------
Epoch 26/40
time = 877.40 secondes

Train loss 0.009487748635499654 micro_f1_score 0.9908193973562912 
 
time = 31.18 secondes

Val loss 0.32988816696661905 micro_f1_score 0.7839233038348083
 
----------
Epoch 27/40
time = 880.28 secondes

Train loss 0.008975151044170808 micro_f1_score 0.9910534130277534 
 
time = 33.63 secondes

Val loss 0.3636014228228663 micro_f1_score 0.7735708982925019
 
----------
Epoch 28/40
time = 878.34 secondes

Train loss 0.007591702035383892 micro_f1_score 0.9923989054423836 
 
time = 31.62 secondes

Val loss 0.3582256331306989 micro_f1_score 0.7861866274797943
 
----------
Epoch 29/40
time = 877.11 secondes

Train loss 0.007531545859950842 micro_f1_score 0.9924400714204308 
 
time = 33.99 secondes

Val loss 0.3501547264759658 micro_f1_score 0.7888647866955893
 
----------
Epoch 30/40
time = 875.73 secondes

Train loss 0.006066443058420895 micro_f1_score 0.9939509225794179 
 
time = 33.03 secondes

Val loss 0.3531698990063589 micro_f1_score 0.7862567811934901
 
----------
Epoch 31/40
time = 877.13 secondes

Train loss 0.00624648920529908 micro_f1_score 0.9940251931346806 
 
time = 34.25 secondes

Val loss 0.3516444467374536 micro_f1_score 0.7862932940309506
 
----------
Epoch 32/40
time = 876.33 secondes

Train loss 0.004644903002649248 micro_f1_score 0.9952851711026617 
 
time = 31.12 secondes

Val loss 0.36580054107748095 micro_f1_score 0.7846924177396281
 
----------
Epoch 33/40
time = 877.56 secondes

Train loss 0.004538974980377961 micro_f1_score 0.9952866048350312 
 
time = 30.99 secondes

Val loss 0.36251382517521497 micro_f1_score 0.7889775199419868
 
----------
Epoch 34/40
time = 877.29 secondes

Train loss 0.004425227284442732 micro_f1_score 0.9955896889970344 
 
time = 31.46 secondes

Val loss 0.3755475645426844 micro_f1_score 0.7832422586520947
 
----------
Epoch 35/40
time = 878.54 secondes

Train loss 0.002869785135250847 micro_f1_score 0.9970369244795624 
 
time = 30.74 secondes

Val loss 0.36825150880412977 micro_f1_score 0.7968127490039841
 
----------
Epoch 36/40
time = 878.47 secondes

Train loss 0.002658630447603263 micro_f1_score 0.9973384030418251 
 
time = 37.25 secondes

Val loss 0.3676569012344861 micro_f1_score 0.7968470082407739
 
----------
Epoch 37/40
time = 877.62 secondes

Train loss 0.002798645164515259 micro_f1_score 0.9974914481185861 
 
time = 30.73 secondes

Val loss 0.3877834305167198 micro_f1_score 0.7852804573061807
 
----------
Epoch 38/40
time = 876.09 secondes

Train loss 0.0016882955681196748 micro_f1_score 0.9984425451092117 
 
time = 34.48 secondes

Val loss 0.3840518096675638 micro_f1_score 0.7898498927805575
 
----------
Epoch 39/40
time = 877.70 secondes

Train loss 0.00157487474596335 micro_f1_score 0.9987461529693377 
 
time = 34.28 secondes

Val loss 0.38403427124512 micro_f1_score 0.789665211062591
 
----------
Epoch 40/40
time = 878.94 secondes

Train loss 0.001245587506488297 micro_f1_score 0.9990121580547112 
 
time = 30.98 secondes

Val loss 0.3932526923838209 micro_f1_score 0.7917271407837446
 
----------
best_f1_socre 0.7968470082407739 best_epoch 36

average train time 874.7614018678665

average val time 31.565739154815674
 
time = 33.13 secondes

test_f1_score 0.7608391608391609

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_512_2
----------
Epoch 1/40
time = 1707.44 secondes

Train loss 0.22775330113263817 micro_f1_score 0.681545809813287 
 
time = 37.96 secondes

Val loss 0.19011288075173488 micro_f1_score 0.7198428290766208
 
----------
Epoch 2/40
time = 1708.40 secondes

Train loss 0.1533682675243498 micro_f1_score 0.7997406596968961 
 
time = 38.31 secondes

Val loss 0.17908229361303518 micro_f1_score 0.7561260210035007
 
----------
Epoch 3/40
time = 1708.93 secondes

Train loss 0.1275878915479323 micro_f1_score 0.8419414065618264 
 
time = 39.34 secondes

Val loss 0.16699423934104013 micro_f1_score 0.7757437070938216
 
----------
Epoch 4/40
time = 1707.35 secondes

Train loss 0.1120512650860054 micro_f1_score 0.8669310071371926 
 
time = 37.98 secondes

Val loss 0.17034544453757708 micro_f1_score 0.7874074074074074
 
----------
Epoch 5/40
time = 1706.61 secondes

Train loss 0.10185589490978568 micro_f1_score 0.880528883991815 
 
time = 36.38 secondes

Val loss 0.1763499097623786 micro_f1_score 0.7832116788321167
 
----------
Epoch 6/40
time = 1706.80 secondes

Train loss 0.09037993533352205 micro_f1_score 0.899482109227872 
 
time = 38.28 secondes

Val loss 0.180785707396562 micro_f1_score 0.7766497461928934
 
----------
Epoch 7/40
time = 1709.58 secondes

Train loss 0.08038442934116533 micro_f1_score 0.9101726094954794 
 
time = 36.95 secondes

Val loss 0.18831456073972044 micro_f1_score 0.791651673263764
 
----------
Epoch 8/40
time = 1706.25 secondes

Train loss 0.0714118233688914 micro_f1_score 0.9241008874357777 
 
time = 36.85 secondes

Val loss 0.19139016792178154 micro_f1_score 0.7931158121190391
 
----------
Epoch 9/40
time = 1705.78 secondes

Train loss 0.06203503064257471 micro_f1_score 0.9351192319603593 
 
time = 37.09 secondes

Val loss 0.2072946827431194 micro_f1_score 0.7810480976310121
 
----------
Epoch 10/40
time = 1706.08 secondes

Train loss 0.05394854877009853 micro_f1_score 0.944493800455792 
 
time = 36.83 secondes

Val loss 0.20543500243640336 micro_f1_score 0.7984274481772694
 
----------
Epoch 11/40
time = 1706.74 secondes

Train loss 0.04835435138125946 micro_f1_score 0.9510936537276649 
 
time = 36.84 secondes

Val loss 0.2285622321679944 micro_f1_score 0.7843275679491705
 
----------
Epoch 12/40
time = 1706.88 secondes

Train loss 0.04291725437668664 micro_f1_score 0.9575557009273867 
 
time = 36.79 secondes

Val loss 0.23905216053616804 micro_f1_score 0.7800917755030004
 
----------
Epoch 13/40
time = 1705.18 secondes

Train loss 0.038550552397731454 micro_f1_score 0.9608318890814559 
 
time = 37.02 secondes

Val loss 0.23539287169448664 micro_f1_score 0.7959405581732512
 
----------
Epoch 14/40
time = 1706.21 secondes

Train loss 0.034043970019011216 micro_f1_score 0.9672735653376354 
 
time = 37.41 secondes

Val loss 0.2496856870587732 micro_f1_score 0.7919746568109821
 
----------
Epoch 15/40
time = 1707.01 secondes

Train loss 0.03028154614117198 micro_f1_score 0.9704269111273062 
 
time = 36.89 secondes

Val loss 0.2703820826088796 micro_f1_score 0.783916083916084
 
----------
Epoch 16/40
time = 1706.97 secondes

Train loss 0.026403265828812108 micro_f1_score 0.974527652413371 
 
time = 36.77 secondes

Val loss 0.29072225570190147 micro_f1_score 0.7743792731198272
 
----------
Epoch 17/40
time = 1706.83 secondes

Train loss 0.023811273808750484 micro_f1_score 0.9754864409376437 
 
time = 36.78 secondes

Val loss 0.2741035722562524 micro_f1_score 0.7864112757499095
 
----------
Epoch 18/40
time = 1707.21 secondes

Train loss 0.021921895940154625 micro_f1_score 0.9781545982279255 
 
time = 36.85 secondes

Val loss 0.2758983373641968 micro_f1_score 0.7929799426934098
 
----------
Epoch 19/40
time = 1707.11 secondes

Train loss 0.019802010443946767 micro_f1_score 0.9799831919932768 
 
time = 37.52 secondes

Val loss 0.28602351358190914 micro_f1_score 0.7909806728704366
 
----------
Epoch 20/40
time = 1707.90 secondes

Train loss 0.01725597860971214 micro_f1_score 0.9830055375214817 
 
time = 37.32 secondes

Val loss 0.2848552195507972 micro_f1_score 0.7979651162790697
 
----------
Epoch 21/40
time = 1707.86 secondes

Train loss 0.015345385223983678 micro_f1_score 0.9844286695672085 
 
time = 36.77 secondes

Val loss 0.29414544144614796 micro_f1_score 0.7961666052340584
 
----------
Epoch 22/40
time = 1706.63 secondes

Train loss 0.013752606594966523 micro_f1_score 0.9857306371613888 
 
time = 35.63 secondes

Val loss 0.30231349841981636 micro_f1_score 0.7930910399424254
 
----------
Epoch 23/40
time = 1703.92 secondes

Train loss 0.013128681009612255 micro_f1_score 0.9863984455366327 
 
time = 35.52 secondes

Val loss 0.3139091055901324 micro_f1_score 0.7896440129449837
 
----------
Epoch 24/40
time = 1701.95 secondes

Train loss 0.010585597538869609 micro_f1_score 0.988985021153333 
 
time = 35.59 secondes

Val loss 0.32423298525028543 micro_f1_score 0.7904451682953312
 
----------
Epoch 25/40
time = 1702.67 secondes

Train loss 0.010541619426940006 micro_f1_score 0.9892661388550548 
 
time = 35.79 secondes

Val loss 0.33177988243396167 micro_f1_score 0.7883636363636363
 
----------
Epoch 26/40
time = 1702.20 secondes

Train loss 0.009784227154880493 micro_f1_score 0.990590834634871 
 
time = 35.58 secondes

Val loss 0.33420381108756925 micro_f1_score 0.7837837837837838
 
----------
Epoch 27/40
time = 1704.30 secondes

Train loss 0.009415281359588744 micro_f1_score 0.9908620164483705 
 
time = 35.63 secondes

Val loss 0.34221998028090744 micro_f1_score 0.7908473364318913
 
----------
Epoch 28/40
time = 1703.53 secondes

Train loss 0.008604867205989698 micro_f1_score 0.9917298677541064 
 
time = 35.81 secondes

Val loss 0.3418105407083621 micro_f1_score 0.7840290381125226
 
----------
Epoch 29/40
time = 1703.79 secondes

Train loss 0.00733285809225751 micro_f1_score 0.9923896499238964 
 
time = 36.03 secondes

Val loss 0.36478366824935693 micro_f1_score 0.7814327485380118
 
----------
Epoch 30/40
time = 1702.13 secondes

Train loss 0.006919996983852544 micro_f1_score 0.9928117749971475 
 
time = 36.49 secondes

Val loss 0.35300366543844097 micro_f1_score 0.7906306306306305
 
----------
Epoch 31/40
time = 1701.50 secondes

Train loss 0.0059321685184316655 micro_f1_score 0.9937250427837993 
 
time = 35.86 secondes

Val loss 0.366286606817949 micro_f1_score 0.785244704163623
 
----------
Epoch 32/40
time = 1702.12 secondes

Train loss 0.004933621762265338 micro_f1_score 0.9945238819592334 
 
time = 35.83 secondes

Val loss 0.37823236825280504 micro_f1_score 0.7802237459400937
 
----------
Epoch 33/40
time = 1702.71 secondes

Train loss 0.004904019510948208 micro_f1_score 0.9950570342205323 
 
time = 35.96 secondes

Val loss 0.359065194965386 micro_f1_score 0.7894736842105264
 
----------
Epoch 34/40
time = 1703.25 secondes

Train loss 0.004273539194557973 micro_f1_score 0.9960474308300394 
 
time = 36.03 secondes

Val loss 0.38300656081467377 micro_f1_score 0.7894546767786205
 
----------
Epoch 35/40
time = 1704.44 secondes

Train loss 0.003481355519132292 micro_f1_score 0.9966539923954372 
 
time = 36.03 secondes

Val loss 0.373415363677701 micro_f1_score 0.7866138898884492
 
----------
Epoch 36/40
time = 1704.62 secondes

Train loss 0.0029825579417242275 micro_f1_score 0.9974164133738601 
 
time = 35.57 secondes

Val loss 0.36626274761606437 micro_f1_score 0.7935251798561153
 
----------
Epoch 37/40
time = 1704.77 secondes

Train loss 0.0027574109152598654 micro_f1_score 0.997226970560304 
 
time = 35.70 secondes

Val loss 0.37137024238949917 micro_f1_score 0.7952415284787311
 
----------
Epoch 38/40
time = 1705.15 secondes

Train loss 0.0018442060467579952 micro_f1_score 0.9982140821522211 
 
time = 35.63 secondes

Val loss 0.3667045537931997 micro_f1_score 0.7976794778825237
 
----------
Epoch 39/40
time = 1704.82 secondes

Train loss 0.0020161502818400795 micro_f1_score 0.9980629723878612 
 
time = 35.56 secondes

Val loss 0.3736267546039136 micro_f1_score 0.7919034090909091
 
----------
Epoch 40/40
time = 1703.24 secondes

Train loss 0.0013791221572515694 micro_f1_score 0.9989364934670313 
 
time = 35.94 secondes

Val loss 0.37629041179526046 micro_f1_score 0.7956989247311829
 
----------
best_f1_socre 0.7984274481772694 best_epoch 10

average train time 1705.4209457576276

average val time 36.5772416472435
 
time = 37.98 secondes

test_f1_score 0.7899511514305653

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_256_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 768.00 MiB (GPU 1; 79.20 GiB total capacity; 72.05 GiB already allocated; 651.31 MiB free; 75.99 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_512_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 1.51 GiB (GPU 1; 79.20 GiB total capacity; 72.41 GiB already allocated; 897.31 MiB free; 75.75 GiB reserved in total by PyTorch)
datasets imported
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_64_3
----------
Epoch 1/40
time = 1129.62 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Train loss 0.2648409375467816 micro_f1_score 0.628394036115605 
 
time = 53.89 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss 0.2234303103607209 micro_f1_score 0.6664025356576861
 
----------
Epoch 2/40
time = 1162.60 secondes

Train loss 0.16789194362926055 micro_f1_score 0.7728860936408107 
 
time = 53.19 secondes

Val loss 0.19047076853572345 micro_f1_score 0.7296340023612751
 
----------
Epoch 3/40
time = 1155.59 secondes

Train loss 0.14433847122095728 micro_f1_score 0.8132864020714488 
 
time = 50.50 secondes

Val loss 0.19197111479083045 micro_f1_score 0.7325448302174743
 
----------
Epoch 4/40
time = 1136.63 secondes

Train loss 0.1250781830417009 micro_f1_score 0.8443144951335764 
 
time = 51.08 secondes

Val loss 0.18513033148206648 micro_f1_score 0.760342899739098
 
----------
Epoch 5/40
time = 1161.23 secondes

Train loss 0.1086894010739015 micro_f1_score 0.8695305968069436 
 
time = 51.43 secondes

Val loss 0.18920828231045458 micro_f1_score 0.7721198988804623
 
----------
Epoch 6/40
time = 1151.06 secondes

Train loss 0.09342165526740992 micro_f1_score 0.8923513865330756 
 
time = 46.39 secondes

Val loss 0.21189981698989868 micro_f1_score 0.7676767676767677
 
----------
Epoch 7/40
time = 1115.19 secondes

Train loss 0.08030302092130925 micro_f1_score 0.9092408592860228 
 
time = 47.43 secondes

Val loss 0.23759667147867014 micro_f1_score 0.755
 
----------
Epoch 8/40
time = 1122.61 secondes

Train loss 0.06679521756548737 micro_f1_score 0.9287188071787287 
 
time = 51.66 secondes

Val loss 0.2473857051036397 micro_f1_score 0.7509051412020274
 
----------
Epoch 9/40
time = 1114.36 secondes

Train loss 0.05823678757601023 micro_f1_score 0.938523635799115 
 
time = 48.20 secondes

Val loss 0.2534240133693961 micro_f1_score 0.7625320160995244
 
----------
Epoch 10/40
time = 1165.34 secondes

Train loss 0.04888296297894002 micro_f1_score 0.9488418854645991 
 
time = 47.17 secondes

Val loss 0.2722700291969737 micro_f1_score 0.7619749447310243
 
----------
Epoch 11/40
time = 1120.41 secondes

Train loss 0.03998154405135292 micro_f1_score 0.9595866111368193 
 
time = 54.99 secondes

Val loss 0.28695547995997256 micro_f1_score 0.7641339575081022
 
----------
Epoch 12/40
time = 1165.53 secondes

Train loss 0.03401244434649712 micro_f1_score 0.9648778611271399 
 
time = 52.03 secondes

Val loss 0.3041346401709025 micro_f1_score 0.7532929868280526
 
----------
Epoch 13/40
time = 1284.61 secondes

Train loss 0.029236542282999402 micro_f1_score 0.9711133246494522 
 
time = 44.24 secondes

Val loss 0.32771615244326047 micro_f1_score 0.7510699001426534
 
----------
Epoch 14/40
time = 1266.31 secondes

Train loss 0.025265988331342637 micro_f1_score 0.9743825036323316 
 
time = 46.25 secondes

Val loss 0.3439557795397571 micro_f1_score 0.7600574712643678
 
----------
Epoch 15/40
time = 1256.17 secondes

Train loss 0.023231768639189788 micro_f1_score 0.9755127027854301 
 
time = 52.28 secondes

Val loss 0.3620942932416181 micro_f1_score 0.7565308254963429
 
----------
Epoch 16/40
time = 1251.82 secondes

Train loss 0.019027234815891733 micro_f1_score 0.979766358708101 
 
time = 46.66 secondes

Val loss 0.37292744390300064 micro_f1_score 0.753840657377635
 
----------
Epoch 17/40
time = 1274.59 secondes

Train loss 0.016253434284590185 micro_f1_score 0.9825645721262065 
 
time = 51.60 secondes

Val loss 0.3899845236637553 micro_f1_score 0.7558386411889597
 
----------
Epoch 18/40
time = 1256.50 secondes

Train loss 0.014910094067926292 micro_f1_score 0.9850028620492272 
 
time = 43.56 secondes

Val loss 0.4053216692609865 micro_f1_score 0.7567949170490647
 
----------
Epoch 19/40
time = 1276.63 secondes

Train loss 0.012574441138988462 micro_f1_score 0.9865945616573997 
 
time = 47.27 secondes

Val loss 0.41508377416700615 micro_f1_score 0.758694109297374
 
----------
Epoch 20/40
time = 1185.50 secondes

Train loss 0.012131120960518134 micro_f1_score 0.9870940724102486 
 
time = 54.47 secondes

Val loss 0.42463056437793323 micro_f1_score 0.7565836298932385
 
----------
Epoch 21/40
time = 1132.97 secondes

Train loss 0.011786159846015581 micro_f1_score 0.9874448333586971 
 
time = 43.63 secondes

Val loss 0.471598317510769 micro_f1_score 0.7509996364958196
 
----------
Epoch 22/40
time = 1110.89 secondes

Train loss 0.010683495012782662 micro_f1_score 0.9888829665727557 
 
time = 45.94 secondes

Val loss 0.46242010837695635 micro_f1_score 0.7561785319070453
 
----------
Epoch 23/40
time = 1126.10 secondes

Train loss 0.009242587655125634 micro_f1_score 0.9911389998098499 
 
time = 51.51 secondes

Val loss 0.48502031640439736 micro_f1_score 0.7498160412067697
 
----------
Epoch 24/40
time = 1132.48 secondes

Train loss 0.009627243854419584 micro_f1_score 0.9910225197808886 
 
time = 46.04 secondes

Val loss 0.45356028185027547 micro_f1_score 0.7625847911460192
 
----------
Epoch 25/40
time = 1132.98 secondes

Train loss 0.008330912511529105 micro_f1_score 0.991640066879465 
 
time = 54.02 secondes

Val loss 0.5003917898799553 micro_f1_score 0.7477638640429337
 
----------
Epoch 26/40
time = 1130.67 secondes

Train loss 0.007482697908975801 micro_f1_score 0.9922474728281523 
 
time = 50.54 secondes

Val loss 0.49932877108698986 micro_f1_score 0.7497275699237196
 
----------
Epoch 27/40
time = 1119.68 secondes

Train loss 0.00649616101775439 micro_f1_score 0.9940688920994601 
 
time = 43.79 secondes

Val loss 0.5034444718087305 micro_f1_score 0.7479323984178354
 
----------
Epoch 28/40
time = 1289.62 secondes

Train loss 0.006875336296962069 micro_f1_score 0.9939933090024331 
 
time = 47.77 secondes

Val loss 0.4840403513097372 micro_f1_score 0.7593423019431987
 
----------
Epoch 29/40
time = 1189.13 secondes

Train loss 0.006476398751666394 micro_f1_score 0.9936834094368341 
 
time = 39.93 secondes

Val loss 0.4923139033747501 micro_f1_score 0.7524825303420375
 
----------
Epoch 30/40
time = 1289.07 secondes

Train loss 0.005273739362541868 micro_f1_score 0.9947910725827915 
 
time = 38.73 secondes

Val loss 0.48185549358852575 micro_f1_score 0.757163323782235
 
----------
Epoch 31/40
time = 1264.30 secondes

Train loss 0.004912750404528688 micro_f1_score 0.9953990646032168 
 
time = 39.94 secondes

Val loss 0.5022015315098841 micro_f1_score 0.7608773822366056
 
----------
Epoch 32/40
time = 1273.00 secondes

Train loss 0.003998558509998917 micro_f1_score 0.9962003191731895 
 
time = 42.27 secondes

Val loss 0.5084938685424992 micro_f1_score 0.7613183629119884
 
----------
Epoch 33/40
time = 1303.67 secondes

Train loss 0.003640628739086825 micro_f1_score 0.9968842617220154 
 
time = 43.02 secondes

Val loss 0.5168898848236584 micro_f1_score 0.7621410847217298
 
----------
Epoch 34/40
time = 1366.62 secondes

Train loss 0.0027804024536955104 micro_f1_score 0.9970759123533208 
 
time = 41.46 secondes

Val loss 0.5140085704013949 micro_f1_score 0.7604477611940297
 
----------
Epoch 35/40
time = 1310.69 secondes

Train loss 0.0024806094039786742 micro_f1_score 0.9975676497415628 
 
time = 42.31 secondes

Val loss 0.5390329866624269 micro_f1_score 0.7579479768786127
 
----------
Epoch 36/40
time = 1245.79 secondes

Train loss 0.0023397002957477393 micro_f1_score 0.9979874691475225 
 
time = 40.22 secondes

Val loss 0.5424679928138608 micro_f1_score 0.7606247729749365
 
----------
Epoch 37/40
time = 1248.46 secondes

Train loss 0.0014858813711156357 micro_f1_score 0.9984430182660541 
 
time = 41.39 secondes

Val loss 0.546140762870429 micro_f1_score 0.7641747923438064
 
----------
Epoch 38/40
time = 1244.29 secondes

Train loss 0.0014002682871922605 micro_f1_score 0.9987085992099666 
 
time = 39.97 secondes

Val loss 0.5532429105922824 micro_f1_score 0.7637145930441017
 
----------
Epoch 39/40
time = 1150.74 secondes

Train loss 0.0008556832291335242 micro_f1_score 0.9989745147935737 
 
time = 39.41 secondes

Val loss 0.5482667064080473 micro_f1_score 0.762217359591539
 
----------
Epoch 40/40
time = 1175.67 secondes

Train loss 0.0006191669837171339 micro_f1_score 0.9993921896368333 
 
time = 41.54 secondes

Val loss 0.5587277107062887 micro_f1_score 0.7626208378088076
 
----------
best_f1_socre 0.7721198988804623 best_epoch 5

average train time 1199.7282265484332

average val time 46.692490118741986
 
time = 39.55 secondes

test_f1_score 0.7627178939879048

----------
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_128_3
----------
Epoch 1/40
Attention type 'block_sparse' is not possible if sequence_length: 1024 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 1408 with config.block_size = 128, config.num_random_blocks = 3. Changing attention type to 'original_full'...
time = 458.88 secondes

Train loss 0.2567840816872614 micro_f1_score 0.6385217275982495 
 
time = 16.95 secondes

Val loss 0.2245809557985087 micro_f1_score 0.6481632653061223
 
----------
Epoch 2/40
time = 458.31 secondes

Train loss 0.16896172983420862 micro_f1_score 0.769895126465145 
 
time = 17.01 secondes

Val loss 0.19371901440327285 micro_f1_score 0.7301103920822231
 
----------
Epoch 3/40
time = 458.52 secondes

Train loss 0.14575650799717452 micro_f1_score 0.8115494103294022 
 
time = 17.00 secondes

Val loss 0.1873829925402266 micro_f1_score 0.7481259370314843
 
----------
Epoch 4/40
time = 457.77 secondes

Train loss 0.1272198222292302 micro_f1_score 0.8394545235126111 
 
time = 17.04 secondes

Val loss 0.1901981858689277 micro_f1_score 0.7510204081632653
 
----------
Epoch 5/40
time = 458.44 secondes

Train loss 0.11296128281434109 micro_f1_score 0.8609160548994574 
 
time = 17.13 secondes

Val loss 0.18984945228353875 micro_f1_score 0.7610226009633198
 
----------
Epoch 6/40
time = 458.09 secondes

Train loss 0.10061228985285706 micro_f1_score 0.8786908573461402 
 
time = 17.17 secondes

Val loss 0.2003627623446652 micro_f1_score 0.7527887729399064
 
----------
Epoch 7/40
time = 458.42 secondes

Train loss 0.087686680702845 micro_f1_score 0.8986271193108061 
 
time = 17.07 secondes

Val loss 0.20828176509650026 micro_f1_score 0.7550229115262601
 
----------
Epoch 8/40
time = 458.32 secondes

Train loss 0.07874206943051504 micro_f1_score 0.909276009240769 
 
time = 17.08 secondes

Val loss 0.23007006051599002 micro_f1_score 0.7576601671309192
 
----------
Epoch 9/40
time = 458.78 secondes

Train loss 0.0683259568273652 micro_f1_score 0.9219125979973609 
 
time = 17.84 secondes

Val loss 0.2563998308338103 micro_f1_score 0.7483870967741937
 
----------
Epoch 10/40
time = 458.23 secondes

Train loss 0.05986913718987961 micro_f1_score 0.9326525086070172 
 
time = 17.63 secondes

Val loss 0.2785747655835308 micro_f1_score 0.7414772727272728
 
----------
Epoch 11/40
time = 458.12 secondes

Train loss 0.05064805533846018 micro_f1_score 0.9455403419200124 
 
time = 17.50 secondes

Val loss 0.28616157271822945 micro_f1_score 0.7523709167544783
 
----------
Epoch 12/40
time = 459.03 secondes

Train loss 0.04342069927359688 micro_f1_score 0.9523076923076922 
 
time = 17.31 secondes

Val loss 0.3241760791813741 micro_f1_score 0.7381882770870337
 
----------
Epoch 13/40
time = 458.55 secondes

Train loss 0.03704650438420038 micro_f1_score 0.9603781561046847 
 
time = 18.47 secondes

Val loss 0.3584364617212874 micro_f1_score 0.7311304347826086
 
----------
Epoch 14/40
time = 457.97 secondes

Train loss 0.032544408023865004 micro_f1_score 0.9657234874626179 
 
time = 17.83 secondes

Val loss 0.3582024220071855 micro_f1_score 0.7356643356643355
 
----------
Epoch 15/40
time = 458.19 secondes

Train loss 0.027721921591400713 micro_f1_score 0.9704418408760241 
 
time = 17.79 secondes

Val loss 0.3647922531503146 micro_f1_score 0.7412831241283125
 
----------
Epoch 16/40
time = 458.30 secondes

Train loss 0.023624440236226864 micro_f1_score 0.9734296746568796 
 
time = 17.72 secondes

Val loss 0.3686754718178608 micro_f1_score 0.7519826964671954
 
----------
Epoch 17/40
time = 458.45 secondes

Train loss 0.022035850260064827 micro_f1_score 0.9769113149847095 
 
time = 17.82 secondes

Val loss 0.3962201752135011 micro_f1_score 0.7500898957209636
 
----------
Epoch 18/40
time = 458.66 secondes

Train loss 0.01959165417075866 micro_f1_score 0.9790337979759405 
 
time = 17.45 secondes

Val loss 0.4125624245307485 micro_f1_score 0.7482695810564662
 
----------
Epoch 19/40
time = 458.36 secondes

Train loss 0.01704081145184941 micro_f1_score 0.9818667684672647 
 
time = 17.83 secondes

Val loss 0.4094763282136839 micro_f1_score 0.7500898957209636
 
----------
Epoch 20/40
time = 458.28 secondes

Train loss 0.015895835849192196 micro_f1_score 0.9836716007935298 
 
time = 17.69 secondes

Val loss 0.42335039792490786 micro_f1_score 0.7537473233404711
 
----------
Epoch 21/40
time = 458.04 secondes

Train loss 0.013818019560773161 micro_f1_score 0.9854607899255867 
 
time = 17.74 secondes

Val loss 0.46196949115542113 micro_f1_score 0.7391455366446683
 
----------
Epoch 22/40
time = 458.41 secondes

Train loss 0.013646675788494338 micro_f1_score 0.985917637207886 
 
time = 18.12 secondes

Val loss 0.4505369646138832 micro_f1_score 0.7488584474885844
 
----------
Epoch 23/40
time = 458.07 secondes

Train loss 0.01208766880078107 micro_f1_score 0.9868260737130672 
 
time = 17.17 secondes

Val loss 0.459931132001955 micro_f1_score 0.7507930912936199
 
----------
Epoch 24/40
time = 458.03 secondes

Train loss 0.010828459504331034 micro_f1_score 0.9884208120667327 
 
time = 17.61 secondes

Val loss 0.48998516651450613 micro_f1_score 0.7463432037103104
 
----------
Epoch 25/40
time = 458.82 secondes

Train loss 0.009334452756494914 micro_f1_score 0.9902927404925959 
 
time = 17.56 secondes

Val loss 0.4912801518059168 micro_f1_score 0.7500912075884714
 
----------
Epoch 26/40
time = 458.01 secondes

Train loss 0.00957377663797013 micro_f1_score 0.9903370615536788 
 
time = 18.05 secondes

Val loss 0.4719065818630281 micro_f1_score 0.7567959405581732
 
----------
Epoch 27/40
time = 458.57 secondes

Train loss 0.00828577274860272 micro_f1_score 0.9913248611216803 
 
time = 17.64 secondes

Val loss 0.5027217116878658 micro_f1_score 0.7551537070524412
 
----------
Epoch 28/40
time = 458.23 secondes

Train loss 0.008390967147919058 micro_f1_score 0.9911860800850999 
 
time = 17.83 secondes

Val loss 0.4790531903749607 micro_f1_score 0.7646632874728458
 
----------
Epoch 29/40
time = 458.47 secondes

Train loss 0.006488247844987069 micro_f1_score 0.9935375959857067 
 
time = 17.76 secondes

Val loss 0.5221791225867193 micro_f1_score 0.7540751240255139
 
----------
Epoch 30/40
time = 459.28 secondes

Train loss 0.006013217763845475 micro_f1_score 0.9936533272526888 
 
time = 17.85 secondes

Val loss 0.5046686781722991 micro_f1_score 0.7578872740163063
 
----------
Epoch 31/40
time = 457.68 secondes

Train loss 0.005738748779251248 micro_f1_score 0.9948671153188092 
 
time = 17.01 secondes

Val loss 0.5277922993800679 micro_f1_score 0.7512653651482285
 
----------
Epoch 32/40
time = 458.26 secondes

Train loss 0.005086477733026628 micro_f1_score 0.9949446957314986 
 
time = 17.73 secondes

Val loss 0.5379531352979238 micro_f1_score 0.7573236889692585
 
----------
Epoch 33/40
time = 458.06 secondes

Train loss 0.00409678807567665 micro_f1_score 0.9962384589080132 
 
time = 17.22 secondes

Val loss 0.5498144230881675 micro_f1_score 0.7540628385698809
 
----------
Epoch 34/40
time = 458.33 secondes

Train loss 0.0035290361591840927 micro_f1_score 0.996085287522329 
 
time = 17.91 secondes

Val loss 0.548111238196248 micro_f1_score 0.7596460176991151
 
----------
Epoch 35/40
time = 458.34 secondes

Train loss 0.003291559354453909 micro_f1_score 0.9966178985369561 
 
time = 17.02 secondes

Val loss 0.5437652121801846 micro_f1_score 0.7632782272247626
 
----------
Epoch 36/40
time = 457.83 secondes

Train loss 0.002626026585007698 micro_f1_score 0.997492972726582 
 
time = 17.81 secondes

Val loss 0.5252822929474174 micro_f1_score 0.7654140824516601
 
----------
Epoch 37/40
time = 458.03 secondes

Train loss 0.0020478641473977125 micro_f1_score 0.9977961851204499 
 
time = 18.04 secondes

Val loss 0.5472620586391355 micro_f1_score 0.760662671836447
 
----------
Epoch 38/40
time = 458.02 secondes

Train loss 0.0018205496183859929 micro_f1_score 0.9981378026070764 
 
time = 17.85 secondes

Val loss 0.5443072387429534 micro_f1_score 0.7666905958363245
 
----------
Epoch 39/40
time = 458.14 secondes

Train loss 0.0011171477754270437 micro_f1_score 0.9989744368898849 
 
time = 17.02 secondes

Val loss 0.5588955798598586 micro_f1_score 0.769394261424017
 
----------
Epoch 40/40
time = 458.02 secondes

Train loss 0.0013597933539950555 micro_f1_score 0.9987466291921456 
 
time = 17.91 secondes

Val loss 0.5633743038920106 micro_f1_score 0.7653532126375577
 
----------
best_f1_socre 0.769394261424017 best_epoch 39

average train time 458.30705746412275

average val time 17.55489726662636
 
time = 18.90 secondes

test_f1_score 0.7521136286777139

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_64_3
----------
Epoch 1/40
time = 1723.45 secondes

Train loss 0.24238248079478203 micro_f1_score 0.6686380240041747 
 
time = 30.61 secondes

Val loss 0.20479827239865162 micro_f1_score 0.6915887850467289
 
----------
Epoch 2/40
time = 1739.78 secondes

Train loss 0.1530774692291612 micro_f1_score 0.7988797337338149 
 
time = 30.22 secondes

Val loss 0.18384846275458572 micro_f1_score 0.7445086705202312
 
----------
Epoch 3/40
time = 1734.54 secondes

Train loss 0.1306423542568007 micro_f1_score 0.8351145038167939 
 
time = 30.11 secondes

Val loss 0.16807785813437134 micro_f1_score 0.7743879472693033
 
----------
Epoch 4/40
time = 1736.49 secondes

Train loss 0.11449129623067271 micro_f1_score 0.8607765987043441 
 
time = 30.11 secondes

Val loss 0.18420698353257337 micro_f1_score 0.7663003663003662
 
----------
Epoch 5/40
time = 1740.28 secondes

Train loss 0.09655834966347561 micro_f1_score 0.8853611341468229 
 
time = 31.31 secondes

Val loss 0.18550661584881487 micro_f1_score 0.7828571428571429
 
----------
Epoch 6/40
time = 1731.84 secondes

Train loss 0.08329606131893826 micro_f1_score 0.9067923046721633 
 
time = 31.24 secondes

Val loss 0.1943922548509035 micro_f1_score 0.7877697841726617
 
----------
Epoch 7/40
Exception
CUDA out of memory. Tried to allocate 384.00 MiB (GPU 1; 79.20 GiB total capacity; 73.85 GiB already allocated; 86.31 MiB free; 75.57 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_128_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 1.12 GiB (GPU 1; 79.20 GiB total capacity; 73.60 GiB already allocated; 1.01 GiB free; 75.62 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_64_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 1; 79.20 GiB total capacity; 73.06 GiB already allocated; 133.31 MiB free; 76.49 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_128_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 2.62 GiB (GPU 1; 79.20 GiB total capacity; 72.99 GiB already allocated; 2.52 GiB free; 74.10 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_256_3
----------
Epoch 1/40
time = 536.04 secondes

Train loss 0.22944014991994377 micro_f1_score 0.6758732737611698 
 
time = 23.35 secondes

Val loss 0.20446506814389934 micro_f1_score 0.7061611374407584
 
----------
Epoch 2/40
time = 540.27 secondes

Train loss 0.1590426026096752 micro_f1_score 0.7840653728294177 
 
time = 22.89 secondes

Val loss 0.18190000719222865 micro_f1_score 0.7410889150019584
 
----------
Epoch 3/40
time = 543.24 secondes

Train loss 0.13752392684822684 micro_f1_score 0.824298160696999 
 
time = 23.62 secondes

Val loss 0.18266534829725983 micro_f1_score 0.7520976353928299
 
----------
Epoch 4/40
time = 536.84 secondes

Train loss 0.12131627888192197 micro_f1_score 0.8481817818138178 
 
time = 21.60 secondes

Val loss 0.19033011000175945 micro_f1_score 0.7392442947998504
 
----------
Epoch 5/40
time = 518.38 secondes

Train loss 0.10759907065143993 micro_f1_score 0.8707153354251898 
 
time = 21.75 secondes

Val loss 0.19266518816107608 micro_f1_score 0.7608616283315078
 
----------
Epoch 6/40
time = 514.48 secondes

Train loss 0.09474748752105075 micro_f1_score 0.8903281860905967 
 
time = 21.75 secondes

Val loss 0.19554085492110643 micro_f1_score 0.7675009067827349
 
----------
Epoch 7/40
time = 513.76 secondes

Train loss 0.08338645693619509 micro_f1_score 0.9057431638329546 
 
time = 21.62 secondes

Val loss 0.21600141205260012 micro_f1_score 0.7518796992481204
 
----------
Epoch 8/40
time = 515.70 secondes

Train loss 0.07239544676230834 micro_f1_score 0.9209807136722106 
 
time = 22.00 secondes

Val loss 0.2301105314590892 micro_f1_score 0.7554915376305366
 
----------
Epoch 9/40
time = 514.21 secondes

Train loss 0.06272157550546097 micro_f1_score 0.9333333333333332 
 
time = 21.56 secondes

Val loss 0.2557342461020243 micro_f1_score 0.7541995200548508
 
----------
Epoch 10/40
time = 517.32 secondes

Train loss 0.055549995542864675 micro_f1_score 0.9418464995544535 
 
time = 21.44 secondes

Val loss 0.2342261915079883 micro_f1_score 0.768562115175661
 
----------
Epoch 11/40
time = 513.82 secondes

Train loss 0.04843648599737601 micro_f1_score 0.9501180294880229 
 
time = 21.53 secondes

Val loss 0.24821690335625507 micro_f1_score 0.7606961566352428
 
----------
Epoch 12/40
time = 516.25 secondes

Train loss 0.041830625186615567 micro_f1_score 0.9575149527300791 
 
time = 21.51 secondes

Val loss 0.261160100581216 micro_f1_score 0.7627365356622998
 
----------
Epoch 13/40
time = 516.09 secondes

Train loss 0.03734646884909084 micro_f1_score 0.9626607129109247 
 
time = 21.59 secondes

Val loss 0.26416010134777085 micro_f1_score 0.7639391745112236
 
----------
Epoch 14/40
time = 516.08 secondes

Train loss 0.03193841107684683 micro_f1_score 0.9679132040627886 
 
time = 21.79 secondes

Val loss 0.2934687990145605 micro_f1_score 0.75421600287047
 
----------
Epoch 15/40
time = 517.25 secondes

Train loss 0.028540496835862662 micro_f1_score 0.9705036247171186 
 
time = 21.52 secondes

Val loss 0.28780947222572856 micro_f1_score 0.7665135994348286
 
----------
Epoch 16/40
time = 511.95 secondes

Train loss 0.026222198570633792 micro_f1_score 0.9741742662272971 
 
time = 21.58 secondes

Val loss 0.2924406285412976 micro_f1_score 0.7599271402550092
 
----------
Epoch 17/40
time = 514.05 secondes

Train loss 0.022186019016127732 micro_f1_score 0.9783847890125866 
 
time = 21.58 secondes

Val loss 0.3150977592487804 micro_f1_score 0.7518905293482174
 
----------
Epoch 18/40
time = 514.41 secondes

Train loss 0.020552597800794117 micro_f1_score 0.9790519877675841 
 
time = 21.59 secondes

Val loss 0.3253446113867838 micro_f1_score 0.7593070401769259
 
----------
Epoch 19/40
time = 515.67 secondes

Train loss 0.019390528242477122 micro_f1_score 0.9802362475629802 
 
time = 21.54 secondes

Val loss 0.3299242283721439 micro_f1_score 0.7610872675250357
 
----------
Epoch 20/40
time = 516.20 secondes

Train loss 0.016315062558026734 micro_f1_score 0.9831660113753483 
 
time = 22.03 secondes

Val loss 0.34739115895306477 micro_f1_score 0.7572274468826192
 
----------
Epoch 21/40
time = 509.86 secondes

Train loss 0.015119985667862804 micro_f1_score 0.9841584914303164 
 
time = 21.65 secondes

Val loss 0.32368876894966503 micro_f1_score 0.7627541919372101
 
----------
Epoch 22/40
time = 512.95 secondes

Train loss 0.013069496987553246 micro_f1_score 0.9872684302813143 
 
time = 21.76 secondes

Val loss 0.3470008672505129 micro_f1_score 0.7609791216702664
 
----------
Epoch 23/40
time = 512.57 secondes

Train loss 0.012367845051407998 micro_f1_score 0.9871041587180467 
 
time = 21.96 secondes

Val loss 0.37553654440113754 micro_f1_score 0.7493707299532543
 
----------
Epoch 24/40
time = 509.91 secondes

Train loss 0.011152681413312182 micro_f1_score 0.9882200449849415 
 
time = 21.81 secondes

Val loss 0.3643533577684496 micro_f1_score 0.7699496764917326
 
----------
Epoch 25/40
time = 508.94 secondes

Train loss 0.01042219050171437 micro_f1_score 0.9895305897133285 
 
time = 22.00 secondes

Val loss 0.3949886724108555 micro_f1_score 0.7527648947556189
 
----------
Epoch 26/40
time = 509.97 secondes

Train loss 0.008543555458419252 micro_f1_score 0.9915089669877775 
 
time = 21.63 secondes

Val loss 0.3711900322652254 micro_f1_score 0.7615918218327856
 
----------
Epoch 27/40
time = 509.09 secondes

Train loss 0.0088862099863593 micro_f1_score 0.9908396366262496 
 
time = 21.76 secondes

Val loss 0.39200237234596347 micro_f1_score 0.7489300998573467
 
----------
Epoch 28/40
time = 508.61 secondes

Train loss 0.007676852321349764 micro_f1_score 0.9924646064850053 
 
time = 21.49 secondes

Val loss 0.39367640482597666 micro_f1_score 0.757260666905701
 
----------
Epoch 29/40
time = 508.66 secondes

Train loss 0.006889088111413068 micro_f1_score 0.993075635367524 
 
time = 21.65 secondes

Val loss 0.38697735818683127 micro_f1_score 0.7614545454545455
 
----------
Epoch 30/40
time = 505.94 secondes

Train loss 0.00629801380696868 micro_f1_score 0.9936465664827848 
 
time = 21.40 secondes

Val loss 0.39545320353058516 micro_f1_score 0.7684021543985639
 
----------
Epoch 31/40
time = 508.32 secondes

Train loss 0.006288606265304413 micro_f1_score 0.9932717527654237 
 
time = 21.47 secondes

Val loss 0.40911734214083095 micro_f1_score 0.7621097954790097
 
----------
Epoch 32/40
time = 508.12 secondes

Train loss 0.005225601877772296 micro_f1_score 0.9949014534662508 
 
time = 21.49 secondes

Val loss 0.4142958916357306 micro_f1_score 0.7572254335260117
 
----------
Epoch 33/40
time = 507.36 secondes

Train loss 0.004112528616324935 micro_f1_score 0.9957055447877475 
 
time = 21.65 secondes

Val loss 0.4112973060519969 micro_f1_score 0.7654232424677189
 
----------
Epoch 34/40
time = 503.25 secondes

Train loss 0.005125826307525753 micro_f1_score 0.9949859454531641 
 
time = 21.51 secondes

Val loss 0.4147153396586903 micro_f1_score 0.7620427381383558
 
----------
Epoch 35/40
time = 507.80 secondes

Train loss 0.0032942276357160252 micro_f1_score 0.9963923593969545 
 
time = 21.37 secondes

Val loss 0.4198951059188999 micro_f1_score 0.7606557377049181
 
----------
Epoch 36/40
time = 506.14 secondes

Train loss 0.0024240435119773376 micro_f1_score 0.9978339350180506 
 
time = 21.44 secondes

Val loss 0.42643692149002044 micro_f1_score 0.760246644903881
 
----------
Epoch 37/40
time = 504.39 secondes

Train loss 0.002309133942401024 micro_f1_score 0.9978350867864333 
 
time = 21.53 secondes

Val loss 0.42231483452144214 micro_f1_score 0.7675981605942696
 
----------
Epoch 38/40
time = 507.73 secondes

Train loss 0.0022591463984713914 micro_f1_score 0.9980249164387724 
 
time = 21.50 secondes

Val loss 0.4243094678052136 micro_f1_score 0.7700035880875492
 
----------
Epoch 39/40
time = 502.45 secondes

Train loss 0.0017723449926625682 micro_f1_score 0.9985560115519077 
 
time = 21.36 secondes

Val loss 0.4166060677561604 micro_f1_score 0.7678442682047584
 
----------
Epoch 40/40
time = 506.45 secondes

Train loss 0.001270063847586825 micro_f1_score 0.9987459623788714 
 
time = 21.49 secondes

Val loss 0.4283793378071707 micro_f1_score 0.7664023071377073
 
----------
best_f1_socre 0.7700035880875492 best_epoch 38

average train time 514.0125499904156

average val time 21.743916231393815
 
time = 23.09 secondes

test_f1_score 0.7612408504705471

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_512_3
----------
Epoch 1/40
time = 643.87 secondes

Train loss 0.22391148868042068 micro_f1_score 0.6942661439710507 
 
time = 25.24 secondes

Val loss 0.2008829620040831 micro_f1_score 0.7193737769080234
 
----------
Epoch 2/40
time = 643.29 secondes

Train loss 0.15937395751946146 micro_f1_score 0.7890906117108276 
 
time = 24.56 secondes

Val loss 0.18082785117821615 micro_f1_score 0.7389937106918238
 
----------
Epoch 3/40
time = 640.55 secondes

Train loss 0.13714344205653614 micro_f1_score 0.8240961904376342 
 
time = 24.52 secondes

Val loss 0.18998622637791712 micro_f1_score 0.7391471379177872
 
----------
Epoch 4/40
time = 641.38 secondes

Train loss 0.12337626947408861 micro_f1_score 0.8475693049417437 
 
time = 24.46 secondes

Val loss 0.18975087047600356 micro_f1_score 0.7525347352609838
 
----------
Epoch 5/40
time = 639.57 secondes

Train loss 0.11110356063456149 micro_f1_score 0.865730023149996 
 
time = 24.42 secondes

Val loss 0.1946149649190121 micro_f1_score 0.7473997028231797
 
----------
Epoch 6/40
time = 642.47 secondes

Train loss 0.09855386757434488 micro_f1_score 0.8848812266624295 
 
time = 24.49 secondes

Val loss 0.19473635808366244 micro_f1_score 0.7661950856291884
 
----------
Epoch 7/40
time = 639.83 secondes

Train loss 0.08700152531115188 micro_f1_score 0.9000670162021523 
 
time = 24.40 secondes

Val loss 0.2048591272752793 micro_f1_score 0.7634525099313831
 
----------
Epoch 8/40
time = 638.08 secondes

Train loss 0.07676855012405294 micro_f1_score 0.9138161516080836 
 
time = 24.47 secondes

Val loss 0.21748138866463645 micro_f1_score 0.7657430730478588
 
----------
Epoch 9/40
time = 647.78 secondes

Train loss 0.06739579731481032 micro_f1_score 0.9277230817315205 
 
time = 24.95 secondes

Val loss 0.23887447560908365 micro_f1_score 0.7560801144492132
 
----------
Epoch 10/40
time = 665.23 secondes

Train loss 0.0591332204392398 micro_f1_score 0.9362399095269665 
 
time = 24.99 secondes

Val loss 0.2455879493815 micro_f1_score 0.7618379515959313
 
----------
Epoch 11/40
time = 669.03 secondes

Train loss 0.050794149902942884 micro_f1_score 0.946618114201596 
 
time = 24.84 secondes

Val loss 0.25629827954241485 micro_f1_score 0.7640769779044904
 
----------
Epoch 12/40
time = 674.21 secondes

Train loss 0.04426808233901455 micro_f1_score 0.9543382523371707 
 
time = 24.87 secondes

Val loss 0.2826064922770516 micro_f1_score 0.7523264137437365
 
----------
Epoch 13/40
time = 681.27 secondes

Train loss 0.03813280013247251 micro_f1_score 0.9612050699233347 
 
time = 25.07 secondes

Val loss 0.2832422688847683 micro_f1_score 0.757163323782235
 
----------
Epoch 14/40
time = 683.16 secondes

Train loss 0.035176773486147124 micro_f1_score 0.9638925244437602 
 
time = 25.09 secondes

Val loss 0.28763210712397685 micro_f1_score 0.7608232789212207
 
----------
Epoch 15/40
time = 681.27 secondes

Train loss 0.029337925957473886 micro_f1_score 0.9691315365123243 
 
time = 24.87 secondes

Val loss 0.29543512192417365 micro_f1_score 0.7682672233820459
 
----------
Epoch 16/40
time = 683.98 secondes

Train loss 0.02736216329933807 micro_f1_score 0.9717501148369315 
 
time = 24.92 secondes

Val loss 0.30041670640472506 micro_f1_score 0.7633642195295794
 
----------
Epoch 17/40
time = 684.59 secondes

Train loss 0.023990190489247006 micro_f1_score 0.9763628701681799 
 
time = 25.16 secondes

Val loss 0.31584034579210596 micro_f1_score 0.7531440891124686
 
----------
Epoch 18/40
time = 680.80 secondes

Train loss 0.0207594967896102 micro_f1_score 0.9782725116670493 
 
time = 25.20 secondes

Val loss 0.31971692489307435 micro_f1_score 0.7586933614330876
 
----------
Epoch 19/40
time = 683.19 secondes

Train loss 0.018638641008717084 micro_f1_score 0.9806742948987792 
 
time = 24.93 secondes

Val loss 0.33041467884036363 micro_f1_score 0.7583749109052031
 
----------
Epoch 20/40
time = 680.65 secondes

Train loss 0.016478623542934657 micro_f1_score 0.9830819171281269 
 
time = 24.93 secondes

Val loss 0.32064156258692506 micro_f1_score 0.7659574468085107
 
----------
Epoch 21/40
time = 680.69 secondes

Train loss 0.015959645402754095 micro_f1_score 0.9838506471194595 
 
time = 25.23 secondes

Val loss 0.32526739674513455 micro_f1_score 0.7680057908070937
 
----------
Epoch 22/40
time = 684.57 secondes

Train loss 0.015267922562758883 micro_f1_score 0.984162118841354 
 
time = 24.81 secondes

Val loss 0.33750538857745344 micro_f1_score 0.769116555308092
 
----------
Epoch 23/40
time = 680.15 secondes

Train loss 0.0138130071149774 micro_f1_score 0.9861779305078274 
 
time = 24.98 secondes

Val loss 0.3483300322636229 micro_f1_score 0.764386536373507
 
----------
Epoch 24/40
time = 680.63 secondes

Train loss 0.01088281294707469 micro_f1_score 0.9887906054598139 
 
time = 24.78 secondes

Val loss 0.357469773194829 micro_f1_score 0.7575539568345323
 
----------
Epoch 25/40
time = 682.03 secondes

Train loss 0.010162428312888789 micro_f1_score 0.9895650849264986 
 
time = 24.90 secondes

Val loss 0.38478513929199 micro_f1_score 0.7494631352899068
 
----------
Epoch 26/40
time = 679.30 secondes

Train loss 0.010374758431980562 micro_f1_score 0.989684442921853 
 
time = 24.98 secondes

Val loss 0.3783472863132836 micro_f1_score 0.7666900913562895
 
----------
Epoch 27/40
time = 680.60 secondes

Train loss 0.00907716258302612 micro_f1_score 0.9901804064854991 
 
time = 24.86 secondes

Val loss 0.3960912865204889 micro_f1_score 0.760662671836447
 
----------
Epoch 28/40
time = 681.26 secondes

Train loss 0.007096201228869341 micro_f1_score 0.9924983816305549 
 
time = 24.92 secondes

Val loss 0.39534329317632266 micro_f1_score 0.7592926741248647
 
----------
Epoch 29/40
time = 682.44 secondes

Train loss 0.006915606240815848 micro_f1_score 0.9932737982139465 
 
time = 24.74 secondes

Val loss 0.3870402728436423 micro_f1_score 0.7644444444444444
 
----------
Epoch 30/40
time = 677.28 secondes

Train loss 0.006144811838512888 micro_f1_score 0.9935047669692711 
 
time = 24.98 secondes

Val loss 0.4036832992903522 micro_f1_score 0.7592926741248647
 
----------
Epoch 31/40
time = 683.94 secondes

Train loss 0.006480770633487253 micro_f1_score 0.9935326789926195 
 
time = 24.83 secondes

Val loss 0.42526248390557336 micro_f1_score 0.7540511343176088
 
----------
Epoch 32/40
time = 683.15 secondes

Train loss 0.0053741650408220916 micro_f1_score 0.9947110079525131 
 
time = 25.40 secondes

Val loss 0.42628484891086327 micro_f1_score 0.7587687902648532
 
----------
Epoch 33/40
time = 684.62 secondes

Train loss 0.0042068023023275275 micro_f1_score 0.995816536091884 
 
time = 25.23 secondes

Val loss 0.42524108339528566 micro_f1_score 0.7625951431678144
 
----------
Epoch 34/40
time = 680.46 secondes

Train loss 0.003928571390127029 micro_f1_score 0.9961604257745675 
 
time = 24.88 secondes

Val loss 0.42452666016875723 micro_f1_score 0.7583454281567489
 
----------
Epoch 35/40
time = 685.64 secondes

Train loss 0.0034543592614034685 micro_f1_score 0.9966550098829254 
 
time = 25.41 secondes

Val loss 0.43006070837622784 micro_f1_score 0.7676912080057184
 
----------
Epoch 36/40
time = 684.76 secondes

Train loss 0.0035740326829532556 micro_f1_score 0.9965813264453391 
 
time = 24.86 secondes

Val loss 0.4129626489565021 micro_f1_score 0.7680057908070937
 
----------
Epoch 37/40
time = 679.06 secondes

Train loss 0.002862097998392225 micro_f1_score 0.9973400212798298 
 
time = 24.88 secondes

Val loss 0.41762635795796504 micro_f1_score 0.7696750902527075
 
----------
Epoch 38/40
time = 684.47 secondes

Train loss 0.002029934922029497 micro_f1_score 0.9981007369140774 
 
time = 24.75 secondes

Val loss 0.4397442003742593 micro_f1_score 0.7629446988376188
 
----------
Epoch 39/40
time = 682.93 secondes

Train loss 0.0018083608196419034 micro_f1_score 0.9984040127678978 
 
time = 25.01 secondes

Val loss 0.42191102880923476 micro_f1_score 0.767902123065851
 
----------
Epoch 40/40
time = 683.33 secondes

Train loss 0.001423411694456779 micro_f1_score 0.9988602689765216 
 
time = 24.94 secondes

Val loss 0.44011821292462894 micro_f1_score 0.7619047619047619
 
----------
best_f1_socre 0.7696750902527075 best_epoch 37

average train time 672.1382075548172

average val time 24.89387500882149
 
time = 27.38 secondes

test_f1_score 0.7538787023977432

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_256_3
----------
Epoch 1/40
time = 910.32 secondes

Train loss 0.22219659399476138 micro_f1_score 0.6986143679386059 
 
time = 30.32 secondes

Val loss 0.19023853845772196 micro_f1_score 0.7328901055924911
 
----------
Epoch 2/40
time = 906.87 secondes

Train loss 0.15333346542944243 micro_f1_score 0.7989786820134553 
 
time = 29.43 secondes

Val loss 0.16687907620531614 micro_f1_score 0.7675758739915481
 
----------
Epoch 3/40
time = 904.62 secondes

Train loss 0.12960268801732644 micro_f1_score 0.834727614778049 
 
time = 29.60 secondes

Val loss 0.17398052772537606 micro_f1_score 0.7715156130997715
 
----------
Epoch 4/40
time = 905.66 secondes

Train loss 0.11530928660419074 micro_f1_score 0.861694539113894 
 
time = 29.59 secondes

Val loss 0.16472775171526144 micro_f1_score 0.786607799852833
 
----------
Epoch 5/40
time = 908.06 secondes

Train loss 0.10162715531449329 micro_f1_score 0.8822532991924366 
 
time = 29.75 secondes

Val loss 0.17107505497873807 micro_f1_score 0.7868131868131868
 
----------
Epoch 6/40
time = 904.96 secondes

Train loss 0.09079020748602915 micro_f1_score 0.8961049159729857 
 
time = 29.64 secondes

Val loss 0.17580614751968227 micro_f1_score 0.7810781078107811
 
----------
Epoch 7/40
time = 904.79 secondes

Train loss 0.07928332410551406 micro_f1_score 0.9124218749999999 
 
time = 29.69 secondes

Val loss 0.18770053433101686 micro_f1_score 0.7849111352919842
 
----------
Epoch 8/40
time = 903.99 secondes

Train loss 0.06951195464326858 micro_f1_score 0.9247286731240518 
 
time = 30.37 secondes

Val loss 0.1974811076385076 micro_f1_score 0.7840789010214864
 
----------
Epoch 9/40
time = 906.74 secondes

Train loss 0.06111407910079301 micro_f1_score 0.9339487259046658 
 
time = 29.72 secondes

Val loss 0.2063321735038132 micro_f1_score 0.7897727272727273
 
----------
Epoch 10/40
time = 906.30 secondes

Train loss 0.053749504489908076 micro_f1_score 0.9447282861124013 
 
time = 29.71 secondes

Val loss 0.22025913616917173 micro_f1_score 0.7813057438458795
 
----------
Epoch 11/40
time = 906.27 secondes

Train loss 0.04714396712244363 micro_f1_score 0.95155749411356 
 
time = 29.81 secondes

Val loss 0.22338261895003866 micro_f1_score 0.7805742644452321
 
----------
Epoch 12/40
time = 907.81 secondes

Train loss 0.0406076539630859 micro_f1_score 0.9591112482641567 
 
time = 29.73 secondes

Val loss 0.2321673615179101 micro_f1_score 0.7811831789023521
 
----------
Epoch 13/40
time = 906.61 secondes

Train loss 0.03731447186694388 micro_f1_score 0.9618908307029024 
 
time = 29.56 secondes

Val loss 0.25875979063452265 micro_f1_score 0.7813471502590673
 
----------
Epoch 14/40
time = 908.86 secondes

Train loss 0.031870685183434735 micro_f1_score 0.9676849183477425 
 
time = 29.86 secondes

Val loss 0.2557211189118565 micro_f1_score 0.7886235955056179
 
----------
Epoch 15/40
time = 908.65 secondes

Train loss 0.02941363671005846 micro_f1_score 0.9695058642568737 
 
time = 29.92 secondes

Val loss 0.2702222021571437 micro_f1_score 0.7822160472386245
 
----------
Epoch 16/40
time = 909.64 secondes

Train loss 0.02548067254079338 micro_f1_score 0.9747905588921617 
 
time = 30.21 secondes

Val loss 0.2751024102822679 micro_f1_score 0.781665500349895
 
----------
Epoch 17/40
time = 908.04 secondes

Train loss 0.022835414415552608 micro_f1_score 0.9770735254717343 
 
time = 29.67 secondes

Val loss 0.2835524399138865 micro_f1_score 0.7842727905358386
 
----------
Epoch 18/40
time = 905.66 secondes

Train loss 0.020540404645516326 micro_f1_score 0.9793420045906657 
 
time = 29.72 secondes

Val loss 0.30215407321687604 micro_f1_score 0.7775438596491229
 
----------
Epoch 19/40
time = 905.04 secondes

Train loss 0.019096978572312625 micro_f1_score 0.9810095143479424 
 
time = 29.54 secondes

Val loss 0.3186041766502818 micro_f1_score 0.7745786516853933
 
----------
Epoch 20/40
time = 901.83 secondes

Train loss 0.016314753754901372 micro_f1_score 0.983686723973257 
 
time = 29.41 secondes

Val loss 0.30968268020231215 micro_f1_score 0.7875574407917993
 
----------
Epoch 21/40
time = 901.38 secondes

Train loss 0.01502346935449168 micro_f1_score 0.9850221610881859 
 
time = 29.35 secondes

Val loss 0.3095683192864793 micro_f1_score 0.7809048806555041
 
----------
Epoch 22/40
time = 903.40 secondes

Train loss 0.01410332005712212 micro_f1_score 0.9857562912895712 
 
time = 29.68 secondes

Val loss 0.31881049776174986 micro_f1_score 0.7840508115737473
 
----------
Epoch 23/40
time = 903.04 secondes

Train loss 0.012982875608098651 micro_f1_score 0.9873080001524565 
 
time = 29.41 secondes

Val loss 0.32038955695805005 micro_f1_score 0.7864285714285714
 
----------
Epoch 24/40
time = 909.21 secondes

Train loss 0.011772851444395922 micro_f1_score 0.9883384146341463 
 
time = 29.78 secondes

Val loss 0.3300546460220071 micro_f1_score 0.7865008880994672
 
----------
Epoch 25/40
time = 902.51 secondes

Train loss 0.011293341126528921 micro_f1_score 0.9879453727016096 
 
time = 29.70 secondes

Val loss 0.3389621824026108 micro_f1_score 0.7902197023387667
 
----------
Epoch 26/40
time = 904.34 secondes

Train loss 0.009028181287188393 micro_f1_score 0.9908298770975229 
 
time = 29.89 secondes

Val loss 0.3481167040643145 micro_f1_score 0.7710498029380152
 
----------
Epoch 27/40
time = 903.81 secondes

Train loss 0.009402039213376623 micro_f1_score 0.9905574169966493 
 
time = 29.57 secondes

Val loss 0.3420965865742965 micro_f1_score 0.7872185911401598
 
----------
Epoch 28/40
time = 903.68 secondes

Train loss 0.008137197298197566 micro_f1_score 0.9914022673666592 
 
time = 29.47 secondes

Val loss 0.35474722319450536 micro_f1_score 0.7847272727272727
 
----------
Epoch 29/40
time = 903.18 secondes

Train loss 0.007936723395418543 micro_f1_score 0.9920858382162697 
 
time = 29.72 secondes

Val loss 0.35681054319758887 micro_f1_score 0.7769010043041608
 
----------
Epoch 30/40
time = 902.82 secondes

Train loss 0.007024830767792033 micro_f1_score 0.9924400714204308 
 
time = 29.42 secondes

Val loss 0.3497429389934071 micro_f1_score 0.7879440258342304
 
----------
Epoch 31/40
time = 904.25 secondes

Train loss 0.005462687553347325 micro_f1_score 0.9945242984257358 
 
time = 29.92 secondes

Val loss 0.3636777178674448 micro_f1_score 0.7872185911401598
 
----------
Epoch 32/40
time = 902.86 secondes

Train loss 0.0062035872712275135 micro_f1_score 0.9941844996008969 
 
time = 30.01 secondes

Val loss 0.3588155806675309 micro_f1_score 0.792507204610951
 
----------
Epoch 33/40
time = 901.30 secondes

Train loss 0.004936995394303734 micro_f1_score 0.9949088145896656 
 
time = 31.27 secondes

Val loss 0.3701639853295733 micro_f1_score 0.7912794853466761
 
----------
Epoch 34/40
time = 903.98 secondes

Train loss 0.004537644807164356 micro_f1_score 0.9958544099189899 
 
time = 29.98 secondes

Val loss 0.35971074746768983 micro_f1_score 0.7967013266403727
 
----------
Epoch 35/40
time = 902.20 secondes

Train loss 0.0035018220567852383 micro_f1_score 0.996693398198472 
 
time = 30.35 secondes

Val loss 0.36087589124675656 micro_f1_score 0.7910823444804027
 
----------
Epoch 36/40
time = 905.15 secondes

Train loss 0.002964187439037896 micro_f1_score 0.9971503476575858 
 
time = 30.26 secondes

Val loss 0.37446691553856504 micro_f1_score 0.7873563218390804
 
----------
Epoch 37/40
time = 901.30 secondes

Train loss 0.002724566041053533 micro_f1_score 0.9976445558848112 
 
time = 29.78 secondes

Val loss 0.3700134062864741 micro_f1_score 0.7861818181818182
 
----------
Epoch 38/40
time = 901.61 secondes

Train loss 0.002350033343486962 micro_f1_score 0.9976819304579136 
 
time = 30.01 secondes

Val loss 0.36817112485649156 micro_f1_score 0.7924528301886793
 
----------
Epoch 39/40
time = 900.39 secondes

Train loss 0.001734407972885943 micro_f1_score 0.998594385138472 
 
time = 30.07 secondes

Val loss 0.3660077416017407 micro_f1_score 0.7935368043087971
 
----------
Epoch 40/40
time = 903.19 secondes

Train loss 0.0015079722337808612 micro_f1_score 0.9987077156974534 
 
time = 30.10 secondes

Val loss 0.36891346589707935 micro_f1_score 0.7971530249110321
 
----------
best_f1_socre 0.7971530249110321 best_epoch 40

average train time 904.8580401659012

average val time 29.824371773004533
 
time = 31.79 secondes

test_f1_score 0.7788227098571926

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_512_3
----------
Epoch 1/40
time = 1700.06 secondes

Train loss 0.2365471326016091 micro_f1_score 0.6647356361592668 
 
time = 36.90 secondes

Val loss 0.19181732589104136 micro_f1_score 0.7241106719367588
 
----------
Epoch 2/40
time = 1699.83 secondes

Train loss 0.1592619128491696 micro_f1_score 0.7906258884691929 
 
time = 36.68 secondes

Val loss 0.1846126117667214 micro_f1_score 0.7479674796747969
 
----------
Epoch 3/40
time = 1699.76 secondes

Train loss 0.1353077476055504 micro_f1_score 0.8271427430305935 
 
time = 36.85 secondes

Val loss 0.17385257059921982 micro_f1_score 0.7603494113178882
 
----------
Epoch 4/40
time = 1701.32 secondes

Train loss 0.12212526154209365 micro_f1_score 0.8508309205568556 
 
time = 36.81 secondes

Val loss 0.17596186074565667 micro_f1_score 0.7646838757949869
 
----------
Epoch 5/40
time = 1701.57 secondes

Train loss 0.10711035730300454 micro_f1_score 0.8724000473615661 
 
time = 36.94 secondes

Val loss 0.17416914240991482 micro_f1_score 0.7759442610927759
 
----------
Epoch 6/40
time = 1700.66 secondes

Train loss 0.09559797346088532 micro_f1_score 0.889865076874804 
 
time = 36.68 secondes

Val loss 0.17337301825402213 micro_f1_score 0.7801994828223126
 
----------
Epoch 7/40
time = 1700.56 secondes

Train loss 0.08549149240795019 micro_f1_score 0.9041941914552634 
 
time = 36.73 secondes

Val loss 0.19685099994549987 micro_f1_score 0.7750737463126844
 
----------
Epoch 8/40
time = 1701.26 secondes

Train loss 0.07697124094156935 micro_f1_score 0.9151763101500098 
 
time = 36.85 secondes

Val loss 0.1969810489748345 micro_f1_score 0.7822931785195936
 
----------
Epoch 9/40
time = 1702.49 secondes

Train loss 0.06853243320895074 micro_f1_score 0.9255691088493513 
 
time = 36.56 secondes

Val loss 0.20576217080481718 micro_f1_score 0.7774545454545454
 
----------
Epoch 10/40
time = 1701.34 secondes

Train loss 0.0611166893647195 micro_f1_score 0.9346402696524738 
 
time = 37.01 secondes

Val loss 0.21420744595835445 micro_f1_score 0.7846265409717187
 
----------
Epoch 11/40
time = 1703.60 secondes

Train loss 0.05491779953123884 micro_f1_score 0.9421168475112948 
 
time = 36.73 secondes

Val loss 0.2297920758118395 micro_f1_score 0.7784172661870503
 
----------
Epoch 12/40
time = 1702.20 secondes

Train loss 0.04866515081333886 micro_f1_score 0.9501714902308375 
 
time = 36.52 secondes

Val loss 0.23171974071225182 micro_f1_score 0.7845263919016632
 
----------
Epoch 13/40
time = 1700.99 secondes

Train loss 0.043921420702643627 micro_f1_score 0.9558433572883441 
 
time = 36.72 secondes

Val loss 0.24890449374425608 micro_f1_score 0.7709537572254335
 
----------
Epoch 14/40
time = 1702.58 secondes

Train loss 0.03894125465345611 micro_f1_score 0.9591460605129781 
 
time = 36.63 secondes

Val loss 0.23775028086221608 micro_f1_score 0.7873107426099495
 
----------
Epoch 15/40
time = 1702.94 secondes

Train loss 0.033270525718779045 micro_f1_score 0.9661621000191608 
 
time = 37.15 secondes

Val loss 0.2588287211466031 micro_f1_score 0.7784131522516083
 
----------
Epoch 16/40
time = 1701.61 secondes

Train loss 0.03102202016612733 micro_f1_score 0.9689889712946239 
 
time = 36.15 secondes

Val loss 0.2699119595230603 micro_f1_score 0.7757107757107756
 
----------
Epoch 17/40
time = 1700.69 secondes

Train loss 0.027374965791078704 micro_f1_score 0.9727411944869832 
 
time = 36.35 secondes

Val loss 0.27821064361783326 micro_f1_score 0.7806288398988073
 
----------
Epoch 18/40
time = 1701.94 secondes

Train loss 0.025785338495614697 micro_f1_score 0.9743668339783134 
 
time = 36.40 secondes

Val loss 0.28838899762171216 micro_f1_score 0.7793226381461675
 
----------
Epoch 19/40
time = 1699.70 secondes

Train loss 0.02173497851388445 micro_f1_score 0.9788713559775339 
 
time = 36.40 secondes

Val loss 0.28809275007883056 micro_f1_score 0.7704026115342765
 
----------
Epoch 20/40
time = 1700.00 secondes

Train loss 0.020129488927924686 micro_f1_score 0.9798072510325837 
 
time = 36.17 secondes

Val loss 0.28988272941014803 micro_f1_score 0.7868383404864092
 
----------
Epoch 21/40
time = 1700.93 secondes

Train loss 0.01820971827821907 micro_f1_score 0.981975101199114 
 
time = 36.06 secondes

Val loss 0.29001111836462723 micro_f1_score 0.7836468885672937
 
----------
Epoch 22/40
time = 1701.07 secondes

Train loss 0.015637994767166673 micro_f1_score 0.9847785449967572 
 
time = 36.77 secondes

Val loss 0.30878575353837406 micro_f1_score 0.7795620437956203
 
----------
Epoch 23/40
time = 1701.22 secondes

Train loss 0.01387216554601207 micro_f1_score 0.9859638416355176 
 
time = 36.10 secondes

Val loss 0.31412178754317954 micro_f1_score 0.7851361295069904
 
----------
Epoch 24/40
time = 1699.74 secondes

Train loss 0.013204123582827297 micro_f1_score 0.9869644762921177 
 
time = 36.30 secondes

Val loss 0.3235452965390487 micro_f1_score 0.7849580138736766
 
----------
Epoch 25/40
time = 1699.21 secondes

Train loss 0.012880466010385856 micro_f1_score 0.9881931748933577 
 
time = 36.35 secondes

Val loss 0.3204062272199109 micro_f1_score 0.7876138433515483
 
----------
Epoch 26/40
time = 1699.17 secondes

Train loss 0.01121378129716283 micro_f1_score 0.9888385204373166 
 
time = 36.54 secondes

Val loss 0.3279787447364604 micro_f1_score 0.7813884785819794
 
----------
Epoch 27/40
time = 1701.71 secondes

Train loss 0.010613900422341995 micro_f1_score 0.9889439573008006 
 
time = 36.78 secondes

Val loss 0.33082080272133235 micro_f1_score 0.7794385709077652
 
----------
Epoch 28/40
time = 1701.36 secondes

Train loss 0.008552152153732766 micro_f1_score 0.991926884996192 
 
time = 36.41 secondes

Val loss 0.3384069774116649 micro_f1_score 0.7830882352941176
 
----------
Epoch 29/40
time = 1703.67 secondes

Train loss 0.008347171910268649 micro_f1_score 0.9916644463898299 
 
time = 37.05 secondes

Val loss 0.32543582658542963 micro_f1_score 0.7931292008961912
 
----------
Epoch 30/40
time = 1705.50 secondes

Train loss 0.007979974506589634 micro_f1_score 0.9925441265976872 
 
time = 36.05 secondes

Val loss 0.33890714074988837 micro_f1_score 0.7886693999254567
 
----------
Epoch 31/40
time = 1701.15 secondes

Train loss 0.006739405689258674 micro_f1_score 0.993837492391966 
 
time = 35.82 secondes

Val loss 0.35972852394229077 micro_f1_score 0.7804878048780489
 
----------
Epoch 32/40
time = 1697.58 secondes

Train loss 0.005801247802263059 micro_f1_score 0.9944112838839676 
 
time = 35.80 secondes

Val loss 0.3636433927983534 micro_f1_score 0.7798603454612275
 
----------
Epoch 33/40
time = 1699.23 secondes

Train loss 0.005288293743587355 micro_f1_score 0.9950577858880779 
 
time = 35.87 secondes

Val loss 0.35505155504482694 micro_f1_score 0.7870404077175099
 
----------
Epoch 34/40
time = 1699.91 secondes

Train loss 0.004596626795440291 micro_f1_score 0.995438303048734 
 
time = 36.43 secondes

Val loss 0.363750159740448 micro_f1_score 0.7863935625457206
 
----------
Epoch 35/40
time = 1702.64 secondes

Train loss 0.004066403677581121 micro_f1_score 0.995778986196144 
 
time = 35.93 secondes

Val loss 0.3551425300843892 micro_f1_score 0.7956521739130435
 
----------
Epoch 36/40
time = 1703.96 secondes

Train loss 0.003245944440192312 micro_f1_score 0.9971104858946087 
 
time = 36.33 secondes

Val loss 0.35935725479340946 micro_f1_score 0.7957617829740592
 
----------
Epoch 37/40
time = 1702.39 secondes

Train loss 0.003207865285766856 micro_f1_score 0.9972633979475485 
 
time = 35.87 secondes

Val loss 0.3676318244123068 micro_f1_score 0.7925628873496172
 
----------
Epoch 38/40
time = 1701.77 secondes

Train loss 0.00248671640281598 micro_f1_score 0.9979470802919708 
 
time = 36.11 secondes

Val loss 0.3674852917673158 micro_f1_score 0.7944707166242271
 
----------
Epoch 39/40
time = 1704.70 secondes

Train loss 0.0017518457452211265 micro_f1_score 0.9984041340527396 
 
time = 37.51 secondes

Val loss 0.37244944311067707 micro_f1_score 0.7904761904761904
 
----------
Epoch 40/40
time = 1705.92 secondes

Train loss 0.002044301207213 micro_f1_score 0.9980229640331534 
 
time = 37.04 secondes

Val loss 0.38640599807754894 micro_f1_score 0.7831669044222539
 
----------
best_f1_socre 0.7957617829740592 best_epoch 36

average train time 1701.4482852339745

average val time 36.508284813165666
 
time = 39.84 secondes

test_f1_score 0.7766585620006947

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_256_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 768.00 MiB (GPU 1; 79.20 GiB total capacity; 72.06 GiB already allocated; 698.31 MiB free; 76.50 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_512_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 1.51 GiB (GPU 1; 79.20 GiB total capacity; 72.41 GiB already allocated; 1.28 GiB free; 75.90 GiB reserved in total by PyTorch)
datasets imported
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_64_4
----------
Epoch 1/40
time = 562.96 secondes

Train loss 0.25864645772286365 micro_f1_score 0.6374094967352196 
 
time = 20.61 secondes

Val loss 0.21806751851175651 micro_f1_score 0.6744730679156908
 
----------
Epoch 2/40
time = 568.42 secondes

Train loss 0.16856636767451827 micro_f1_score 0.7747466874512859 
 
time = 21.18 secondes

Val loss 0.18820460861335037 micro_f1_score 0.731839258114374
 
----------
Epoch 3/40
time = 561.47 secondes

Train loss 0.1463564439131333 micro_f1_score 0.8083069684432548 
 
time = 20.92 secondes

Val loss 0.1842574220700342 micro_f1_score 0.7455576559546314
 
----------
Epoch 4/40
time = 566.62 secondes

Train loss 0.12590659792023198 micro_f1_score 0.8416246094688777 
 
time = 20.84 secondes

Val loss 0.19668261340407076 micro_f1_score 0.7410207939508507
 
----------
Epoch 5/40
time = 567.10 secondes

Train loss 0.11080121761607425 micro_f1_score 0.8664832755252562 
 
time = 20.70 secondes

Val loss 0.2043117750130716 micro_f1_score 0.7472283813747228
 
----------
Epoch 6/40
time = 566.70 secondes

Train loss 0.09550748120338932 micro_f1_score 0.8864723792953457 
 
time = 20.53 secondes

Val loss 0.20358933691607148 micro_f1_score 0.7534646243617797
 
----------
Epoch 7/40
time = 567.85 secondes

Train loss 0.08257210761610721 micro_f1_score 0.9047712546632634 
 
time = 20.79 secondes

Val loss 0.23260506047088592 micro_f1_score 0.7548666186012978
 
----------
Epoch 8/40
time = 566.93 secondes

Train loss 0.07257668467003617 micro_f1_score 0.9157898853991473 
 
time = 20.56 secondes

Val loss 0.25834804932113553 micro_f1_score 0.7438075017692853
 
----------
Epoch 9/40
time = 573.31 secondes

Train loss 0.06078948234669394 micro_f1_score 0.9334419994566694 
 
time = 20.54 secondes

Val loss 0.27248377536163954 micro_f1_score 0.7488254427177449
 
----------
Epoch 10/40
time = 568.84 secondes

Train loss 0.05165682168727791 micro_f1_score 0.946234067207416 
 
time = 20.45 secondes

Val loss 0.2803789577523216 micro_f1_score 0.7577729573391179
 
----------
Epoch 11/40
time = 567.08 secondes

Train loss 0.04359162066421295 micro_f1_score 0.9531701676623626 
 
time = 20.71 secondes

Val loss 0.2984556862321056 micro_f1_score 0.7573018080667594
 
----------
Epoch 12/40
time = 570.74 secondes

Train loss 0.03754029223928228 micro_f1_score 0.9607782819349381 
 
time = 20.62 secondes

Val loss 0.29961389950552925 micro_f1_score 0.7563025210084034
 
----------
Epoch 13/40
time = 570.24 secondes

Train loss 0.0325116043955482 micro_f1_score 0.9653323118176593 
 
time = 20.45 secondes

Val loss 0.32144089201923276 micro_f1_score 0.7546777546777547
 
----------
Epoch 14/40
time = 568.88 secondes

Train loss 0.02595522806000149 micro_f1_score 0.9738684623330911 
 
time = 20.51 secondes

Val loss 0.33316625886764684 micro_f1_score 0.7511504424778761
 
----------
Epoch 15/40
time = 567.98 secondes

Train loss 0.023827428430704786 micro_f1_score 0.9750611995104039 
 
time = 20.58 secondes

Val loss 0.3452612926236919 micro_f1_score 0.7665937272064186
 
----------
Epoch 16/40
time = 567.19 secondes

Train loss 0.02081928082284526 micro_f1_score 0.9784112185243209 
 
time = 20.77 secondes

Val loss 0.3664094745868542 micro_f1_score 0.7594012413289521
 
----------
Epoch 17/40
time = 567.53 secondes

Train loss 0.01819929694497291 micro_f1_score 0.980523944092263 
 
time = 20.64 secondes

Val loss 0.3977861244414673 micro_f1_score 0.7598192561696211
 
----------
Epoch 18/40
time = 570.59 secondes

Train loss 0.016082315145192566 micro_f1_score 0.9824775720557358 
 
time = 20.52 secondes

Val loss 0.4075718432909153 micro_f1_score 0.7542857142857142
 
----------
Epoch 19/40
time = 567.55 secondes

Train loss 0.014446746882458682 micro_f1_score 0.9845203599206954 
 
time = 20.53 secondes

Val loss 0.41836666401292455 micro_f1_score 0.753840657377635
 
----------
Epoch 20/40
time = 568.54 secondes

Train loss 0.013384843642665031 micro_f1_score 0.9863567073170733 
 
time = 20.79 secondes

Val loss 0.4615804958538931 micro_f1_score 0.742876997915219
 
----------
Epoch 21/40
time = 570.07 secondes

Train loss 0.011840625909452855 micro_f1_score 0.9876289444634768 
 
time = 20.55 secondes

Val loss 0.4445113797412544 micro_f1_score 0.7488355428161948
 
----------
Epoch 22/40
time = 570.38 secondes

Train loss 0.010920662380509007 micro_f1_score 0.9884978671541742 
 
time = 20.68 secondes

Val loss 0.43857574841526686 micro_f1_score 0.755539922616954
 
----------
Epoch 23/40
time = 575.77 secondes

Train loss 0.008821985644166702 micro_f1_score 0.990034233548878 
 
time = 21.27 secondes

Val loss 0.4487601907038298 micro_f1_score 0.7555717939349652
 
----------
Epoch 24/40
time = 574.26 secondes

Train loss 0.008569530828396826 micro_f1_score 0.9908654944051153 
 
time = 21.14 secondes

Val loss 0.47693034615673 micro_f1_score 0.7555715312724659
 
----------
Epoch 25/40
time = 575.23 secondes

Train loss 0.008548115204849547 micro_f1_score 0.9910697320919628 
 
time = 21.12 secondes

Val loss 0.491588225496597 micro_f1_score 0.7494505494505495
 
----------
Epoch 26/40
time = 572.36 secondes

Train loss 0.007185537808418467 micro_f1_score 0.9927007299270073 
 
time = 21.14 secondes

Val loss 0.4745840457130651 micro_f1_score 0.7593818984547461
 
----------
Epoch 27/40
time = 574.97 secondes

Train loss 0.007433957875731688 micro_f1_score 0.9927745664739884 
 
time = 21.11 secondes

Val loss 0.5142658402929541 micro_f1_score 0.7486592777976404
 
----------
Epoch 28/40
time = 578.23 secondes

Train loss 0.007121338581559899 micro_f1_score 0.9929352780309936 
 
time = 21.13 secondes

Val loss 0.5336911375405359 micro_f1_score 0.7398800599700148
 
----------
Epoch 29/40
time = 574.44 secondes

Train loss 0.006767803811532291 micro_f1_score 0.99330900243309 
 
time = 21.12 secondes

Val loss 0.5138361897380626 micro_f1_score 0.7555555555555556
 
----------
Epoch 30/40
time = 574.58 secondes

Train loss 0.005411827195228824 micro_f1_score 0.9947162352225644 
 
time = 20.96 secondes

Val loss 0.4973585708219497 micro_f1_score 0.7541818181818182
 
----------
Epoch 31/40
time = 575.93 secondes

Train loss 0.003900942647394274 micro_f1_score 0.9956302010107535 
 
time = 21.11 secondes

Val loss 0.5187780601812191 micro_f1_score 0.7526182737450344
 
----------
Epoch 32/40
time = 573.23 secondes

Train loss 0.003922500843806234 micro_f1_score 0.9965815861440293 
 
time = 21.27 secondes

Val loss 0.5131711233101908 micro_f1_score 0.7592997811816191
 
----------
Epoch 33/40
time = 574.09 secondes

Train loss 0.004670494474295402 micro_f1_score 0.9955940443634154 
 
time = 20.99 secondes

Val loss 0.5333100993369446 micro_f1_score 0.7504527345164794
 
----------
Epoch 34/40
time = 570.75 secondes

Train loss 0.0032195794106129297 micro_f1_score 0.9966951566951567 
 
time = 20.58 secondes

Val loss 0.5295990909465024 micro_f1_score 0.7623869801084991
 
----------
Epoch 35/40
time = 572.96 secondes

Train loss 0.002779362976521493 micro_f1_score 0.9973789173789174 
 
time = 21.23 secondes

Val loss 0.5293293288740956 micro_f1_score 0.7650667629014796
 
----------
Epoch 36/40
time = 574.02 secondes

Train loss 0.0020923005783868016 micro_f1_score 0.9979484841577388 
 
time = 21.53 secondes

Val loss 0.5438558228436063 micro_f1_score 0.7540294323756133
 
----------
Epoch 37/40
time = 573.60 secondes

Train loss 0.0025030862172292193 micro_f1_score 0.9978344287831009 
 
time = 21.30 secondes

Val loss 0.5462135278054925 micro_f1_score 0.7629233511586454
 
----------
Epoch 38/40
time = 568.06 secondes

Train loss 0.0018942817780592672 micro_f1_score 0.9982898187207843 
 
time = 20.96 secondes

Val loss 0.545345944092899 micro_f1_score 0.762142857142857
 
----------
Epoch 39/40
time = 577.18 secondes

Train loss 0.0010231858993057022 micro_f1_score 0.9992402947656309 
 
time = 21.15 secondes

Val loss 0.5472441288779993 micro_f1_score 0.7615658362989324
 
----------
Epoch 40/40
time = 573.60 secondes

Train loss 0.000889065399166554 micro_f1_score 0.9989740471938291 
 
time = 21.47 secondes

Val loss 0.574572353822286 micro_f1_score 0.7586206896551724
 
----------
best_f1_socre 0.7665937272064186 best_epoch 15

average train time 570.755770021677

average val time 20.874991381168364
 
time = 22.13 secondes

test_f1_score 0.7532743362831857

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_128_4
----------
Epoch 1/40
Attention type 'block_sparse' is not possible if sequence_length: 1024 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 1408 with config.block_size = 128, config.num_random_blocks = 3. Changing attention type to 'original_full'...
time = 435.26 secondes

Train loss 0.28381372997621157 micro_f1_score 0.5725761772853185 
 
time = 18.35 secondes

Val loss 0.23454971855781118 micro_f1_score 0.6424742268041237
 
----------
Epoch 2/40
time = 434.07 secondes

Train loss 0.18855639927693316 micro_f1_score 0.7383503613256915 
 
time = 18.73 secondes

Val loss 0.21013900503271915 micro_f1_score 0.6956521739130435
 
----------
Epoch 3/40
time = 432.62 secondes

Train loss 0.16082444639524093 micro_f1_score 0.7858746682996529 
 
time = 18.81 secondes

Val loss 0.19861526628498172 micro_f1_score 0.7169074580242093
 
----------
Epoch 4/40
time = 439.07 secondes

Train loss 0.14203206541872507 micro_f1_score 0.8176003886010363 
 
time = 18.45 secondes

Val loss 0.19926605031627123 micro_f1_score 0.7326283987915407
 
----------
Epoch 5/40
time = 434.33 secondes

Train loss 0.12743179295379836 micro_f1_score 0.8381909547738694 
 
time = 18.48 secondes

Val loss 0.19843487123974035 micro_f1_score 0.744079449961803
 
----------
Epoch 6/40
time = 436.97 secondes

Train loss 0.11416745783408751 micro_f1_score 0.8604465071617189 
 
time = 18.25 secondes

Val loss 0.1935538079162113 micro_f1_score 0.7571955719557195
 
----------
Epoch 7/40
time = 434.52 secondes

Train loss 0.10307095230028436 micro_f1_score 0.8753326184518845 
 
time = 18.53 secondes

Val loss 0.20971993869933925 micro_f1_score 0.7473414008067473
 
----------
Epoch 8/40
time = 436.42 secondes

Train loss 0.09055207995865662 micro_f1_score 0.8936404370725571 
 
time = 18.36 secondes

Val loss 0.22319271775786995 micro_f1_score 0.75650623885918
 
----------
Epoch 9/40
time = 435.42 secondes

Train loss 0.0809729241061251 micro_f1_score 0.9065438795020877 
 
time = 18.49 secondes

Val loss 0.23154611653480373 micro_f1_score 0.752883607130374
 
----------
Epoch 10/40
time = 438.02 secondes

Train loss 0.0707695206493005 micro_f1_score 0.9216632923220682 
 
time = 18.45 secondes

Val loss 0.25205920404586635 micro_f1_score 0.7551454932576295
 
----------
Epoch 11/40
time = 440.59 secondes

Train loss 0.06135026917851589 micro_f1_score 0.9323255813953489 
 
time = 18.52 secondes

Val loss 0.25760590859123916 micro_f1_score 0.7596899224806202
 
----------
Epoch 12/40
time = 436.33 secondes

Train loss 0.05411012814514473 micro_f1_score 0.9415358440955842 
 
time = 18.37 secondes

Val loss 0.2813539818906393 micro_f1_score 0.7499130434782608
 
----------
Epoch 13/40
time = 437.54 secondes

Train loss 0.04677596288200401 micro_f1_score 0.9511913023363404 
 
time = 18.17 secondes

Val loss 0.28464615076291755 micro_f1_score 0.766407904022583
 
----------
Epoch 14/40
time = 436.21 secondes

Train loss 0.04086375646083406 micro_f1_score 0.9584551469456839 
 
time = 18.33 secondes

Val loss 0.31739052409519913 micro_f1_score 0.7514164305949008
 
----------
Epoch 15/40
time = 435.45 secondes

Train loss 0.03614069584312404 micro_f1_score 0.96237714987715 
 
time = 18.61 secondes

Val loss 0.3129410172095064 micro_f1_score 0.7616690240452617
 
----------
Epoch 16/40
time = 438.11 secondes

Train loss 0.03235430069370531 micro_f1_score 0.9658450569249051 
 
time = 18.71 secondes

Val loss 0.35277671586783205 micro_f1_score 0.7498228206945429
 
----------
Epoch 17/40
time = 439.42 secondes

Train loss 0.028233467015806955 micro_f1_score 0.9707423413775959 
 
time = 18.87 secondes

Val loss 0.35923618456867873 micro_f1_score 0.7487579843860893
 
----------
Epoch 18/40
time = 435.80 secondes

Train loss 0.025146399316459744 micro_f1_score 0.973479058391929 
 
time = 18.55 secondes

Val loss 0.3630324748695874 micro_f1_score 0.752916224814422
 
----------
Epoch 19/40
time = 437.72 secondes

Train loss 0.022692449129897648 micro_f1_score 0.9759681616409001 
 
time = 17.86 secondes

Val loss 0.37419078205941153 micro_f1_score 0.7553982300884955
 
----------
Epoch 20/40
time = 435.73 secondes

Train loss 0.019381279994572117 micro_f1_score 0.9801284011005809 
 
time = 18.30 secondes

Val loss 0.37135789641102807 micro_f1_score 0.7657754010695188
 
----------
Epoch 21/40
time = 440.61 secondes

Train loss 0.01910673428766287 micro_f1_score 0.9806731978805321 
 
time = 18.44 secondes

Val loss 0.3805823116028895 micro_f1_score 0.7682449270202919
 
----------
Epoch 22/40
time = 435.29 secondes

Train loss 0.015840046461839397 micro_f1_score 0.9832525845954297 
 
time = 18.24 secondes

Val loss 0.404261585752495 micro_f1_score 0.7566044381824586
 
----------
Epoch 23/40
time = 439.54 secondes

Train loss 0.013500512625155803 micro_f1_score 0.9857834356062048 
 
time = 18.50 secondes

Val loss 0.42352176042365247 micro_f1_score 0.75673707210488
 
----------
Epoch 24/40
time = 434.60 secondes

Train loss 0.013610719521291373 micro_f1_score 0.9853889291572884 
 
time = 17.72 secondes

Val loss 0.42487115351880184 micro_f1_score 0.7601398601398601
 
----------
Epoch 25/40
time = 436.73 secondes

Train loss 0.011231448205420222 micro_f1_score 0.9878517841501961 
 
time = 17.33 secondes

Val loss 0.41907450727752 micro_f1_score 0.7688477951635847
 
----------
Epoch 26/40
time = 432.94 secondes

Train loss 0.010855240248870482 micro_f1_score 0.9892612338156893 
 
time = 17.32 secondes

Val loss 0.4736963102563483 micro_f1_score 0.7504375218760938
 
----------
Epoch 27/40
time = 437.81 secondes

Train loss 0.01022224888669092 micro_f1_score 0.9905197030268418 
 
time = 17.49 secondes

Val loss 0.4723348517398365 micro_f1_score 0.7548228691687128
 
----------
Epoch 28/40
time = 433.02 secondes

Train loss 0.008829477613511012 micro_f1_score 0.9910232027386838 
 
time = 17.28 secondes

Val loss 0.43711682250265216 micro_f1_score 0.7690120824449184
 
----------
Epoch 29/40
time = 438.29 secondes

Train loss 0.010268776371381834 micro_f1_score 0.9907551835647708 
 
time = 17.36 secondes

Val loss 0.48573003636031853 micro_f1_score 0.7472605160834217
 
----------
Epoch 30/40
time = 434.35 secondes

Train loss 0.008184492115545148 micro_f1_score 0.9918946687469082 
 
time = 17.28 secondes

Val loss 0.48215157013447557 micro_f1_score 0.7554782608695652
 
----------
Epoch 31/40
time = 433.45 secondes

Train loss 0.00577590319137983 micro_f1_score 0.9941053432211446 
 
time = 17.34 secondes

Val loss 0.47416913692579893 micro_f1_score 0.7581837381203802
 
----------
Epoch 32/40
time = 431.36 secondes

Train loss 0.0052863099510170024 micro_f1_score 0.9944503573057624 
 
time = 17.42 secondes

Val loss 0.4982323692958863 micro_f1_score 0.7620701632511288
 
----------
Epoch 33/40
time = 432.77 secondes

Train loss 0.0049486384155406505 micro_f1_score 0.9947536496350365 
 
time = 17.52 secondes

Val loss 0.5246157851375517 micro_f1_score 0.74822695035461
 
----------
Epoch 34/40
time = 431.51 secondes

Train loss 0.004884380956971179 micro_f1_score 0.9950222289774671 
 
time = 17.38 secondes

Val loss 0.5028113507833637 micro_f1_score 0.7583274273564847
 
----------
Epoch 35/40
time = 432.30 secondes

Train loss 0.004343593283975916 micro_f1_score 0.9955927051671731 
 
time = 17.26 secondes

Val loss 0.5403502620145922 micro_f1_score 0.7537050105857446
 
----------
Epoch 36/40
time = 431.76 secondes

Train loss 0.003413350850190057 micro_f1_score 0.996502167135579 
 
time = 17.36 secondes

Val loss 0.5534173225770231 micro_f1_score 0.7518273581621997
 
----------
Epoch 37/40
time = 432.05 secondes

Train loss 0.0038275548042489586 micro_f1_score 0.9960135160788185 
 
time = 17.47 secondes

Val loss 0.5396817084707197 micro_f1_score 0.7585247042449548
 
----------
Epoch 38/40
time = 430.94 secondes

Train loss 0.0020486901380872375 micro_f1_score 0.9978710462287104 
 
time = 17.46 secondes

Val loss 0.5438586852101029 micro_f1_score 0.7661064425770308
 
----------
Epoch 39/40
time = 432.50 secondes

Train loss 0.001383238248316965 micro_f1_score 0.9986700611771858 
 
time = 17.13 secondes

Val loss 0.5372384136817494 micro_f1_score 0.7631115804294263
 
----------
Epoch 40/40
time = 431.20 secondes

Train loss 0.0013703420169222554 micro_f1_score 0.9986323227718259 
 
time = 17.16 secondes

Val loss 0.563055508937992 micro_f1_score 0.763724808895066
 
----------
best_f1_socre 0.7690120824449184 best_epoch 28

average train time 435.3151836752892

average val time 17.990990710258483
 
time = 18.08 secondes

test_f1_score 0.7431693989071039

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_64_4
----------
Epoch 1/40
time = 1744.00 secondes

Train loss 0.2568347557238093 micro_f1_score 0.6358901933092066 
 
time = 30.43 secondes

Val loss 0.22470875792816036 micro_f1_score 0.6690561529271206
 
----------
Epoch 2/40
time = 1745.19 secondes

Train loss 0.16135677353621602 micro_f1_score 0.7874318954569661 
 
time = 30.20 secondes

Val loss 0.1851312292159581 micro_f1_score 0.7475840742172399
 
----------
Epoch 3/40
time = 1742.50 secondes

Train loss 0.13662096126055395 micro_f1_score 0.8265528075404817 
 
time = 30.18 secondes

Val loss 0.1783141774720833 micro_f1_score 0.758076634109692
 
----------
Epoch 4/40
time = 1741.44 secondes

Train loss 0.11907801070326084 micro_f1_score 0.8534599728629579 
 
time = 30.51 secondes

Val loss 0.16702625265375512 micro_f1_score 0.77994839660892
 
----------
Epoch 5/40
time = 1738.61 secondes

Train loss 0.10357994965638395 micro_f1_score 0.8764222503160557 
 
time = 30.46 secondes

Val loss 0.19952273161196318 micro_f1_score 0.7643219724438
 
----------
Epoch 6/40
time = 1738.56 secondes

Train loss 0.08856274546585508 micro_f1_score 0.8980105370763545 
 
time = 30.60 secondes

Val loss 0.2002539853336381 micro_f1_score 0.7695060844667144
 
----------
Epoch 7/40
time = 1739.06 secondes

Train loss 0.07575890569620439 micro_f1_score 0.9141135501036898 
 
time = 30.41 secondes

Val loss 0.19982668000166534 micro_f1_score 0.7855093256814921
 
----------
Epoch 8/40
time = 1737.72 secondes

Train loss 0.06427388199991001 micro_f1_score 0.9292913569677395 
 
time = 30.02 secondes

Val loss 0.21265821739054117 micro_f1_score 0.7855352667382742
 
----------
Epoch 9/40
time = 1737.81 secondes

Train loss 0.05373704134075491 micro_f1_score 0.9428593580400062 
 
time = 30.32 secondes

Val loss 0.2436964668822093 micro_f1_score 0.776386404293381
 
----------
Epoch 10/40
time = 1738.55 secondes

Train loss 0.04657878581652636 micro_f1_score 0.9515867500579106 
 
time = 30.39 secondes

Val loss 0.23560063785216848 micro_f1_score 0.7890961262553802
 
----------
Epoch 11/40
time = 1738.25 secondes

Train loss 0.040694357050364616 micro_f1_score 0.9582226329367372 
 
time = 30.39 secondes

Val loss 0.24891391433164722 micro_f1_score 0.786756852972588
 
----------
Epoch 12/40
time = 1738.19 secondes

Train loss 0.035488462662072603 micro_f1_score 0.9637098325957283 
 
time = 30.89 secondes

Val loss 0.26592787746034685 micro_f1_score 0.7837351565311264
 
----------
Epoch 13/40
time = 1741.68 secondes

Train loss 0.029727093945991517 micro_f1_score 0.9696342305037956 
 
time = 30.44 secondes

Val loss 0.26981349264989135 micro_f1_score 0.7799145299145299
 
----------
Epoch 14/40
time = 1742.94 secondes

Train loss 0.025425869216282457 micro_f1_score 0.9734323558686165 
 
time = 30.12 secondes

Val loss 0.27083590614502545 micro_f1_score 0.7998525617397716
 
----------
Epoch 15/40
time = 1742.45 secondes

Train loss 0.02207467563619191 micro_f1_score 0.9773987532984053 
 
time = 30.01 secondes

Val loss 0.30577827171712624 micro_f1_score 0.7813953488372093
 
----------
Epoch 16/40
time = 1742.80 secondes

Train loss 0.019899511114360665 micro_f1_score 0.9792661040895032 
 
time = 30.38 secondes

Val loss 0.33928204016363034 micro_f1_score 0.7762289068231841
 
----------
Epoch 17/40
time = 1742.24 secondes

Train loss 0.018032285496188738 micro_f1_score 0.9807838950739668 
 
time = 30.30 secondes

Val loss 0.31531026348715924 micro_f1_score 0.7951363301400147
 
----------
Epoch 18/40
time = 1738.41 secondes

Train loss 0.015949083162998142 micro_f1_score 0.9839225845778726 
 
time = 30.34 secondes

Val loss 0.3426201654384371 micro_f1_score 0.7948070681572306
 
----------
Epoch 19/40
time = 1739.82 secondes

Train loss 0.014136438597825372 micro_f1_score 0.9858253315043437 
 
time = 30.08 secondes

Val loss 0.358727864921093 micro_f1_score 0.7916666666666666
 
----------
Epoch 20/40
time = 1738.41 secondes

Train loss 0.013805074060256047 micro_f1_score 0.9860947083698427 
 
time = 29.91 secondes

Val loss 0.3472640542466132 micro_f1_score 0.7887424296401853
 
----------
Epoch 21/40
time = 1739.47 secondes

Train loss 0.012096058509993233 micro_f1_score 0.9883955408438915 
 
time = 30.23 secondes

Val loss 0.3590211900042706 micro_f1_score 0.7978339350180506
 
----------
Epoch 22/40
time = 1739.04 secondes

Train loss 0.009775786261600736 micro_f1_score 0.9900650907845154 
 
time = 30.56 secondes

Val loss 0.38248436763638355 micro_f1_score 0.7870036101083033
 
----------
Epoch 23/40
time = 1738.91 secondes

Train loss 0.009729290785341889 micro_f1_score 0.9897636896381141 
 
time = 30.71 secondes

Val loss 0.384564498042474 micro_f1_score 0.7865497076023392
 
----------
Epoch 24/40
time = 1739.01 secondes

Train loss 0.009435721875769076 micro_f1_score 0.9902180946218552 
 
time = 29.91 secondes

Val loss 0.4005220102482155 micro_f1_score 0.7822189017556966
 
----------
Epoch 25/40
time = 1738.64 secondes

Train loss 0.0068386577621945555 micro_f1_score 0.9933411970625166 
 
time = 29.92 secondes

Val loss 0.41286558218178204 micro_f1_score 0.7823550856726211
 
----------
Epoch 26/40
time = 1739.05 secondes

Train loss 0.007635965802150134 micro_f1_score 0.9926957315681351 
 
time = 29.95 secondes

Val loss 0.4089119562604388 micro_f1_score 0.7929644558446318
 
----------
Epoch 27/40
time = 1738.56 secondes

Train loss 0.007062421050605727 micro_f1_score 0.9931936575535193 
 
time = 29.97 secondes

Val loss 0.4311819339140517 micro_f1_score 0.7743379336068632
 
----------
Epoch 28/40
time = 1736.45 secondes

Train loss 0.006301673635932053 micro_f1_score 0.9938076966911067 
 
time = 29.84 secondes

Val loss 0.4214087016269809 micro_f1_score 0.7867195958137857
 
----------
Epoch 29/40
time = 1740.05 secondes

Train loss 0.005916908023504515 micro_f1_score 0.9940720474236207 
 
time = 29.89 secondes

Val loss 0.42987902096060454 micro_f1_score 0.7898860398860399
 
----------
Epoch 30/40
time = 1740.33 secondes

Train loss 0.004621315153810822 micro_f1_score 0.9950974803329153 
 
time = 29.82 secondes

Val loss 0.4519317230728806 micro_f1_score 0.7917113254733834
 
----------
Epoch 31/40
time = 1737.77 secondes

Train loss 0.004755381326780939 micro_f1_score 0.9955548801337335 
 
time = 30.32 secondes

Val loss 0.4220163464301922 micro_f1_score 0.7988316903979553
 
----------
Epoch 32/40
time = 1739.04 secondes

Train loss 0.004140925841546722 micro_f1_score 0.9960116990162191 
 
time = 29.90 secondes

Val loss 0.4390989759417831 micro_f1_score 0.7902580879680116
 
----------
Epoch 33/40
time = 1737.25 secondes

Train loss 0.0035253244879165755 micro_f1_score 0.9967691664449428 
 
time = 30.78 secondes

Val loss 0.4318558218293503 micro_f1_score 0.7982646420824295
 
----------
Epoch 34/40
time = 1736.77 secondes

Train loss 0.0032663488778731284 micro_f1_score 0.997568943250019 
 
time = 29.83 secondes

Val loss 0.43867987467617287 micro_f1_score 0.7929292929292928
 
----------
Epoch 35/40
time = 1738.36 secondes

Train loss 0.002078032997298152 micro_f1_score 0.9980629723878612 
 
time = 29.91 secondes

Val loss 0.4510802096519314 micro_f1_score 0.790513833992095
 
----------
Epoch 36/40
time = 1739.31 secondes

Train loss 0.0019859885800747384 micro_f1_score 0.9980243161094225 
 
time = 29.87 secondes

Val loss 0.46149542219326145 micro_f1_score 0.7975809320526502
 
----------
Epoch 37/40
time = 1741.11 secondes

Train loss 0.001746245177311131 micro_f1_score 0.9985182933779111 
 
time = 29.98 secondes

Val loss 0.4853065077398644 micro_f1_score 0.7876138433515483
 
----------
Epoch 38/40
time = 1739.73 secondes

Train loss 0.0012554222839081615 micro_f1_score 0.9988600957519569 
 
time = 29.89 secondes

Val loss 0.4732793520708553 micro_f1_score 0.7912794853466761
 
----------
Epoch 39/40
time = 1739.91 secondes

Train loss 0.0009672222857728697 micro_f1_score 0.998898720236965 
 
time = 30.43 secondes

Val loss 0.4758680420087986 micro_f1_score 0.7962896896182662
 
----------
Epoch 40/40
time = 1741.00 secondes

Train loss 0.000893027833015711 micro_f1_score 0.9991265048801794 
 
time = 29.92 secondes

Val loss 0.4837407475856484 micro_f1_score 0.7928952042628775
 
----------
best_f1_socre 0.7998525617397716 best_epoch 14

average train time 1739.7092663943768

average val time 30.20063572525978
 
time = 31.33 secondes

test_f1_score 0.7864845434938893

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_128_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 1; 79.20 GiB total capacity; 75.73 GiB already allocated; 72.31 MiB free; 77.11 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_64_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 768.00 MiB (GPU 1; 79.20 GiB total capacity; 71.64 GiB already allocated; 690.31 MiB free; 76.51 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_128_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 2.62 GiB (GPU 1; 79.20 GiB total capacity; 75.62 GiB already allocated; 830.31 MiB free; 76.37 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_256_4
----------
Epoch 1/40
time = 489.40 secondes

Train loss 0.23731847155738522 micro_f1_score 0.6566378667602298 
 
time = 21.70 secondes

Val loss 0.20294710273136857 micro_f1_score 0.7107692307692308
 
----------
Epoch 2/40
time = 485.65 secondes

Train loss 0.1648222129791975 micro_f1_score 0.7756859849883105 
 
time = 21.68 secondes

Val loss 0.18857613047126864 micro_f1_score 0.7274872198191112
 
----------
Epoch 3/40
time = 485.72 secondes

Train loss 0.14114727567344368 micro_f1_score 0.8131423484664562 
 
time = 21.26 secondes

Val loss 0.19089940055960514 micro_f1_score 0.7357784431137725
 
----------
Epoch 4/40
time = 483.43 secondes

Train loss 0.12350682902510639 micro_f1_score 0.8453946053946054 
 
time = 21.33 secondes

Val loss 0.19634444547481225 micro_f1_score 0.738484398216939
 
----------
Epoch 5/40
time = 484.46 secondes

Train loss 0.11034298744052648 micro_f1_score 0.8643919510061242 
 
time = 21.31 secondes

Val loss 0.19708998208163214 micro_f1_score 0.7499999999999999
 
----------
Epoch 6/40
time = 485.80 secondes

Train loss 0.09873559514715059 micro_f1_score 0.8813236519461969 
 
time = 21.49 secondes

Val loss 0.21140685714170582 micro_f1_score 0.7302904564315352
 
----------
Epoch 7/40
time = 485.45 secondes

Train loss 0.09035695902164187 micro_f1_score 0.8947472120424006 
 
time = 21.32 secondes

Val loss 0.214910090702479 micro_f1_score 0.7504621072088725
 
----------
Epoch 8/40
time = 488.00 secondes

Train loss 0.078215745054521 micro_f1_score 0.9128559102674719 
 
time = 21.44 secondes

Val loss 0.23079803109657568 micro_f1_score 0.7518573551263001
 
----------
Epoch 9/40
time = 482.36 secondes

Train loss 0.06941518221419674 micro_f1_score 0.9234492719678339 
 
time = 21.40 secondes

Val loss 0.22711481801310524 micro_f1_score 0.7646416878865043
 
----------
Epoch 10/40
time = 486.74 secondes

Train loss 0.061025622132158765 micro_f1_score 0.9351113875821313 
 
time = 21.29 secondes

Val loss 0.23671029251618464 micro_f1_score 0.7569493941553812
 
----------
Epoch 11/40
time = 483.24 secondes

Train loss 0.055231151023778964 micro_f1_score 0.9421013903411952 
 
time = 21.73 secondes

Val loss 0.25295984702276403 micro_f1_score 0.7598284488920658
 
----------
Epoch 12/40
time = 485.23 secondes

Train loss 0.04863305862841977 micro_f1_score 0.9488875991487716 
 
time = 21.37 secondes

Val loss 0.26071480038713235 micro_f1_score 0.7532005689900426
 
----------
Epoch 13/40
time = 485.05 secondes

Train loss 0.04415390123529276 micro_f1_score 0.9550587871287127 
 
time = 21.38 secondes

Val loss 0.26848313493318243 micro_f1_score 0.758694109297374
 
----------
Epoch 14/40
time = 484.59 secondes

Train loss 0.03766746937406358 micro_f1_score 0.9629143673059853 
 
time = 21.35 secondes

Val loss 0.29133588796267745 micro_f1_score 0.7543424317617867
 
----------
Epoch 15/40
time = 485.30 secondes

Train loss 0.035498276599347255 micro_f1_score 0.9652970144659896 
 
time = 21.38 secondes

Val loss 0.3043786968852653 micro_f1_score 0.7460992907801419
 
----------
Epoch 16/40
time = 486.46 secondes

Train loss 0.031256240907519516 micro_f1_score 0.9677964799016217 
 
time = 21.31 secondes

Val loss 0.3100756681600555 micro_f1_score 0.7491065046461758
 
----------
Epoch 17/40
time = 484.98 secondes

Train loss 0.027686535636894404 micro_f1_score 0.9726121979286536 
 
time = 21.30 secondes

Val loss 0.3130145197520491 micro_f1_score 0.7551748750892221
 
----------
Epoch 18/40
time = 483.14 secondes

Train loss 0.024123630274049436 micro_f1_score 0.9766996977464897 
 
time = 21.45 secondes

Val loss 0.3322871641301718 micro_f1_score 0.7518689925240298
 
----------
Epoch 19/40
time = 482.47 secondes

Train loss 0.022998327057838004 micro_f1_score 0.9770770349393441 
 
time = 21.33 secondes

Val loss 0.3149239530817407 micro_f1_score 0.7601156069364163
 
----------
Epoch 20/40
time = 483.05 secondes

Train loss 0.01992873403417819 micro_f1_score 0.9804822043628014 
 
time = 21.72 secondes

Val loss 0.3251029645077518 micro_f1_score 0.756717501815541
 
----------
Epoch 21/40
time = 478.69 secondes

Train loss 0.017964783415139594 micro_f1_score 0.9826426059030432 
 
time = 21.30 secondes

Val loss 0.338509132627581 micro_f1_score 0.7591865858009277
 
----------
Epoch 22/40
time = 480.75 secondes

Train loss 0.016802019134388297 micro_f1_score 0.9827842882772837 
 
time = 21.34 secondes

Val loss 0.3346299315329458 micro_f1_score 0.7627737226277371
 
----------
Epoch 23/40
time = 479.50 secondes

Train loss 0.01443042036881125 micro_f1_score 0.9853558081000686 
 
time = 21.35 secondes

Val loss 0.34474428926335005 micro_f1_score 0.758221900975786
 
----------
Epoch 24/40
time = 478.32 secondes

Train loss 0.014142301819981537 micro_f1_score 0.9854552395495323 
 
time = 21.64 secondes

Val loss 0.34683347615550775 micro_f1_score 0.7556350626118069
 
----------
Epoch 25/40
time = 481.04 secondes

Train loss 0.01183373932579851 micro_f1_score 0.9880189255189256 
 
time = 21.42 secondes

Val loss 0.3602475126991507 micro_f1_score 0.7607052896725441
 
----------
Epoch 26/40
time = 480.71 secondes

Train loss 0.011193682842621545 micro_f1_score 0.9889498552049992 
 
time = 21.66 secondes

Val loss 0.3749871597182555 micro_f1_score 0.7500909421607858
 
----------
Epoch 27/40
time = 481.60 secondes

Train loss 0.010150250975175778 micro_f1_score 0.9896309850564198 
 
time = 21.35 secondes

Val loss 0.3722500503063202 micro_f1_score 0.7519769949676491
 
----------
Epoch 28/40
time = 479.56 secondes

Train loss 0.008488708035758201 micro_f1_score 0.9917028240846464 
 
time = 21.31 secondes

Val loss 0.38058910787594125 micro_f1_score 0.7569546120058566
 
----------
Epoch 29/40
time = 482.50 secondes

Train loss 0.007876213088968291 micro_f1_score 0.9917374252751019 
 
time = 21.31 secondes

Val loss 0.3790156200039582 micro_f1_score 0.7688387331634511
 
----------
Epoch 30/40
time = 479.49 secondes

Train loss 0.0069553127022028 micro_f1_score 0.9931496422590959 
 
time = 21.38 secondes

Val loss 0.40231781684961476 micro_f1_score 0.7569120287253143
 
----------
Epoch 31/40
time = 477.96 secondes

Train loss 0.006333219492781764 micro_f1_score 0.9934228034824925 
 
time = 21.39 secondes

Val loss 0.39833657003817013 micro_f1_score 0.7574774774774775
 
----------
Epoch 32/40
time = 481.77 secondes

Train loss 0.006024376541661384 micro_f1_score 0.9942196531791907 
 
time = 21.33 secondes

Val loss 0.4249064108875931 micro_f1_score 0.7473645946928391
 
----------
Epoch 33/40
time = 480.32 secondes

Train loss 0.005449058123230242 micro_f1_score 0.9951341899186498 
 
time = 21.38 secondes

Val loss 0.43413832368420774 micro_f1_score 0.7436357117246325
 
----------
Epoch 34/40
time = 476.99 secondes

Train loss 0.004763979761693983 micro_f1_score 0.9950967349576191 
 
time = 21.35 secondes

Val loss 0.42782222980358564 micro_f1_score 0.7565813198701768
 
----------
Epoch 35/40
time = 481.83 secondes

Train loss 0.003886343944982441 micro_f1_score 0.9958544099189899 
 
time = 21.28 secondes

Val loss 0.4276587938676115 micro_f1_score 0.7531135531135531
 
----------
Epoch 36/40
time = 482.29 secondes

Train loss 0.0035281957507666793 micro_f1_score 0.9963887938571483 
 
time = 21.30 secondes

Val loss 0.4391678042831968 micro_f1_score 0.7576936681995047
 
----------
Epoch 37/40
time = 477.33 secondes

Train loss 0.0029300547321654776 micro_f1_score 0.9973013037363639 
 
time = 21.33 secondes

Val loss 0.43653180560127636 micro_f1_score 0.7603946441155743
 
----------
Epoch 38/40
time = 480.53 secondes

Train loss 0.0022062027396247217 micro_f1_score 0.9982517482517483 
 
time = 21.64 secondes

Val loss 0.41980122884766 micro_f1_score 0.7684478371501273
 
----------
Epoch 39/40
time = 481.06 secondes

Train loss 0.0017150635140694192 micro_f1_score 0.9986320109439125 
 
time = 21.31 secondes

Val loss 0.4261671492799384 micro_f1_score 0.765273311897106
 
----------
Epoch 40/40
time = 478.53 secondes

Train loss 0.001572329271296333 micro_f1_score 0.9986322188449848 
 
time = 21.63 secondes

Val loss 0.43352668988900106 micro_f1_score 0.764069264069264
 
----------
best_f1_socre 0.7688387331634511 best_epoch 29

average train time 482.53280969262124

average val time 21.413088923692705
 
time = 23.42 secondes

test_f1_score 0.7416201117318435

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_512_4
----------
Epoch 1/40
time = 646.96 secondes

Train loss 0.2343730515389292 micro_f1_score 0.6679765959304865 
 
time = 24.80 secondes

Val loss 0.1995851693094754 micro_f1_score 0.7173738991192954
 
----------
Epoch 2/40
time = 640.74 secondes

Train loss 0.16464260105748435 micro_f1_score 0.7798003040637713 
 
time = 24.42 secondes

Val loss 0.1850915532864508 micro_f1_score 0.736842105263158
 
----------
Epoch 3/40
time = 641.05 secondes

Train loss 0.142699418938509 micro_f1_score 0.8147757683886141 
 
time = 24.36 secondes

Val loss 0.18866662742173085 micro_f1_score 0.7453560371517027
 
----------
Epoch 4/40
time = 638.91 secondes

Train loss 0.1267741395110214 micro_f1_score 0.8420503770255094 
 
time = 24.40 secondes

Val loss 0.18211590400973304 micro_f1_score 0.7512418800152847
 
----------
Epoch 5/40
time = 643.21 secondes

Train loss 0.11359581265363607 micro_f1_score 0.8613802176773114 
 
time = 24.39 secondes

Val loss 0.18033502214267605 micro_f1_score 0.7624765478424015
 
----------
Epoch 6/40
time = 640.73 secondes

Train loss 0.09957498404289689 micro_f1_score 0.8821199397924424 
 
time = 24.37 secondes

Val loss 0.2005210796706989 micro_f1_score 0.7540612013600303
 
----------
Epoch 7/40
time = 639.53 secondes

Train loss 0.08855115045197644 micro_f1_score 0.8980155442458673 
 
time = 24.68 secondes

Val loss 0.20666683489670518 micro_f1_score 0.7536982248520709
 
----------
Epoch 8/40
time = 640.36 secondes

Train loss 0.07822853348108831 micro_f1_score 0.9133006856023506 
 
time = 24.35 secondes

Val loss 0.20527489721530773 micro_f1_score 0.7616300036062026
 
----------
Epoch 9/40
time = 639.27 secondes

Train loss 0.06871235455545756 micro_f1_score 0.9256359315959644 
 
time = 24.36 secondes

Val loss 0.22665596411364977 micro_f1_score 0.7582025677603423
 
----------
Epoch 10/40
time = 639.39 secondes

Train loss 0.05835162635456334 micro_f1_score 0.9384424063422976 
 
time = 24.34 secondes

Val loss 0.24787933892402492 micro_f1_score 0.7578427916813536
 
----------
Epoch 11/40
time = 642.53 secondes

Train loss 0.05129510842154625 micro_f1_score 0.9448433800411442 
 
time = 24.70 secondes

Val loss 0.2587306687577826 micro_f1_score 0.7544996400287978
 
----------
Epoch 12/40
time = 640.18 secondes

Train loss 0.04428522507869848 micro_f1_score 0.955655129789864 
 
time = 24.61 secondes

Val loss 0.25876649322568396 micro_f1_score 0.7464887640449437
 
----------
Epoch 13/40
time = 636.56 secondes

Train loss 0.03852073012784056 micro_f1_score 0.960070916518924 
 
time = 24.61 secondes

Val loss 0.28599422168536265 micro_f1_score 0.7442857142857142
 
----------
Epoch 14/40
time = 640.73 secondes

Train loss 0.03489966519944672 micro_f1_score 0.9638026437134952 
 
time = 24.42 secondes

Val loss 0.2692755849146452 micro_f1_score 0.7551537070524412
 
----------
Epoch 15/40
time = 640.16 secondes

Train loss 0.03046030300388297 micro_f1_score 0.9686239870962786 
 
time = 24.51 secondes

Val loss 0.28773965979697275 micro_f1_score 0.751788268955651
 
----------
Epoch 16/40
time = 638.50 secondes

Train loss 0.025576638783588335 micro_f1_score 0.9739936420391436 
 
time = 24.34 secondes

Val loss 0.3027244814839519 micro_f1_score 0.7557767507998578
 
----------
Epoch 17/40
time = 637.06 secondes

Train loss 0.024050314269796317 micro_f1_score 0.9760428626100268 
 
time = 24.36 secondes

Val loss 0.28857245797016584 micro_f1_score 0.7603485838779955
 
----------
Epoch 18/40
time = 639.36 secondes

Train loss 0.020414372062025305 micro_f1_score 0.9793388429752066 
 
time = 24.35 secondes

Val loss 0.3115846458517137 micro_f1_score 0.7605531295487626
 
----------
Epoch 19/40
time = 638.42 secondes

Train loss 0.018811839112596637 micro_f1_score 0.9813526939243409 
 
time = 24.36 secondes

Val loss 0.31112502368747214 micro_f1_score 0.7562225475841874
 
----------
Epoch 20/40
time = 643.20 secondes

Train loss 0.01722967722128053 micro_f1_score 0.9820277025222268 
 
time = 24.26 secondes

Val loss 0.3212854387085946 micro_f1_score 0.7604017216642756
 
----------
Epoch 21/40
time = 641.27 secondes

Train loss 0.015435141312568826 micro_f1_score 0.9837718125930734 
 
time = 24.34 secondes

Val loss 0.33631988297231863 micro_f1_score 0.7509967379485322
 
----------
Epoch 22/40
time = 641.75 secondes

Train loss 0.013681470529807664 micro_f1_score 0.9866422410503015 
 
time = 24.40 secondes

Val loss 0.3434929575343601 micro_f1_score 0.7509157509157509
 
----------
Epoch 23/40
time = 640.40 secondes

Train loss 0.012577964363882931 micro_f1_score 0.987026862026862 
 
time = 24.33 secondes

Val loss 0.3425180218258842 micro_f1_score 0.7634914885910902
 
----------
Epoch 24/40
time = 641.84 secondes

Train loss 0.010429390770360106 micro_f1_score 0.9896404631322364 
 
time = 24.26 secondes

Val loss 0.3660399394690013 micro_f1_score 0.7528049221860297
 
----------
Epoch 25/40
time = 642.54 secondes

Train loss 0.01051892384059249 micro_f1_score 0.988577520560463 
 
time = 24.24 secondes

Val loss 0.36332389523015646 micro_f1_score 0.7682793743179339
 
----------
Epoch 26/40
time = 642.91 secondes

Train loss 0.008718408039652507 micro_f1_score 0.9917015607156452 
 
time = 24.37 secondes

Val loss 0.37485854425391213 micro_f1_score 0.7601156069364163
 
----------
Epoch 27/40
time = 643.50 secondes

Train loss 0.00829275476652919 micro_f1_score 0.9914029214850882 
 
time = 24.39 secondes

Val loss 0.37648020977856683 micro_f1_score 0.7593738623953404
 
----------
Epoch 28/40
time = 643.70 secondes

Train loss 0.007768931121369803 micro_f1_score 0.9920176372206174 
 
time = 24.37 secondes

Val loss 0.37444664721117643 micro_f1_score 0.7600147004777655
 
----------
Epoch 29/40
time = 642.87 secondes

Train loss 0.00676448419774035 micro_f1_score 0.9932691942046621 
 
time = 24.32 secondes

Val loss 0.3976260553129384 micro_f1_score 0.754898336414048
 
----------
Epoch 30/40
time = 643.89 secondes

Train loss 0.006949868093274622 micro_f1_score 0.9931522483451266 
 
time = 24.32 secondes

Val loss 0.4020518728944122 micro_f1_score 0.7655296229802514
 
----------
Epoch 31/40
time = 643.25 secondes

Train loss 0.005780637768665664 micro_f1_score 0.9941053432211446 
 
time = 24.37 secondes

Val loss 0.3927822858095169 micro_f1_score 0.7629246676514032
 
----------
Epoch 32/40
time = 646.00 secondes

Train loss 0.004902753245656017 micro_f1_score 0.9955542045065927 
 
time = 24.34 secondes

Val loss 0.4079929100441151 micro_f1_score 0.7620427381383558
 
----------
Epoch 33/40
time = 643.30 secondes

Train loss 0.003909801942941854 micro_f1_score 0.9958563010834443 
 
time = 24.35 secondes

Val loss 0.41295277657078916 micro_f1_score 0.7524680073126143
 
----------
Epoch 34/40
time = 642.68 secondes

Train loss 0.004304242836846865 micro_f1_score 0.996045326640809 
 
time = 24.50 secondes

Val loss 0.41108323218392545 micro_f1_score 0.7602764641687886
 
----------
Epoch 35/40
time = 641.11 secondes

Train loss 0.0031841670262610523 micro_f1_score 0.9968833143291524 
 
time = 24.30 secondes

Val loss 0.41389702712414694 micro_f1_score 0.7646198830409355
 
----------
Epoch 36/40
time = 643.44 secondes

Train loss 0.002945445227571228 micro_f1_score 0.9973769245390611 
 
time = 24.54 secondes

Val loss 0.43315307417365373 micro_f1_score 0.7652802893309221
 
----------
Epoch 37/40
time = 642.74 secondes

Train loss 0.002221690745900232 micro_f1_score 0.9978345933214299 
 
time = 24.48 secondes

Val loss 0.409484897236355 micro_f1_score 0.7661261261261262
 
----------
Epoch 38/40
time = 641.43 secondes

Train loss 0.002168356036077262 micro_f1_score 0.9980620891438994 
 
time = 24.34 secondes

Val loss 0.4099273008645558 micro_f1_score 0.7697320782041999
 
----------
Epoch 39/40
time = 642.39 secondes

Train loss 0.001491861037186584 micro_f1_score 0.9988982181528058 
 
time = 24.24 secondes

Val loss 0.41528908908367157 micro_f1_score 0.7691751085383501
 
----------
Epoch 40/40
time = 641.71 secondes

Train loss 0.0017654029535733244 micro_f1_score 0.9982522796352583 
 
time = 24.50 secondes

Val loss 0.4295667089888307 micro_f1_score 0.763873775843308
 
----------
best_f1_socre 0.7697320782041999 best_epoch 38

average train time 641.4892225444316

average val time 24.407225584983827
 
time = 26.44 secondes

test_f1_score 0.7496532593619971

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_256_4
----------
Epoch 1/40
time = 848.50 secondes

Train loss 0.21848407860274788 micro_f1_score 0.702062120304612 
 
time = 29.61 secondes

Val loss 0.1830080199192782 micro_f1_score 0.7486252945797329
 
----------
Epoch 2/40
time = 850.04 secondes

Train loss 0.1517576983013937 micro_f1_score 0.8027370637298566 
 
time = 29.22 secondes

Val loss 0.1716741727756672 micro_f1_score 0.766304347826087
 
----------
Epoch 3/40
time = 850.19 secondes

Train loss 0.13003484962491302 micro_f1_score 0.8359830630342734 
 
time = 29.46 secondes

Val loss 0.1745762597830569 micro_f1_score 0.7658926532165969
 
----------
Epoch 4/40
time = 850.44 secondes

Train loss 0.11525450722054319 micro_f1_score 0.8615421246874876 
 
time = 29.16 secondes

Val loss 0.16933761427148444 micro_f1_score 0.7857402691887959
 
----------
Epoch 5/40
time = 861.21 secondes

Train loss 0.10228281380054918 micro_f1_score 0.8794952681388014 
 
time = 29.31 secondes

Val loss 0.1755405751163842 micro_f1_score 0.7832618025751074
 
----------
Epoch 6/40
time = 861.90 secondes

Train loss 0.09104668209580956 micro_f1_score 0.8955539872971064 
 
time = 29.14 secondes

Val loss 0.17987467244756025 micro_f1_score 0.7815674891146589
 
----------
Epoch 7/40
time = 864.64 secondes

Train loss 0.0799791474314826 micro_f1_score 0.9098511195342113 
 
time = 29.20 secondes

Val loss 0.19690885571915595 micro_f1_score 0.7798652959943283
 
----------
Epoch 8/40
time = 865.10 secondes

Train loss 0.07049665453902504 micro_f1_score 0.9223826012527719 
 
time = 29.23 secondes

Val loss 0.2163904700122896 micro_f1_score 0.7684623617552623
 
----------
Epoch 9/40
time = 865.14 secondes

Train loss 0.062438030826213124 micro_f1_score 0.9339172900275182 
 
time = 29.37 secondes

Val loss 0.2114233219476997 micro_f1_score 0.7864285714285714
 
----------
Epoch 10/40
time = 865.02 secondes

Train loss 0.05483463416798963 micro_f1_score 0.9435054410743227 
 
time = 29.20 secondes

Val loss 0.22389745529069274 micro_f1_score 0.7817863397548162
 
----------
Epoch 11/40
time = 861.97 secondes

Train loss 0.0488919565185636 micro_f1_score 0.9513284559106663 
 
time = 29.41 secondes

Val loss 0.22560245391042505 micro_f1_score 0.7875611460517121
 
----------
Epoch 12/40
time = 869.79 secondes

Train loss 0.0424086879599034 micro_f1_score 0.9579173706035938 
 
time = 29.83 secondes

Val loss 0.23345045024742844 micro_f1_score 0.7803138373751783
 
----------
Epoch 13/40
time = 870.90 secondes

Train loss 0.038539809115684116 micro_f1_score 0.960367957071675 
 
time = 29.95 secondes

Val loss 0.24990602252913302 micro_f1_score 0.7829787234042552
 
----------
Epoch 14/40
time = 875.18 secondes

Train loss 0.03372237259031248 micro_f1_score 0.966726791265303 
 
time = 30.86 secondes

Val loss 0.24517426664223435 micro_f1_score 0.7851851851851852
 
----------
Epoch 15/40
time = 869.78 secondes

Train loss 0.029172024060972034 micro_f1_score 0.9716825688776487 
 
time = 29.69 secondes

Val loss 0.25641407793174026 micro_f1_score 0.7883995703544575
 
----------
Epoch 16/40
time = 865.90 secondes

Train loss 0.026198342260862657 micro_f1_score 0.9730679604232724 
 
time = 29.62 secondes

Val loss 0.2676828636742029 micro_f1_score 0.7865008880994672
 
----------
Epoch 17/40
time = 864.94 secondes

Train loss 0.024072562258266886 micro_f1_score 0.9768739727074652 
 
time = 29.34 secondes

Val loss 0.2661292064385336 micro_f1_score 0.7902225465158701
 
----------
Epoch 18/40
time = 866.58 secondes

Train loss 0.021103735282589253 micro_f1_score 0.9782592053892676 
 
time = 30.02 secondes

Val loss 0.2758213852028378 micro_f1_score 0.7853782120883099
 
----------
Epoch 19/40
time = 856.80 secondes

Train loss 0.018305714781131795 micro_f1_score 0.9820094526604666 
 
time = 29.56 secondes

Val loss 0.30670447763605196 micro_f1_score 0.7754662840746055
 
----------
Epoch 20/40
time = 863.93 secondes

Train loss 0.016940612330936265 micro_f1_score 0.9830741079597437 
 
time = 30.95 secondes

Val loss 0.3101062162733469 micro_f1_score 0.7750637987604813
 
----------
Epoch 21/40
time = 861.94 secondes

Train loss 0.014482649382065378 micro_f1_score 0.9857339029600245 
 
time = 29.47 secondes

Val loss 0.31857345778434004 micro_f1_score 0.7769066286528867
 
----------
Epoch 22/40
time = 860.35 secondes

Train loss 0.01498372964692084 micro_f1_score 0.9850678043577632 
 
time = 29.35 secondes

Val loss 0.31382164908725707 micro_f1_score 0.7821025827573663
 
----------
Epoch 23/40
time = 861.96 secondes

Train loss 0.013240945795084232 micro_f1_score 0.986697160282066 
 
time = 29.89 secondes

Val loss 0.3097403718799841 micro_f1_score 0.7860550458715596
 
----------
Epoch 24/40
time = 859.24 secondes

Train loss 0.010942203457769308 micro_f1_score 0.9898274088467254 
 
time = 30.27 secondes

Val loss 0.32214842058840343 micro_f1_score 0.7795132582637123
 
----------
Epoch 25/40
time = 861.57 secondes

Train loss 0.010088404416429667 micro_f1_score 0.9905929847278822 
 
time = 30.08 secondes

Val loss 0.3199410866029927 micro_f1_score 0.7863554757630162
 
----------
Epoch 26/40
time = 863.13 secondes

Train loss 0.00958530268144714 micro_f1_score 0.99089627852055 
 
time = 29.78 secondes

Val loss 0.34977040317703467 micro_f1_score 0.7761413843888071
 
----------
Epoch 27/40
time = 860.24 secondes

Train loss 0.009518208734896704 micro_f1_score 0.9914373786961982 
 
time = 30.19 secondes

Val loss 0.34310625298101394 micro_f1_score 0.7818247385503065
 
----------
Epoch 28/40
time = 858.33 secondes

Train loss 0.008116034286302254 micro_f1_score 0.9918166939443535 
 
time = 29.98 secondes

Val loss 0.35516809061413906 micro_f1_score 0.7757750540735401
 
----------
Epoch 29/40
time = 875.14 secondes

Train loss 0.00701843236936445 micro_f1_score 0.9930714176945332 
 
time = 42.80 secondes

Val loss 0.3684395919324922 micro_f1_score 0.7756708407871199
 
----------
Epoch 30/40
time = 969.49 secondes

Train loss 0.007409883605820748 micro_f1_score 0.9922757885925193 
 
time = 42.65 secondes

Val loss 0.35530419366770105 micro_f1_score 0.7848837209302325
 
----------
Epoch 31/40
time = 977.18 secondes

Train loss 0.006040050087025733 micro_f1_score 0.9941039978698315 
 
time = 41.24 secondes

Val loss 0.3608383543667246 micro_f1_score 0.7836944546456115
 
----------
Epoch 32/40
time = 980.40 secondes

Train loss 0.0051047527283190975 micro_f1_score 0.9948671153188092 
 
time = 42.61 secondes

Val loss 0.35651020827840585 micro_f1_score 0.7839484055893945
 
----------
Epoch 33/40
time = 978.58 secondes

Train loss 0.004643279037344991 micro_f1_score 0.9953269252687967 
 
time = 41.75 secondes

Val loss 0.36664876415104164 micro_f1_score 0.7842576028622541
 
----------
Epoch 34/40
time = 975.39 secondes

Train loss 0.0035337238525604696 micro_f1_score 0.9964272139870771 
 
time = 43.08 secondes

Val loss 0.37629750163340175 micro_f1_score 0.77942774357117
 
----------
Epoch 35/40
time = 972.38 secondes

Train loss 0.003181599613765741 micro_f1_score 0.9963492546394889 
 
time = 42.43 secondes

Val loss 0.3760260883413377 micro_f1_score 0.782041998551774
 
----------
Epoch 36/40
time = 976.17 secondes

Train loss 0.0030457809059510977 micro_f1_score 0.9972263383867168 
 
time = 42.81 secondes

Val loss 0.3717833625488594 micro_f1_score 0.7868147617341453
 
----------
Epoch 37/40
time = 975.59 secondes

Train loss 0.0024700399060174614 micro_f1_score 0.9977569098581911 
 
time = 41.89 secondes

Val loss 0.3848967835551403 micro_f1_score 0.7827348567283279
 
----------
Epoch 38/40
time = 918.71 secondes

Train loss 0.001883979134042936 micro_f1_score 0.9981386514719848 
 
time = 29.64 secondes

Val loss 0.3680126298157895 micro_f1_score 0.7893792608539648
 
----------
Epoch 39/40
time = 856.90 secondes

Train loss 0.0013461568982559068 micro_f1_score 0.9989363318644582 
 
time = 30.75 secondes

Val loss 0.37348235911521754 micro_f1_score 0.793178519593614
 
----------
Epoch 40/40
time = 857.86 secondes

Train loss 0.0013962484150954527 micro_f1_score 0.9987843792736667 
 
time = 30.15 secondes

Val loss 0.3788378109208873 micro_f1_score 0.7892473118279569
 
----------
best_f1_socre 0.793178519593614 best_epoch 39

average train time 886.2119479835034

average val time 32.57384214401245
 
time = 31.55 secondes

test_f1_score 0.7773851590106007

----------
/vol/fob-vol3/nebenf20/wubingti/data/22_wub_longdocclassification/trainer.py:255: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_512_4
----------
Epoch 1/40
time = 1703.75 secondes

Train loss 0.2222544349193036 micro_f1_score 0.6938654465344858 
 
time = 36.81 secondes

Val loss 0.20390289313480503 micro_f1_score 0.7086017430845017
 
----------
Epoch 2/40
time = 1701.86 secondes

Train loss 0.15094627251361942 micro_f1_score 0.804294105732226 
 
time = 36.49 secondes

Val loss 0.17296442401702286 micro_f1_score 0.7688787185354691
 
----------
Epoch 3/40
time = 1702.45 secondes

Train loss 0.12865507931591155 micro_f1_score 0.8406987874920229 
 
time = 35.90 secondes

Val loss 0.16692747090195045 micro_f1_score 0.7871939736346515
 
----------
Epoch 4/40
time = 1701.55 secondes

Train loss 0.11128419152217674 micro_f1_score 0.8658396417105941 
 
time = 36.17 secondes

Val loss 0.16968067515580382 micro_f1_score 0.7834976268711209
 
----------
Epoch 5/40
time = 1701.04 secondes

Train loss 0.09804437235501166 micro_f1_score 0.8875832971885966 
 
time = 36.48 secondes

Val loss 0.1749211866103235 micro_f1_score 0.783284023668639
 
----------
Epoch 6/40
time = 1702.21 secondes

Train loss 0.08703859430477694 micro_f1_score 0.9018210299588799 
 
time = 36.33 secondes

Val loss 0.17598913793192536 micro_f1_score 0.7932031814895155
 
----------
Epoch 7/40
time = 1700.97 secondes

Train loss 0.07667290458019387 micro_f1_score 0.9167643610785464 
 
time = 35.80 secondes

Val loss 0.18373624956021545 micro_f1_score 0.7911460192788291
 
----------
Epoch 8/40
time = 1702.11 secondes

Train loss 0.0677442122812945 micro_f1_score 0.9284770271320844 
 
time = 38.24 secondes

Val loss 0.1925035587466154 micro_f1_score 0.7860515800944425
 
----------
Epoch 9/40
time = 1703.58 secondes

Train loss 0.05793037448950984 micro_f1_score 0.9411810321429955 
 
time = 36.09 secondes

Val loss 0.2079447907868956 micro_f1_score 0.7831412103746397
 
----------
Epoch 10/40
time = 1702.29 secondes

Train loss 0.05247232778425756 micro_f1_score 0.946700212889491 
 
time = 37.73 secondes

Val loss 0.21374799598191604 micro_f1_score 0.7968356706220785
 
----------
Epoch 11/40
time = 1703.00 secondes

Train loss 0.045768414327858 micro_f1_score 0.9563269876819709 
 
time = 36.13 secondes

Val loss 0.24206710032752302 micro_f1_score 0.7811731647348086
 
----------
Epoch 12/40
time = 1704.59 secondes

Train loss 0.04163163745154937 micro_f1_score 0.958609016771811 
 
time = 36.71 secondes

Val loss 0.24283951673595633 micro_f1_score 0.7856635911994322
 
----------
Epoch 13/40
time = 1701.15 secondes

Train loss 0.03478852894937468 micro_f1_score 0.9658959981565404 
 
time = 37.06 secondes

Val loss 0.24449221002029592 micro_f1_score 0.7830121341898644
 
----------
Epoch 14/40
time = 1701.70 secondes

Train loss 0.030395432768666464 micro_f1_score 0.9697085889570551 
 
time = 36.55 secondes

Val loss 0.25761478064490145 micro_f1_score 0.7839088643645424
 
----------
Epoch 15/40
time = 1702.34 secondes

Train loss 0.028651313686969916 micro_f1_score 0.9720636137191033 
 
time = 36.57 secondes

Val loss 0.275200009468149 micro_f1_score 0.7842565597667639
 
----------
Epoch 16/40
time = 1703.07 secondes

Train loss 0.024597451374928997 micro_f1_score 0.9758228284608605 
 
time = 35.94 secondes

Val loss 0.2834363420967196 micro_f1_score 0.779184247538678
 
----------
Epoch 17/40
time = 1702.29 secondes

Train loss 0.0243287177922137 micro_f1_score 0.9751857241326493 
 
time = 35.88 secondes

Val loss 0.2647557378303809 micro_f1_score 0.7857142857142857
 
----------
Epoch 18/40
time = 1701.29 secondes

Train loss 0.019795755478700303 micro_f1_score 0.9804790464911946 
 
time = 36.45 secondes

Val loss 0.2905712062340291 micro_f1_score 0.7752890173410404
 
----------
Epoch 19/40
time = 1704.68 secondes

Train loss 0.018155340185212726 micro_f1_score 0.9823140685282097 
 
time = 35.85 secondes

Val loss 0.3016818182878807 micro_f1_score 0.7743835112256165
 
----------
Epoch 20/40
time = 1705.46 secondes

Train loss 0.0161974209151021 micro_f1_score 0.9846341556411332 
 
time = 36.86 secondes

Val loss 0.305716374125637 micro_f1_score 0.7867298578199052
 
----------
Epoch 21/40
time = 1704.23 secondes

Train loss 0.014914380189929124 micro_f1_score 0.9850598368778108 
 
time = 35.96 secondes

Val loss 0.3178192340448254 micro_f1_score 0.7773801002147459
 
----------
Epoch 22/40
time = 1704.88 secondes

Train loss 0.0135759148817522 micro_f1_score 0.9865050320219579 
 
time = 35.87 secondes

Val loss 0.32139030318768297 micro_f1_score 0.7745132743362831
 
----------
Epoch 23/40
time = 1704.78 secondes

Train loss 0.012477505923225754 micro_f1_score 0.9875085688171225 
 
time = 36.63 secondes

Val loss 0.3216369613760807 micro_f1_score 0.7797728105533164
 
----------
Epoch 24/40
time = 1704.28 secondes

Train loss 0.011502125489169958 micro_f1_score 0.9891776541422148 
 
time = 36.01 secondes

Val loss 0.3248446787722775 micro_f1_score 0.7810433504775899
 
----------
Epoch 25/40
time = 1705.04 secondes

Train loss 0.010566036878489294 micro_f1_score 0.9893815413891531 
 
time = 36.48 secondes

Val loss 0.35788893589719395 micro_f1_score 0.775045871559633
 
----------
Epoch 26/40
time = 1705.49 secondes

Train loss 0.010052031442535588 micro_f1_score 0.9895305897133285 
 
time = 36.59 secondes

Val loss 0.34757234217202077 micro_f1_score 0.778104335047759
 
----------
Epoch 27/40
time = 1707.03 secondes

Train loss 0.00976165224405253 micro_f1_score 0.9906228558359381 
 
time = 35.86 secondes

Val loss 0.36509558998170444 micro_f1_score 0.776889534883721
 
----------
Epoch 28/40
time = 1705.59 secondes

Train loss 0.007371381309270431 micro_f1_score 0.9927351565174395 
 
time = 36.32 secondes

Val loss 0.3530117540818746 micro_f1_score 0.782069970845481
 
----------
Epoch 29/40
time = 1704.97 secondes

Train loss 0.00704528708081705 micro_f1_score 0.9933457545914294 
 
time = 35.84 secondes

Val loss 0.3421736498836611 micro_f1_score 0.7938931297709924
 
----------
Epoch 30/40
time = 1705.49 secondes

Train loss 0.005847595617091724 micro_f1_score 0.9942530922930541 
 
time = 36.59 secondes

Val loss 0.34871247264205435 micro_f1_score 0.7892636924192964
 
----------
Epoch 31/40
time = 1706.40 secondes

Train loss 0.006247060799193548 micro_f1_score 0.9940315529367041 
 
time = 36.38 secondes

Val loss 0.367829723192043 micro_f1_score 0.781491002570694
 
----------
Epoch 32/40
time = 1705.56 secondes

Train loss 0.005366466260451702 micro_f1_score 0.9949053303931261 
 
time = 36.28 secondes

Val loss 0.3840910231725114 micro_f1_score 0.7824817518248175
 
----------
Epoch 33/40
time = 1705.76 secondes

Train loss 0.004662135241698445 micro_f1_score 0.9962740476009428 
 
time = 35.85 secondes

Val loss 0.38022495903929726 micro_f1_score 0.7831149927219797
 
----------
Epoch 34/40
time = 1707.37 secondes

Train loss 0.004409154654068344 micro_f1_score 0.9956292045152217 
 
time = 35.85 secondes

Val loss 0.39283171943465217 micro_f1_score 0.7771387491013659
 
----------
Epoch 35/40
time = 1708.09 secondes

Train loss 0.0035848799073711552 micro_f1_score 0.9968080255357956 
 
time = 36.30 secondes

Val loss 0.37199448635343646 micro_f1_score 0.7841700256504215
 
----------
Epoch 36/40
time = 1706.67 secondes

Train loss 0.0031295349404883367 micro_f1_score 0.9971133394105134 
 
time = 36.19 secondes

Val loss 0.38234642966360344 micro_f1_score 0.7914438502673797
 
----------
Epoch 37/40
time = 1706.91 secondes

Train loss 0.0023980777691666936 micro_f1_score 0.9979104137380798 
 
time = 35.75 secondes

Val loss 0.3820590811674712 micro_f1_score 0.7849854227405247
 
----------
Epoch 38/40
time = 1705.63 secondes

Train loss 0.002109949454468012 micro_f1_score 0.9980244662259706 
 
time = 35.89 secondes

Val loss 0.3740202997063027 micro_f1_score 0.7899891186071818
 
----------
Epoch 39/40
time = 1706.70 secondes

Train loss 0.0016319439072135168 micro_f1_score 0.9985566697052568 
 
time = 36.51 secondes

Val loss 0.38129509374743603 micro_f1_score 0.7895302975977053
 
----------
Epoch 40/40
time = 1705.96 secondes

Train loss 0.0011533201100023649 micro_f1_score 0.999088422971741 
 
time = 36.04 secondes

Val loss 0.3907385866661541 micro_f1_score 0.7846649946255821
 
----------
best_f1_socre 0.7968356706220785 best_epoch 10

average train time 1704.2050608336926

average val time 36.3304352581501
 
time = 37.73 secondes

test_f1_score 0.774762072611914

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_256_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 768.00 MiB (GPU 1; 79.20 GiB total capacity; 72.05 GiB already allocated; 555.31 MiB free; 76.08 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_512_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 2.82 GiB (GPU 1; 79.20 GiB total capacity; 70.56 GiB already allocated; 2.53 GiB free; 74.09 GiB reserved in total by PyTorch)
datasets imported
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_64_5
----------
Epoch 1/40
time = 570.93 secondes

Train loss 0.2682809697346644 micro_f1_score 0.6131099063578117 
 
time = 20.41 secondes

Val loss 0.22639013753562678 micro_f1_score 0.6455026455026455
 
----------
Epoch 2/40
time = 571.03 secondes

Train loss 0.17260500361119305 micro_f1_score 0.763819510382694 
 
time = 20.07 secondes

Val loss 0.20036313145375642 micro_f1_score 0.7159922928709056
 
----------
Epoch 3/40
time = 571.13 secondes

Train loss 0.14540408599685442 micro_f1_score 0.8110931174089069 
 
time = 19.73 secondes

Val loss 0.1779583097725618 micro_f1_score 0.7571049640015157
 
----------
Epoch 4/40
time = 569.54 secondes

Train loss 0.12507830593265123 micro_f1_score 0.8422023499218029 
 
time = 20.03 secondes

Val loss 0.1896886962359069 micro_f1_score 0.7603494113178882
 
----------
Epoch 5/40
time = 568.77 secondes

Train loss 0.10840326140441739 micro_f1_score 0.868706182281708 
 
time = 20.27 secondes

Val loss 0.19809814609709334 micro_f1_score 0.7529673590504451
 
----------
Epoch 6/40
time = 573.36 secondes

Train loss 0.09370102879616457 micro_f1_score 0.8882993466640269 
 
time = 19.90 secondes

Val loss 0.21003587983670782 micro_f1_score 0.749006861682918
 
----------
Epoch 7/40
time = 568.83 secondes

Train loss 0.08024039457065497 micro_f1_score 0.9076197957580519 
 
time = 19.87 secondes

Val loss 0.22911723654289715 micro_f1_score 0.7563636363636363
 
----------
Epoch 8/40
time = 570.53 secondes

Train loss 0.06811307621280754 micro_f1_score 0.9241169115348546 
 
time = 20.28 secondes

Val loss 0.2539292047502565 micro_f1_score 0.7511471937875044
 
----------
Epoch 9/40
time = 569.22 secondes

Train loss 0.05538144080673118 micro_f1_score 0.9398703466480339 
 
time = 20.01 secondes

Val loss 0.26744118858067717 micro_f1_score 0.752990851513019
 
----------
Epoch 10/40
time = 568.42 secondes

Train loss 0.046696784752756695 micro_f1_score 0.9507727975270479 
 
time = 19.88 secondes

Val loss 0.28158431084918195 micro_f1_score 0.7642805495300072
 
----------
Epoch 11/40
time = 571.57 secondes

Train loss 0.03888528808502497 micro_f1_score 0.9583365481058561 
 
time = 20.09 secondes

Val loss 0.30085898007525774 micro_f1_score 0.7548364403798804
 
----------
Epoch 12/40
time = 570.64 secondes

Train loss 0.03296480396655514 micro_f1_score 0.9633982409647809 
 
time = 20.93 secondes

Val loss 0.31123569693233144 micro_f1_score 0.7573712255772647
 
----------
Epoch 13/40
time = 567.93 secondes

Train loss 0.027765268820698734 micro_f1_score 0.9708432440727384 
 
time = 21.41 secondes

Val loss 0.336778885516964 micro_f1_score 0.7574990964943983
 
----------
Epoch 14/40
time = 571.61 secondes

Train loss 0.023806966247397837 micro_f1_score 0.9741194486983155 
 
time = 20.55 secondes

Val loss 0.37283529267936455 micro_f1_score 0.7504514265077645
 
----------
Epoch 15/40
time = 571.78 secondes

Train loss 0.021584618262299505 micro_f1_score 0.9769701606732977 
 
time = 20.15 secondes

Val loss 0.3667963721224519 micro_f1_score 0.7578151837856406
 
----------
Epoch 16/40
time = 569.41 secondes

Train loss 0.019179342807688005 micro_f1_score 0.9790177718326009 
 
time = 19.81 secondes

Val loss 0.38274278052029065 micro_f1_score 0.7593850554165178
 
----------
Epoch 17/40
time = 569.77 secondes

Train loss 0.01690903064718277 micro_f1_score 0.9822042312686168 
 
time = 20.45 secondes

Val loss 0.4345292295100259 micro_f1_score 0.7465437788018432
 
----------
Epoch 18/40
time = 572.18 secondes

Train loss 0.015376828998738379 micro_f1_score 0.9834902962595798 
 
time = 20.28 secondes

Val loss 0.40810381755477093 micro_f1_score 0.759568933481977
 
----------
Epoch 19/40
time = 570.42 secondes

Train loss 0.013766950409547929 micro_f1_score 0.9851690876510732 
 
time = 19.85 secondes

Val loss 0.409595572069043 micro_f1_score 0.7655961609449982
 
----------
Epoch 20/40
time = 572.50 secondes

Train loss 0.012575028022711359 micro_f1_score 0.9864705209802203 
 
time = 19.88 secondes

Val loss 0.43192811188150626 micro_f1_score 0.7569923719578642
 
----------
Epoch 21/40
time = 570.11 secondes

Train loss 0.010444203336413233 micro_f1_score 0.9891817766265428 
 
time = 19.78 secondes

Val loss 0.43547130583739674 micro_f1_score 0.7606038820992091
 
----------
Epoch 22/40
time = 571.79 secondes

Train loss 0.009889218544543969 micro_f1_score 0.9892105463110706 
 
time = 19.87 secondes

Val loss 0.44384659900039924 micro_f1_score 0.7637969094922737
 
----------
Epoch 23/40
time = 570.49 secondes

Train loss 0.009538374492183574 micro_f1_score 0.9897582333904436 
 
time = 20.54 secondes

Val loss 0.4827096093384946 micro_f1_score 0.7483138090166844
 
----------
Epoch 24/40
time = 569.67 secondes

Train loss 0.008450952241735256 micro_f1_score 0.9915505823247317 
 
time = 20.48 secondes

Val loss 0.45780063591531067 micro_f1_score 0.7524972253052165
 
----------
Epoch 25/40
time = 570.32 secondes

Train loss 0.007753651002041134 micro_f1_score 0.9918637365979774 
 
time = 19.78 secondes

Val loss 0.4745483586534125 micro_f1_score 0.751357220412595
 
----------
Epoch 26/40
time = 572.71 secondes

Train loss 0.007042894096056451 micro_f1_score 0.9923937019852438 
 
time = 19.79 secondes

Val loss 0.48800445995369895 micro_f1_score 0.7518573551263001
 
----------
Epoch 27/40
time = 569.11 secondes

Train loss 0.006084141880678585 micro_f1_score 0.9934968625213919 
 
time = 19.98 secondes

Val loss 0.5327343449729388 micro_f1_score 0.7457754412316936
 
----------
Epoch 28/40
time = 572.31 secondes

Train loss 0.005815605650888494 micro_f1_score 0.9940688920994601 
 
time = 20.49 secondes

Val loss 0.48830105352108594 micro_f1_score 0.7690623859905145
 
----------
Epoch 29/40
time = 568.37 secondes

Train loss 0.005064643423814617 micro_f1_score 0.9951771541411917 
 
time = 20.47 secondes

Val loss 0.5120675303652639 micro_f1_score 0.7612103536274152
 
----------
Epoch 30/40
time = 569.24 secondes

Train loss 0.004185774504072174 micro_f1_score 0.9955943790353209 
 
time = 19.98 secondes

Val loss 0.589037437907985 micro_f1_score 0.7411242603550295
 
----------
Epoch 31/40
time = 570.60 secondes

Train loss 0.005647687098591944 micro_f1_score 0.9950218506555196 
 
time = 19.79 secondes

Val loss 0.5297805102633648 micro_f1_score 0.7608535688005886
 
----------
Epoch 32/40
time = 570.31 secondes

Train loss 0.0036835483288332645 micro_f1_score 0.9962370291535215 
 
time = 19.87 secondes

Val loss 0.55667743844087 micro_f1_score 0.7509578544061303
 
----------
Epoch 33/40
time = 568.44 secondes

Train loss 0.0038271199135255433 micro_f1_score 0.9962370291535215 
 
time = 19.82 secondes

Val loss 0.5269638099631325 micro_f1_score 0.7642105263157895
 
----------
Epoch 34/40
time = 569.78 secondes

Train loss 0.002805297891155443 micro_f1_score 0.9973025340982485 
 
time = 20.61 secondes

Val loss 0.5456835820538098 micro_f1_score 0.764405543398979
 
----------
Epoch 35/40
time = 570.70 secondes

Train loss 0.0020793083052856607 micro_f1_score 0.9977200182398541 
 
time = 20.45 secondes

Val loss 0.5735313061807976 micro_f1_score 0.7553461399057629
 
----------
Epoch 36/40
time = 566.51 secondes

Train loss 0.002296087335081039 micro_f1_score 0.9977958501178079 
 
time = 19.98 secondes

Val loss 0.5501082102294828 micro_f1_score 0.7618702428416092
 
----------
Epoch 37/40
time = 569.13 secondes

Train loss 0.001770722164062406 micro_f1_score 0.9982534740678868 
 
time = 19.83 secondes

Val loss 0.5458888992911479 micro_f1_score 0.7650273224043715
 
----------
Epoch 38/40
time = 567.30 secondes

Train loss 0.001348233612043669 micro_f1_score 0.998670263287869 
 
time = 20.03 secondes

Val loss 0.5597286639643497 micro_f1_score 0.7629388346000725
 
----------
Epoch 39/40
time = 568.61 secondes

Train loss 0.0009756788858020308 micro_f1_score 0.9991261730177425 
 
time = 20.29 secondes

Val loss 0.5657585869069959 micro_f1_score 0.7646632874728458
 
----------
Epoch 40/40
time = 567.04 secondes

Train loss 0.0008207178068382213 micro_f1_score 0.9992401793176809 
 
time = 20.49 secondes

Val loss 0.5981142242423824 micro_f1_score 0.7627840909090909
 
----------
best_f1_socre 0.7690623859905145 best_epoch 28

average train time 570.0528804957867

average val time 20.154291993379594
 
time = 20.88 secondes

test_f1_score 0.7549157303370787

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_128_5
----------
Epoch 1/40
Attention type 'block_sparse' is not possible if sequence_length: 1024 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 1408 with config.block_size = 128, config.num_random_blocks = 3. Changing attention type to 'original_full'...
time = 399.38 secondes

Train loss 0.27039492533013626 micro_f1_score 0.6161494021228 
 
time = 17.08 secondes

Val loss 0.21507591668699608 micro_f1_score 0.6696392379408188
 
----------
Epoch 2/40
time = 400.80 secondes

Train loss 0.17174862170474486 micro_f1_score 0.764175416924027 
 
time = 17.87 secondes

Val loss 0.1869814884467203 micro_f1_score 0.7272018706157444
 
----------
Epoch 3/40
time = 401.96 secondes

Train loss 0.14867110428576533 micro_f1_score 0.8067807634456684 
 
time = 17.38 secondes

Val loss 0.1895589608638013 micro_f1_score 0.7395277989337395
 
----------
Epoch 4/40
time = 399.90 secondes

Train loss 0.12963403000890672 micro_f1_score 0.8353584936026394 
 
time = 17.13 secondes

Val loss 0.1996523294536794 micro_f1_score 0.7556881760537113
 
----------
Epoch 5/40
time = 401.55 secondes

Train loss 0.11494370586834513 micro_f1_score 0.8580683863227355 
 
time = 17.56 secondes

Val loss 0.19424334730281204 micro_f1_score 0.7615553925165077
 
----------
Epoch 6/40
time = 401.90 secondes

Train loss 0.0998411328053555 micro_f1_score 0.8808409361364538 
 
time = 17.96 secondes

Val loss 0.202370679280797 micro_f1_score 0.7663828211773417
 
----------
Epoch 7/40
time = 403.14 secondes

Train loss 0.08538672090885607 micro_f1_score 0.8993780017321471 
 
time = 17.20 secondes

Val loss 0.2202959751985112 micro_f1_score 0.760229720028715
 
----------
Epoch 8/40
time = 400.75 secondes

Train loss 0.07406875597950709 micro_f1_score 0.9163371569510237 
 
time = 17.47 secondes

Val loss 0.2243583406336972 micro_f1_score 0.7636103151862464
 
----------
Epoch 9/40
time = 399.96 secondes

Train loss 0.06494949729777537 micro_f1_score 0.9284798570374111 
 
time = 17.82 secondes

Val loss 0.26577272492109755 micro_f1_score 0.7430007178750898
 
----------
Epoch 10/40
time = 401.52 secondes

Train loss 0.05595696179892633 micro_f1_score 0.9394926693041658 
 
time = 17.05 secondes

Val loss 0.29169926071753266 micro_f1_score 0.7405289492494639
 
----------
Epoch 11/40
time = 399.83 secondes

Train loss 0.047880394046555634 micro_f1_score 0.9484201998379692 
 
time = 17.07 secondes

Val loss 0.300290012335191 micro_f1_score 0.7511471937875044
 
----------
Epoch 12/40
time = 401.15 secondes

Train loss 0.04129730038122517 micro_f1_score 0.9558755582935469 
 
time = 17.57 secondes

Val loss 0.30788885472250765 micro_f1_score 0.7544610992148465
 
----------
Epoch 13/40
time = 400.14 secondes

Train loss 0.03509444895951363 micro_f1_score 0.9630199123548858 
 
time = 17.06 secondes

Val loss 0.31495455151698626 micro_f1_score 0.7565217391304347
 
----------
Epoch 14/40
time = 401.23 secondes

Train loss 0.030585537250321527 micro_f1_score 0.9697783232338728 
 
time = 17.15 secondes

Val loss 0.33771261377412765 micro_f1_score 0.7562787407145384
 
----------
Epoch 15/40
time = 403.16 secondes

Train loss 0.025612505371598444 micro_f1_score 0.9737517722343563 
 
time = 17.22 secondes

Val loss 0.3612328279213827 micro_f1_score 0.7504350852767142
 
----------
Epoch 16/40
time = 400.81 secondes

Train loss 0.02253761426194336 micro_f1_score 0.9770374049593092 
 
time = 17.50 secondes

Val loss 0.37336185140932193 micro_f1_score 0.7435443933498409
 
----------
Epoch 17/40
time = 400.36 secondes

Train loss 0.02028041608764807 micro_f1_score 0.9780156757790098 
 
time = 17.25 secondes

Val loss 0.3910895131650518 micro_f1_score 0.7514569763455606
 
----------
Epoch 18/40
time = 402.87 secondes

Train loss 0.018065068913940423 micro_f1_score 0.9822191697191697 
 
time = 17.13 secondes

Val loss 0.392662628874427 micro_f1_score 0.7561475409836065
 
----------
Epoch 19/40
time = 401.55 secondes

Train loss 0.017369249487937603 micro_f1_score 0.9813162510485777 
 
time = 17.62 secondes

Val loss 0.40940064448313634 micro_f1_score 0.7464839523981247
 
----------
Epoch 20/40
time = 403.29 secondes

Train loss 0.014854168728164918 micro_f1_score 0.9844512195121952 
 
time = 17.19 secondes

Val loss 0.4538491036071152 micro_f1_score 0.7371428571428571
 
----------
Epoch 21/40
time = 399.66 secondes

Train loss 0.014456472277143384 micro_f1_score 0.9846587232098671 
 
time = 17.30 secondes

Val loss 0.4419677080433877 micro_f1_score 0.7444794952681388
 
----------
Epoch 22/40
time = 400.35 secondes

Train loss 0.01254937395124134 micro_f1_score 0.9863295380983208 
 
time = 17.73 secondes

Val loss 0.44062851222812155 micro_f1_score 0.7533502354219486
 
----------
Epoch 23/40
time = 401.95 secondes

Train loss 0.01078268180793751 micro_f1_score 0.9893799246317232 
 
time = 17.29 secondes

Val loss 0.4409202649945118 micro_f1_score 0.7590486039296793
 
----------
Epoch 24/40
time = 400.13 secondes

Train loss 0.008994525601142946 micro_f1_score 0.9905197030268418 
 
time = 17.30 secondes

Val loss 0.446277919362803 micro_f1_score 0.7598870056497176
 
----------
Epoch 25/40
time = 400.43 secondes

Train loss 0.00808312934365801 micro_f1_score 0.9910622599170882 
 
time = 17.69 secondes

Val loss 0.47221094724096235 micro_f1_score 0.753956834532374
 
----------
Epoch 26/40
time = 400.40 secondes

Train loss 0.008426310179886452 micro_f1_score 0.9914016131486836 
 
time = 17.26 secondes

Val loss 0.4774809528569706 micro_f1_score 0.7637161667885881
 
----------
Epoch 27/40
time = 399.86 secondes

Train loss 0.008022721764027992 micro_f1_score 0.9917154366496922 
 
time = 17.11 secondes

Val loss 0.4806651512130362 micro_f1_score 0.7550143266475644
 
----------
Epoch 28/40
time = 401.91 secondes

Train loss 0.007291398895111277 micro_f1_score 0.9925092208829233 
 
time = 17.52 secondes

Val loss 0.5022283736311022 micro_f1_score 0.7536646406864497
 
----------
Epoch 29/40
time = 400.75 secondes

Train loss 0.005424967457735408 micro_f1_score 0.994410008746245 
 
time = 17.18 secondes

Val loss 0.5123583686644914 micro_f1_score 0.7562366357804703
 
----------
Epoch 30/40
time = 401.71 secondes

Train loss 0.005302614030071775 micro_f1_score 0.9949041679342867 
 
time = 17.01 secondes

Val loss 0.5280560918274473 micro_f1_score 0.7515238436715668
 
----------
Epoch 31/40
time = 401.79 secondes

Train loss 0.005571559683783168 micro_f1_score 0.994294842537654 
 
time = 17.08 secondes

Val loss 0.533463753759861 micro_f1_score 0.7524822695035461
 
----------
Epoch 32/40
time = 400.10 secondes

Train loss 0.004882783572673787 micro_f1_score 0.9950169272319221 
 
time = 17.74 secondes

Val loss 0.5547702495680481 micro_f1_score 0.7487615003538569
 
----------
Epoch 33/40
time = 401.75 secondes

Train loss 0.00403935840330067 micro_f1_score 0.9959357313784328 
 
time = 17.61 secondes

Val loss 0.5383230921674947 micro_f1_score 0.7499120647203659
 
----------
Epoch 34/40
time = 399.76 secondes

Train loss 0.0032166907925194528 micro_f1_score 0.996809480401094 
 
time = 17.07 secondes

Val loss 0.5625148684274955 micro_f1_score 0.7521663778162911
 
----------
Epoch 35/40
time = 401.20 secondes

Train loss 0.002781673774519699 micro_f1_score 0.9970348969816772 
 
time = 17.10 secondes

Val loss 0.5655870950612866 micro_f1_score 0.7508871540099362
 
----------
Epoch 36/40
time = 399.85 secondes

Train loss 0.0025511730134198146 micro_f1_score 0.9975304889631854 
 
time = 17.70 secondes

Val loss 0.5742997549107818 micro_f1_score 0.7515657620041754
 
----------
Epoch 37/40
time = 403.42 secondes

Train loss 0.002139465705955969 micro_f1_score 0.9980238656228624 
 
time = 17.16 secondes

Val loss 0.5783608558236576 micro_f1_score 0.7497396737244013
 
----------
Epoch 38/40
time = 402.63 secondes

Train loss 0.0019072474943370892 micro_f1_score 0.998176291793313 
 
time = 17.02 secondes

Val loss 0.5608405341989682 micro_f1_score 0.7611052815669815
 
----------
Epoch 39/40
time = 400.37 secondes

Train loss 0.0010432093933182465 micro_f1_score 0.9989364934670313 
 
time = 18.07 secondes

Val loss 0.5487602899675487 micro_f1_score 0.7667256637168143
 
----------
Epoch 40/40
time = 403.36 secondes

Train loss 0.0007738432766862323 micro_f1_score 0.9992784170749307 
 
time = 17.55 secondes

Val loss 0.5606519769205421 micro_f1_score 0.7640529896168994
 
----------
best_f1_socre 0.7667256637168143 best_epoch 39

average train time 401.16668915748596

average val time 17.370194172859193
 
time = 18.55 secondes

test_f1_score 0.7527472527472526

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_64_5
----------
Epoch 1/40
time = 1666.45 secondes

Train loss 0.26648986375143935 micro_f1_score 0.6077164482185712 
 
time = 30.83 secondes

Val loss 0.21724126109334288 micro_f1_score 0.6780982073265783
 
----------
Epoch 2/40
time = 1668.47 secondes

Train loss 0.1675915514765022 micro_f1_score 0.7720200906529463 
 
time = 30.80 secondes

Val loss 0.17830648028948268 micro_f1_score 0.7488514548238897
 
----------
Epoch 3/40
time = 1667.56 secondes

Train loss 0.13959440509075516 micro_f1_score 0.8190728689644091 
 
time = 31.18 secondes

Val loss 0.18611190199363428 micro_f1_score 0.7497151538169389
 
----------
Epoch 4/40
time = 1668.34 secondes

Train loss 0.12269288789447363 micro_f1_score 0.8480618918487798 
 
time = 31.09 secondes

Val loss 0.17769903909476076 micro_f1_score 0.765011119347665
 
----------
Epoch 5/40
time = 1668.09 secondes

Train loss 0.10652375831394582 micro_f1_score 0.8724566542633205 
 
time = 31.03 secondes

Val loss 0.18242234333616789 micro_f1_score 0.7769679300291544
 
----------
Epoch 6/40
time = 1667.29 secondes

Train loss 0.09533791527621918 micro_f1_score 0.8881799204379849 
 
time = 30.60 secondes

Val loss 0.1983270513840386 micro_f1_score 0.7763682493657122
 
----------
Epoch 7/40
time = 1668.55 secondes

Train loss 0.08242325102742594 micro_f1_score 0.9066050301653217 
 
time = 30.56 secondes

Val loss 0.2018251064859453 micro_f1_score 0.7806475081847944
 
----------
Epoch 8/40
time = 1669.15 secondes

Train loss 0.06950765073148382 micro_f1_score 0.9237612173234492 
 
time = 30.72 secondes

Val loss 0.22253348727206715 micro_f1_score 0.7792776358993069
 
----------
Epoch 9/40
Exception
CUDA out of memory. Tried to allocate 384.00 MiB (GPU 1; 79.20 GiB total capacity; 73.85 GiB already allocated; 200.31 MiB free; 75.87 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_128_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 1.12 GiB (GPU 1; 79.20 GiB total capacity; 72.48 GiB already allocated; 892.31 MiB free; 75.20 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_64_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 180.00 MiB (GPU 1; 79.20 GiB total capacity; 72.36 GiB already allocated; 168.31 MiB free; 75.90 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_128_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 2.62 GiB (GPU 1; 79.20 GiB total capacity; 75.62 GiB already allocated; 63.31 MiB free; 76.56 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_256_5
----------
Epoch 1/40
time = 528.16 secondes

Train loss 0.22714926459499307 micro_f1_score 0.6822756738314568 
 
time = 22.58 secondes

Val loss 0.2011259714843797 micro_f1_score 0.7066246056782336
 
----------
Epoch 2/40
time = 519.57 secondes

Train loss 0.16100429899587826 micro_f1_score 0.7849826566006936 
 
time = 22.22 secondes

Val loss 0.1837459011156051 micro_f1_score 0.7412915851272015
 
----------
Epoch 3/40
time = 516.55 secondes

Train loss 0.13825048006701846 micro_f1_score 0.8207634821248233 
 
time = 22.18 secondes

Val loss 0.1819462942295387 micro_f1_score 0.7548608463591306
 
----------
Epoch 4/40
time = 517.36 secondes

Train loss 0.12095329152369821 micro_f1_score 0.8507791531466571 
 
time = 22.09 secondes

Val loss 0.18629052380069358 micro_f1_score 0.7549167927382753
 
----------
Epoch 5/40
time = 521.17 secondes

Train loss 0.1079899645066476 micro_f1_score 0.8680489354941213 
 
time = 21.44 secondes

Val loss 0.19424413767505866 micro_f1_score 0.7535159141376758
 
----------
Epoch 6/40
time = 520.71 secondes

Train loss 0.09513369687908405 micro_f1_score 0.8886337543053962 
 
time = 21.93 secondes

Val loss 0.22274876605780397 micro_f1_score 0.7314285714285715
 
----------
Epoch 7/40
time = 520.05 secondes

Train loss 0.08345550390984025 micro_f1_score 0.9060487038491751 
 
time = 21.89 secondes

Val loss 0.21442107807417385 micro_f1_score 0.7562658917544497
 
----------
Epoch 8/40
time = 520.71 secondes

Train loss 0.0734349877095303 micro_f1_score 0.9187985782916064 
 
time = 21.38 secondes

Val loss 0.22824594305186977 micro_f1_score 0.7534784159828755
 
----------
Epoch 9/40
time = 517.79 secondes

Train loss 0.06253646665361819 micro_f1_score 0.9338363997043837 
 
time = 21.78 secondes

Val loss 0.2459674004892834 micro_f1_score 0.7479050279329609
 
----------
Epoch 10/40
time = 520.95 secondes

Train loss 0.05556611141131201 micro_f1_score 0.9417061243499186 
 
time = 21.53 secondes

Val loss 0.24357373682690447 micro_f1_score 0.7570977917981073
 
----------
Epoch 11/40
time = 521.51 secondes

Train loss 0.04962256067466024 micro_f1_score 0.9493724657269743 
 
time = 21.40 secondes

Val loss 0.2607094351141179 micro_f1_score 0.756158514816137
 
----------
Epoch 12/40
time = 519.44 secondes

Train loss 0.04173876388577392 micro_f1_score 0.9580602883355176 
 
time = 21.57 secondes

Val loss 0.28230797450561995 micro_f1_score 0.7495717711545049
 
----------
Epoch 13/40
time = 521.65 secondes

Train loss 0.03682744145091321 micro_f1_score 0.9629942742958153 
 
time = 21.41 secondes

Val loss 0.2827295873497353 micro_f1_score 0.7536945812807883
 
----------
Epoch 14/40
time = 517.99 secondes

Train loss 0.032768365288329486 micro_f1_score 0.9671545465022473 
 
time = 21.45 secondes

Val loss 0.2955980514649485 micro_f1_score 0.7518796992481204
 
----------
Epoch 15/40
time = 516.38 secondes

Train loss 0.02927722037897806 micro_f1_score 0.9701022211974483 
 
time = 21.56 secondes

Val loss 0.3080037300948237 micro_f1_score 0.7571783055654024
 
----------
Epoch 16/40
time = 519.26 secondes

Train loss 0.025965317808561497 micro_f1_score 0.9747068291561278 
 
time = 21.57 secondes

Val loss 0.3253332396022609 micro_f1_score 0.7452930728241562
 
----------
Epoch 17/40
time = 519.09 secondes

Train loss 0.024386830893261276 micro_f1_score 0.9761777096897741 
 
time = 21.96 secondes

Val loss 0.3263729102298862 micro_f1_score 0.7547169811320755
 
----------
Epoch 18/40
time = 519.17 secondes

Train loss 0.020415844319195294 micro_f1_score 0.9794795368565861 
 
time = 21.43 secondes

Val loss 0.33016688694230845 micro_f1_score 0.7600713012477719
 
----------
Epoch 19/40
time = 519.52 secondes

Train loss 0.019614436721580253 micro_f1_score 0.9802018040055037 
 
time = 21.36 secondes

Val loss 0.3429016547857738 micro_f1_score 0.7534391534391535
 
----------
Epoch 20/40
time = 521.17 secondes

Train loss 0.01739392088986262 micro_f1_score 0.9823370083546332 
 
time = 22.09 secondes

Val loss 0.3455551178728948 micro_f1_score 0.7557471264367815
 
----------
Epoch 21/40
time = 517.58 secondes

Train loss 0.015494095264233414 micro_f1_score 0.9849230886675063 
 
time = 21.84 secondes

Val loss 0.35138555237504304 micro_f1_score 0.7609720710917663
 
----------
Epoch 22/40
time = 523.07 secondes

Train loss 0.014394989889954125 micro_f1_score 0.9852705487292988 
 
time = 22.07 secondes

Val loss 0.350874767195983 micro_f1_score 0.7551686615886833
 
----------
Epoch 23/40
time = 523.47 secondes

Train loss 0.012766990331861584 micro_f1_score 0.9867683508102955 
 
time = 21.96 secondes

Val loss 0.3650728117247097 micro_f1_score 0.7533599709407918
 
----------
Epoch 24/40
time = 517.90 secondes

Train loss 0.011408252632067128 micro_f1_score 0.9883016423427199 
 
time = 22.06 secondes

Val loss 0.3664094605406777 micro_f1_score 0.7602956705385427
 
----------
Epoch 25/40
time = 521.27 secondes

Train loss 0.010757090880845983 micro_f1_score 0.9892268453309985 
 
time = 21.71 secondes

Val loss 0.38684511233548646 micro_f1_score 0.7546902654867257
 
----------
Epoch 26/40
time = 518.91 secondes

Train loss 0.00950920357437582 micro_f1_score 0.9902949571836347 
 
time = 21.49 secondes

Val loss 0.40328355402242944 micro_f1_score 0.7446731672083784
 
----------
Epoch 27/40
time = 520.96 secondes

Train loss 0.008408021000696363 micro_f1_score 0.9914412872304005 
 
time = 22.25 secondes

Val loss 0.40582576406295184 micro_f1_score 0.7494646680942184
 
----------
Epoch 28/40
time = 521.00 secondes

Train loss 0.008102301282069839 micro_f1_score 0.9917795707109149 
 
time = 22.26 secondes

Val loss 0.4140648950563102 micro_f1_score 0.7506225542511561
 
----------
Epoch 29/40
time = 517.70 secondes

Train loss 0.007607912765301948 micro_f1_score 0.9927018397445644 
 
time = 21.53 secondes

Val loss 0.4045593137623834 micro_f1_score 0.7638297872340426
 
----------
Epoch 30/40
time = 514.89 secondes

Train loss 0.0057474159567039 micro_f1_score 0.9938431134083308 
 
time = 22.10 secondes

Val loss 0.39240701508815173 micro_f1_score 0.7687224669603524
 
----------
Epoch 31/40
time = 512.62 secondes

Train loss 0.006093148540520331 micro_f1_score 0.9941008563273073 
 
time = 21.80 secondes

Val loss 0.41055819074638555 micro_f1_score 0.7657754010695188
 
----------
Epoch 32/40
time = 515.42 secondes

Train loss 0.005558172062100729 micro_f1_score 0.9943330924580687 
 
time = 21.51 secondes

Val loss 0.4070031537872846 micro_f1_score 0.7677929547088426
 
----------
Epoch 33/40
time = 516.23 secondes

Train loss 0.004351357031227635 micro_f1_score 0.9954004637548941 
 
time = 21.82 secondes

Val loss 0.42338622862198316 micro_f1_score 0.7583670467083486
 
----------
Epoch 34/40
time = 517.21 secondes

Train loss 0.0036074545245978413 micro_f1_score 0.9965421590606832 
 
time = 21.69 secondes

Val loss 0.4110700055712559 micro_f1_score 0.7652237710931769
 
----------
Epoch 35/40
time = 515.57 secondes

Train loss 0.003351136950565175 micro_f1_score 0.9968814178139499 
 
time = 21.41 secondes

Val loss 0.42731672232268286 micro_f1_score 0.7621097954790097
 
----------
Epoch 36/40
time = 517.81 secondes

Train loss 0.00333588462895719 micro_f1_score 0.9969999620248357 
 
time = 21.49 secondes

Val loss 0.42200947418564655 micro_f1_score 0.7654232424677189
 
----------
Epoch 37/40
time = 515.05 secondes

Train loss 0.0023809632174274778 micro_f1_score 0.9976064739181641 
 
time = 21.43 secondes

Val loss 0.4326331222155055 micro_f1_score 0.7660336796847008
 
----------
Epoch 38/40
time = 514.73 secondes

Train loss 0.002726941346099101 micro_f1_score 0.9980252164666565 
 
time = 21.51 secondes

Val loss 0.4380081562233753 micro_f1_score 0.7620738636363636
 
----------
Epoch 39/40
time = 513.97 secondes

Train loss 0.0016776671651609217 micro_f1_score 0.998480358635362 
 
time = 21.96 secondes

Val loss 0.42885920209962813 micro_f1_score 0.7713774597495529
 
----------
Epoch 40/40
time = 514.05 secondes

Train loss 0.001400466467892837 micro_f1_score 0.9988223226835846 
 
time = 22.23 secondes

Val loss 0.4296101309725496 micro_f1_score 0.7685185185185185
 
----------
best_f1_socre 0.7713774597495529 best_epoch 39

average train time 518.691488289833

average val time 21.77311775684357
 
time = 23.68 secondes

test_f1_score 0.7578288100208769

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_512_5
----------
Epoch 1/40
time = 667.24 secondes

Train loss 0.23315276433084461 micro_f1_score 0.6742485783915516 
 
time = 25.42 secondes

Val loss 0.19956448649773834 micro_f1_score 0.7021617293835068
 
----------
Epoch 2/40
time = 663.00 secondes

Train loss 0.16010174570722624 micro_f1_score 0.785411227154047 
 
time = 24.77 secondes

Val loss 0.18927384082411156 micro_f1_score 0.7357032457496138
 
----------
Epoch 3/40
time = 661.31 secondes

Train loss 0.13835858914262808 micro_f1_score 0.8186042752656888 
 
time = 24.71 secondes

Val loss 0.18398859474014062 micro_f1_score 0.748650732459522
 
----------
Epoch 4/40
time = 661.48 secondes

Train loss 0.12148008352766435 micro_f1_score 0.8477564102564102 
 
time = 24.86 secondes

Val loss 0.19799586153421245 micro_f1_score 0.738356683074593
 
----------
Epoch 5/40
time = 663.32 secondes

Train loss 0.10700727568788303 micro_f1_score 0.871956071940156 
 
time = 24.75 secondes

Val loss 0.2030465496612377 micro_f1_score 0.7455489614243322
 
----------
Epoch 6/40
time = 661.65 secondes

Train loss 0.09325084767497338 micro_f1_score 0.89256655344024 
 
time = 24.65 secondes

Val loss 0.21589291156803975 micro_f1_score 0.7426130295478818
 
----------
Epoch 7/40
time = 663.80 secondes

Train loss 0.08268534989003931 micro_f1_score 0.9061738104600865 
 
time = 24.59 secondes

Val loss 0.21455328203127033 micro_f1_score 0.7611567297393786
 
----------
Epoch 8/40
time = 661.01 secondes

Train loss 0.07157565458921028 micro_f1_score 0.9213527022805373 
 
time = 24.84 secondes

Val loss 0.22678264083920932 micro_f1_score 0.7559334041799504
 
----------
Epoch 9/40
time = 661.24 secondes

Train loss 0.06157082566773301 micro_f1_score 0.9353632894634374 
 
time = 25.09 secondes

Val loss 0.23951824546837416 micro_f1_score 0.7548111190306487
 
----------
Epoch 10/40
time = 656.96 secondes

Train loss 0.0545569905474245 micro_f1_score 0.9434313649414139 
 
time = 24.91 secondes

Val loss 0.24478607991191206 micro_f1_score 0.7525035765379112
 
----------
Epoch 11/40
time = 663.10 secondes

Train loss 0.0468711728007776 micro_f1_score 0.953069429410855 
 
time = 24.49 secondes

Val loss 0.2631723572973345 micro_f1_score 0.7608540925266905
 
----------
Epoch 12/40
time = 658.05 secondes

Train loss 0.04135084738304654 micro_f1_score 0.9586369068270306 
 
time = 24.66 secondes

Val loss 0.2715978488081791 micro_f1_score 0.7522281639928698
 
----------
Epoch 13/40
time = 663.49 secondes

Train loss 0.036732918350655286 micro_f1_score 0.9618109023714198 
 
time = 25.05 secondes

Val loss 0.2843582830468162 micro_f1_score 0.7551954913702007
 
----------
Epoch 14/40
time = 662.23 secondes

Train loss 0.031824850613200985 micro_f1_score 0.9680720110786275 
 
time = 24.49 secondes

Val loss 0.2826386679757814 micro_f1_score 0.7573878146661802
 
----------
Epoch 15/40
time = 665.50 secondes

Train loss 0.02833802576432726 micro_f1_score 0.9714789542283216 
 
time = 24.66 secondes

Val loss 0.3046616676156638 micro_f1_score 0.7569493941553812
 
----------
Epoch 16/40
time = 661.01 secondes

Train loss 0.02487377291912347 micro_f1_score 0.9748008578431372 
 
time = 24.83 secondes

Val loss 0.2963106380622895 micro_f1_score 0.75366568914956
 
----------
Epoch 17/40
time = 666.75 secondes

Train loss 0.02280623454869062 micro_f1_score 0.9774545454545456 
 
time = 24.87 secondes

Val loss 0.3042932701159696 micro_f1_score 0.7633477633477633
 
----------
Epoch 18/40
time = 663.96 secondes

Train loss 0.020111288442297395 micro_f1_score 0.9796808632763173 
 
time = 24.90 secondes

Val loss 0.3191202186414453 micro_f1_score 0.7676837725381415
 
----------
Epoch 19/40
time = 646.37 secondes

Train loss 0.018776241198563866 micro_f1_score 0.9809102015882712 
 
time = 24.58 secondes

Val loss 0.3381744499822132 micro_f1_score 0.7520055807464249
 
----------
Epoch 20/40
time = 647.60 secondes

Train loss 0.017540742353502628 micro_f1_score 0.9821857715048636 
 
time = 25.12 secondes

Val loss 0.3438085726782924 micro_f1_score 0.7581837381203802
 
----------
Epoch 21/40
time = 651.49 secondes

Train loss 0.014584908562267686 micro_f1_score 0.9853878142764487 
 
time = 24.58 secondes

Val loss 0.34536753216239274 micro_f1_score 0.7559055118110236
 
----------
Epoch 22/40
time = 651.99 secondes

Train loss 0.013712448922986467 micro_f1_score 0.9866137828458106 
 
time = 25.26 secondes

Val loss 0.35654263699152433 micro_f1_score 0.7580419580419581
 
----------
Epoch 23/40
time = 647.12 secondes

Train loss 0.012604069644884155 micro_f1_score 0.9870753745853825 
 
time = 25.17 secondes

Val loss 0.3628182853343057 micro_f1_score 0.7595744680851064
 
----------
Epoch 24/40
time = 647.67 secondes

Train loss 0.011894398419315473 micro_f1_score 0.9881374680550788 
 
time = 24.77 secondes

Val loss 0.36436569702918414 micro_f1_score 0.7597009611961552
 
----------
Epoch 25/40
time = 646.24 secondes

Train loss 0.010000347301179359 micro_f1_score 0.9899497487437185 
 
time = 24.63 secondes

Val loss 0.35904876338165315 micro_f1_score 0.767741935483871
 
----------
Epoch 26/40
time = 647.64 secondes

Train loss 0.008498466106001624 micro_f1_score 0.9915825557036755 
 
time = 25.18 secondes

Val loss 0.35345971486607536 micro_f1_score 0.7655677655677655
 
----------
Epoch 27/40
time = 646.68 secondes

Train loss 0.008523813104870527 micro_f1_score 0.9915018482527342 
 
time = 24.72 secondes

Val loss 0.38708051428443097 micro_f1_score 0.7613227893601725
 
----------
Epoch 28/40
time = 645.96 secondes

Train loss 0.007727008728443662 micro_f1_score 0.9919299581271412 
 
time = 25.13 secondes

Val loss 0.3696704867189048 micro_f1_score 0.7674247982391783
 
----------
Epoch 29/40
time = 648.28 secondes

Train loss 0.006293880619393947 micro_f1_score 0.9934546008067585 
 
time = 25.25 secondes

Val loss 0.39200024099135006 micro_f1_score 0.7610040014550746
 
----------
Epoch 30/40
time = 646.64 secondes

Train loss 0.0064628138586297745 micro_f1_score 0.9937954398386053 
 
time = 24.64 secondes

Val loss 0.4020978196478281 micro_f1_score 0.7553648068669527
 
----------
Epoch 31/40
time = 647.97 secondes

Train loss 0.005396751214648903 micro_f1_score 0.9946048632218846 
 
time = 24.62 secondes

Val loss 0.3926720190487924 micro_f1_score 0.7648787656135194
 
----------
Epoch 32/40
time = 646.62 secondes

Train loss 0.00550617742431371 micro_f1_score 0.9948281107392759 
 
time = 24.65 secondes

Val loss 0.39945754469906697 micro_f1_score 0.7676258992805755
 
----------
Epoch 33/40
time = 649.84 secondes

Train loss 0.005186936078118034 micro_f1_score 0.9949844213086101 
 
time = 24.93 secondes

Val loss 0.41401537019209783 micro_f1_score 0.7607926397735314
 
----------
Epoch 34/40
time = 647.75 secondes

Train loss 0.004978131915865831 micro_f1_score 0.9953994144709327 
 
time = 25.03 secondes

Val loss 0.4081910949994306 micro_f1_score 0.7660187185025198
 
----------
Epoch 35/40
time = 647.02 secondes

Train loss 0.0038752082734644575 micro_f1_score 0.9964648192496294 
 
time = 24.57 secondes

Val loss 0.42173068445236955 micro_f1_score 0.7558264610971676
 
----------
Epoch 36/40
time = 645.07 secondes

Train loss 0.00335678597151822 micro_f1_score 0.9967300380228137 
 
time = 24.69 secondes

Val loss 0.40864459767204814 micro_f1_score 0.7656646142701921
 
----------
Epoch 37/40
time = 648.47 secondes

Train loss 0.0027354294481123 micro_f1_score 0.9973004828713736 
 
time = 25.07 secondes

Val loss 0.4235752331184559 micro_f1_score 0.7624507345037622
 
----------
Epoch 38/40
time = 648.34 secondes

Train loss 0.0019034028986900486 micro_f1_score 0.9983659509785293 
 
time = 24.80 secondes

Val loss 0.42171674209540005 micro_f1_score 0.7620416966211359
 
----------
Epoch 39/40
time = 647.63 secondes

Train loss 0.001956102395886954 micro_f1_score 0.998403163257547 
 
time = 25.10 secondes

Val loss 0.4181059955573473 micro_f1_score 0.7621776504297996
 
----------
Epoch 40/40
time = 645.52 secondes

Train loss 0.0018653202222225986 micro_f1_score 0.9983272506082725 
 
time = 25.04 secondes

Val loss 0.4315485799166023 micro_f1_score 0.7592329545454547
 
----------
best_f1_socre 0.767741935483871 best_epoch 25

average train time 654.326267594099

average val time 24.851493459939956
 
time = 27.53 secondes

test_f1_score 0.7523642732049037

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_256_5
----------
Epoch 1/40
time = 855.93 secondes

Train loss 0.23375833207019814 micro_f1_score 0.6681785118309015 
 
time = 31.08 secondes

Val loss 0.19404898425106143 micro_f1_score 0.7152831652443753
 
----------
Epoch 2/40
time = 854.35 secondes

Train loss 0.15654846535206915 micro_f1_score 0.7921177472326967 
 
time = 29.83 secondes

Val loss 0.1790904516323668 micro_f1_score 0.7501945525291829
 
----------
Epoch 3/40
time = 852.81 secondes

Train loss 0.136477444260507 micro_f1_score 0.8252574439211304 
 
time = 29.89 secondes

Val loss 0.1720695076662986 micro_f1_score 0.7705041384499625
 
----------
Epoch 4/40
time = 856.94 secondes

Train loss 0.11927608105077131 micro_f1_score 0.8531124935610414 
 
time = 29.51 secondes

Val loss 0.172959106012446 micro_f1_score 0.7665165165165166
 
----------
Epoch 5/40
time = 855.30 secondes

Train loss 0.10632606782858167 micro_f1_score 0.8718594943687485 
 
time = 29.54 secondes

Val loss 0.18338031846968855 micro_f1_score 0.7710487444608567
 
----------
Epoch 6/40
time = 856.33 secondes

Train loss 0.0961371781572968 micro_f1_score 0.8867258026532695 
 
time = 29.40 secondes

Val loss 0.17804023873854857 micro_f1_score 0.7851690294438386
 
----------
Epoch 7/40
time = 853.86 secondes

Train loss 0.08391838425283765 micro_f1_score 0.9058855649596318 
 
time = 29.35 secondes

Val loss 0.19007810571643172 micro_f1_score 0.7803468208092486
 
----------
Epoch 8/40
time = 853.90 secondes

Train loss 0.07440132126379925 micro_f1_score 0.9195938849340646 
 
time = 29.34 secondes

Val loss 0.20081556692230898 micro_f1_score 0.7835497835497836
 
----------
Epoch 9/40
time = 853.95 secondes

Train loss 0.06657500468983173 micro_f1_score 0.9315854793670493 
 
time = 29.36 secondes

Val loss 0.21915533369193313 micro_f1_score 0.7746376811594203
 
----------
Epoch 10/40
time = 856.80 secondes

Train loss 0.05881879373478728 micro_f1_score 0.9403042706000464 
 
time = 29.44 secondes

Val loss 0.2132117440466021 micro_f1_score 0.7855611150822015
 
----------
Epoch 11/40
time = 854.06 secondes

Train loss 0.05376324259533404 micro_f1_score 0.9450261780104712 
 
time = 29.25 secondes

Val loss 0.22549758509534304 micro_f1_score 0.775305096913137
 
----------
Epoch 12/40
time = 855.35 secondes

Train loss 0.04988591251391414 micro_f1_score 0.9506771314250538 
 
time = 29.54 secondes

Val loss 0.23523697715069428 micro_f1_score 0.7782901738205037
 
----------
Epoch 13/40
time = 855.03 secondes

Train loss 0.04436497485122501 micro_f1_score 0.9559835466881943 
 
time = 29.62 secondes

Val loss 0.2452855772170864 micro_f1_score 0.7740315638450501
 
----------
Epoch 14/40
time = 856.42 secondes

Train loss 0.040413289277621355 micro_f1_score 0.9611277560113697 
 
time = 29.42 secondes

Val loss 0.25880738457695385 micro_f1_score 0.7714385719285966
 
----------
Epoch 15/40
time = 856.32 secondes

Train loss 0.03633918881999446 micro_f1_score 0.9654009732173645 
 
time = 29.69 secondes

Val loss 0.26198063363305857 micro_f1_score 0.7795527156549521
 
----------
Epoch 16/40
time = 854.46 secondes

Train loss 0.033518535348477665 micro_f1_score 0.9662680159460287 
 
time = 29.74 secondes

Val loss 0.2822595616100264 micro_f1_score 0.7690685413005273
 
----------
Epoch 17/40
time = 860.43 secondes

Train loss 0.030300073801992913 micro_f1_score 0.9704689771249331 
 
time = 29.84 secondes

Val loss 0.28471677640422444 micro_f1_score 0.7705322523792739
 
----------
Epoch 18/40
time = 854.55 secondes

Train loss 0.027430054960302538 micro_f1_score 0.9728717713587038 
 
time = 29.35 secondes

Val loss 0.2808862716936674 micro_f1_score 0.7794908569379706
 
----------
Epoch 19/40
time = 858.19 secondes

Train loss 0.02409587014667891 micro_f1_score 0.9771528998242531 
 
time = 29.96 secondes

Val loss 0.29117581248283386 micro_f1_score 0.7767988252569751
 
----------
Epoch 20/40
time = 858.10 secondes

Train loss 0.02244025381735048 micro_f1_score 0.9789055334760012 
 
time = 29.91 secondes

Val loss 0.3006033134020743 micro_f1_score 0.77726135144798
 
----------
Epoch 21/40
time = 855.94 secondes

Train loss 0.01987455121644244 micro_f1_score 0.9816100724914155 
 
time = 29.29 secondes

Val loss 0.3009358885102585 micro_f1_score 0.7709332349686462
 
----------
Epoch 22/40
time = 857.60 secondes

Train loss 0.01814429139999488 micro_f1_score 0.9817945879928247 
 
time = 29.94 secondes

Val loss 0.3169673639731329 micro_f1_score 0.7703916636722961
 
----------
Epoch 23/40
time = 854.97 secondes

Train loss 0.015792516034792873 micro_f1_score 0.9846893971211484 
 
time = 30.58 secondes

Val loss 0.3182894859890469 micro_f1_score 0.7741702741702742
 
----------
Epoch 24/40
time = 853.17 secondes

Train loss 0.014907762378785624 micro_f1_score 0.9856222112047596 
 
time = 30.60 secondes

Val loss 0.31172257881672655 micro_f1_score 0.7849111352919842
 
----------
Epoch 25/40
time = 856.77 secondes

Train loss 0.013313338658583628 micro_f1_score 0.9867266763292395 
 
time = 29.79 secondes

Val loss 0.32117658145115024 micro_f1_score 0.7781818181818182
 
----------
Epoch 26/40
time = 858.48 secondes

Train loss 0.01225546808018735 micro_f1_score 0.9881573435893531 
 
time = 29.51 secondes

Val loss 0.32735995569678605 micro_f1_score 0.784115523465704
 
----------
Epoch 27/40
time = 857.65 secondes

Train loss 0.010035841799837217 micro_f1_score 0.9909738355486156 
 
time = 29.86 secondes

Val loss 0.33519589778829795 micro_f1_score 0.777254617892068
 
----------
Epoch 28/40
time = 857.12 secondes

Train loss 0.009075446127361182 micro_f1_score 0.9913169319826338 
 
time = 29.89 secondes

Val loss 0.3430752761539866 micro_f1_score 0.772298767222625
 
----------
Epoch 29/40
time = 854.69 secondes

Train loss 0.009214748558876742 micro_f1_score 0.9913652097835597 
 
time = 29.82 secondes

Val loss 0.35713598864977475 micro_f1_score 0.7746327481189539
 
----------
Epoch 30/40
time = 858.89 secondes

Train loss 0.0074703294971825655 micro_f1_score 0.9927734672143619 
 
time = 29.65 secondes

Val loss 0.37606723899724054 micro_f1_score 0.7727756114852888
 
----------
Epoch 31/40
time = 856.03 secondes

Train loss 0.006912612552604605 micro_f1_score 0.9933452485074343 
 
time = 29.54 secondes

Val loss 0.3828932347356296 micro_f1_score 0.7680115273775217
 
----------
Epoch 32/40
time = 857.68 secondes

Train loss 0.006427013188357222 micro_f1_score 0.9939126464769442 
 
time = 29.31 secondes

Val loss 0.35985512958198296 micro_f1_score 0.7879858657243816
 
----------
Epoch 33/40
time = 857.79 secondes

Train loss 0.006364582870748137 micro_f1_score 0.9941538227924986 
 
time = 29.38 secondes

Val loss 0.35374579224430147 micro_f1_score 0.7830188679245284
 
----------
Epoch 34/40
time = 855.77 secondes

Train loss 0.004701958631775195 micro_f1_score 0.9957039121012813 
 
time = 29.73 secondes

Val loss 0.35980067414338474 micro_f1_score 0.786767349874146
 
----------
Epoch 35/40
time = 857.96 secondes

Train loss 0.0038394060073056475 micro_f1_score 0.996236743072186 
 
time = 29.47 secondes

Val loss 0.36104482632191454 micro_f1_score 0.7849192100538598
 
----------
Epoch 36/40
time = 857.94 secondes

Train loss 0.0037746807263649134 micro_f1_score 0.9963890683796419 
 
time = 29.33 secondes

Val loss 0.36833879406579206 micro_f1_score 0.7833935018050542
 
----------
Epoch 37/40
time = 856.67 secondes

Train loss 0.002740604560761943 micro_f1_score 0.9975291747443646 
 
time = 29.59 secondes

Val loss 0.37003422162083327 micro_f1_score 0.7881844380403459
 
----------
Epoch 38/40
time = 856.81 secondes

Train loss 0.002554522399441339 micro_f1_score 0.997720537953043 
 
time = 29.27 secondes

Val loss 0.3612653272318058 micro_f1_score 0.787725631768953
 
----------
Epoch 39/40
time = 856.12 secondes

Train loss 0.0023144266985934687 micro_f1_score 0.9979100961355777 
 
time = 29.80 secondes

Val loss 0.3638935012162709 micro_f1_score 0.7883211678832117
 
----------
Epoch 40/40
time = 858.54 secondes

Train loss 0.0019437882671620894 micro_f1_score 0.9979860926397386 
 
time = 29.69 secondes

Val loss 0.37239983995429804 micro_f1_score 0.7890173410404623
 
----------
best_f1_socre 0.7890173410404623 best_epoch 40

average train time 856.2413215100765

average val time 29.67782884836197
 
time = 31.72 secondes

test_f1_score 0.7747368421052632

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_512_5
----------
Epoch 1/40
time = 1745.62 secondes

Train loss 0.2217979458404017 micro_f1_score 0.6983033330462493 
 
time = 36.82 secondes

Val loss 0.180271927450524 micro_f1_score 0.7381137957911146
 
----------
Epoch 2/40
time = 1747.56 secondes

Train loss 0.15248482433391047 micro_f1_score 0.8022502833090497 
 
time = 36.01 secondes

Val loss 0.16615747257334287 micro_f1_score 0.7723765432098766
 
----------
Epoch 3/40
time = 1745.47 secondes

Train loss 0.12857029550448731 micro_f1_score 0.8392015968063873 
 
time = 36.26 secondes

Val loss 0.17092130088903865 micro_f1_score 0.7771556550951848
 
----------
Epoch 4/40
time = 1744.56 secondes

Train loss 0.11475255122525735 micro_f1_score 0.8601476307643463 
 
time = 36.70 secondes

Val loss 0.17066202129496902 micro_f1_score 0.7801994828223126
 
----------
Epoch 5/40
time = 1744.08 secondes

Train loss 0.10150646348525812 micro_f1_score 0.8809833353031556 
 
time = 36.14 secondes

Val loss 0.1730954626300296 micro_f1_score 0.78640059127864
 
----------
Epoch 6/40
time = 1745.55 secondes

Train loss 0.08851458424752628 micro_f1_score 0.9012292345756587 
 
time = 36.12 secondes

Val loss 0.18373060031015365 micro_f1_score 0.7759681505609843
 
----------
Epoch 7/40
time = 1747.17 secondes

Train loss 0.08037448664875449 micro_f1_score 0.9126327754478109 
 
time = 35.88 secondes

Val loss 0.2007798508542483 micro_f1_score 0.7770148174918684
 
----------
Epoch 8/40
time = 1744.01 secondes

Train loss 0.07093362491493961 micro_f1_score 0.9256604067638121 
 
time = 37.16 secondes

Val loss 0.20942414802361708 micro_f1_score 0.7746727980191016
 
----------
Epoch 9/40
time = 1745.21 secondes

Train loss 0.0620571469342722 micro_f1_score 0.9347116430903156 
 
time = 36.81 secondes

Val loss 0.20424683214943917 micro_f1_score 0.7814521926671459
 
----------
Epoch 10/40
time = 1743.73 secondes

Train loss 0.055826486114345424 micro_f1_score 0.9416644055971162 
 
time = 35.91 secondes

Val loss 0.2223480696805188 micro_f1_score 0.7760979121670266
 
----------
Epoch 11/40
time = 1747.06 secondes

Train loss 0.047872402307132744 micro_f1_score 0.9531135247961038 
 
time = 36.23 secondes

Val loss 0.2248308278742384 micro_f1_score 0.787707523843165
 
----------
Epoch 12/40
time = 1744.78 secondes

Train loss 0.04354968945420272 micro_f1_score 0.9566256698924317 
 
time = 36.25 secondes

Val loss 0.23395834844864782 micro_f1_score 0.7822410147991542
 
----------
Epoch 13/40
time = 1744.91 secondes

Train loss 0.03797274201537843 micro_f1_score 0.9631963119477526 
 
time = 35.76 secondes

Val loss 0.23539363153156687 micro_f1_score 0.7901498929336189
 
----------
Epoch 14/40
time = 1745.29 secondes

Train loss 0.033920813128268265 micro_f1_score 0.9655861883339101 
 
time = 36.02 secondes

Val loss 0.26214278208427744 micro_f1_score 0.7811280595956014
 
----------
Epoch 15/40
time = 1745.60 secondes

Train loss 0.029795057454507227 micro_f1_score 0.9709833422890919 
 
time = 36.11 secondes

Val loss 0.2631602538291548 micro_f1_score 0.7806086341118188
 
----------
Epoch 16/40
time = 1744.59 secondes

Train loss 0.027086055697348843 micro_f1_score 0.9731327243417517 
 
time = 36.51 secondes

Val loss 0.2784017521094103 micro_f1_score 0.78125
 
----------
Epoch 17/40
time = 1745.64 secondes

Train loss 0.02368085858672186 micro_f1_score 0.9766800689259046 
 
time = 36.25 secondes

Val loss 0.28422012387729084 micro_f1_score 0.7748917748917749
 
----------
Epoch 18/40
time = 1744.83 secondes

Train loss 0.0217442392110447 micro_f1_score 0.9777556568015622 
 
time = 36.17 secondes

Val loss 0.2811113144042062 micro_f1_score 0.782174688057041
 
----------
Epoch 19/40
time = 1746.16 secondes

Train loss 0.020037191899426213 micro_f1_score 0.9798010711553174 
 
time = 36.45 secondes

Val loss 0.300627310256489 micro_f1_score 0.7828212290502794
 
----------
Epoch 20/40
time = 1746.57 secondes

Train loss 0.018426945426518113 micro_f1_score 0.9815627743634768 
 
time = 36.20 secondes

Val loss 0.2929361985355127 micro_f1_score 0.7823252444766389
 
----------
Epoch 21/40
time = 1745.89 secondes

Train loss 0.016156556721837554 micro_f1_score 0.9834865184394188 
 
time = 36.49 secondes

Val loss 0.29148155230967726 micro_f1_score 0.7896463725847612
 
----------
Epoch 22/40
time = 1743.76 secondes

Train loss 0.015146286036349363 micro_f1_score 0.9845825064875592 
 
time = 36.26 secondes

Val loss 0.3210807666182518 micro_f1_score 0.7796123474515434
 
----------
Epoch 23/40
time = 1744.01 secondes

Train loss 0.013855134888115834 micro_f1_score 0.9862763037511436 
 
time = 36.17 secondes

Val loss 0.31127346929956656 micro_f1_score 0.7808908045977012
 
----------
Epoch 24/40
time = 1746.99 secondes

Train loss 0.01247691551961743 micro_f1_score 0.9872567722243418 
 
time = 35.93 secondes

Val loss 0.3287771448004441 micro_f1_score 0.776824034334764
 
----------
Epoch 25/40
time = 1743.91 secondes

Train loss 0.012266919384490246 micro_f1_score 0.9882281229761134 
 
time = 36.59 secondes

Val loss 0.3427009913520735 micro_f1_score 0.7763532763532763
 
----------
Epoch 26/40
time = 1744.09 secondes

Train loss 0.009815715296248494 micro_f1_score 0.98968051483188 
 
time = 37.53 secondes

Val loss 0.32756074701176313 micro_f1_score 0.7932674716428832
 
----------
Epoch 27/40
time = 1747.75 secondes

Train loss 0.008853739681512093 micro_f1_score 0.9906271431837232 
 
time = 37.16 secondes

Val loss 0.3423127117948454 micro_f1_score 0.7813411078717202
 
----------
Epoch 28/40
time = 1747.48 secondes

Train loss 0.007727168573261667 micro_f1_score 0.9919738293582867 
 
time = 36.37 secondes

Val loss 0.346703403797306 micro_f1_score 0.7811816192560175
 
----------
Epoch 29/40
time = 1747.75 secondes

Train loss 0.00790545835015773 micro_f1_score 0.9928840519045626 
 
time = 35.90 secondes

Val loss 0.35489751007713255 micro_f1_score 0.7846754168144732
 
----------
Epoch 30/40
time = 1747.12 secondes

Train loss 0.008291731857372708 micro_f1_score 0.9922746127792365 
 
time = 36.34 secondes

Val loss 0.3374295113760917 micro_f1_score 0.787900612171408
 
----------
Epoch 31/40
time = 1747.91 secondes

Train loss 0.00658407180899385 micro_f1_score 0.9934585837073097 
 
time = 36.51 secondes

Val loss 0.3685213287101417 micro_f1_score 0.7813953488372093
 
----------
Epoch 32/40
time = 1747.10 secondes

Train loss 0.005507094489131542 micro_f1_score 0.9945629443747386 
 
time = 36.42 secondes

Val loss 0.36964626182786753 micro_f1_score 0.7828301200436523
 
----------
Epoch 33/40
time = 1751.55 secondes

Train loss 0.0052237992012754775 micro_f1_score 0.9950218506555196 
 
time = 44.29 secondes

Val loss 0.3861215161006959 micro_f1_score 0.7751882395123701
 
----------
Epoch 34/40
time = 1749.85 secondes

Train loss 0.004707257592825634 micro_f1_score 0.9954754572069504 
 
time = 39.11 secondes

Val loss 0.36979909351126095 micro_f1_score 0.7841700256504215
 
----------
Epoch 35/40
time = 1752.37 secondes

Train loss 0.003552614006755018 micro_f1_score 0.9966926439840336 
 
time = 37.11 secondes

Val loss 0.3738380323667995 micro_f1_score 0.7850000000000001
 
----------
Epoch 36/40
time = 1750.69 secondes

Train loss 0.004084782639557972 micro_f1_score 0.9959354226020893 
 
time = 37.09 secondes

Val loss 0.368552385172883 micro_f1_score 0.7848375451263537
 
----------
Epoch 37/40
time = 1749.20 secondes

Train loss 0.0028930688533905227 micro_f1_score 0.9975308641975309 
 
time = 38.11 secondes

Val loss 0.39029239912013536 micro_f1_score 0.779136690647482
 
----------
Epoch 38/40
time = 1755.36 secondes

Train loss 0.0025410363680134036 micro_f1_score 0.9977948444985172 
 
time = 36.99 secondes

Val loss 0.3801172177811138 micro_f1_score 0.7834234234234233
 
----------
Epoch 39/40
time = 1753.00 secondes

Train loss 0.0019364942596013434 micro_f1_score 0.9982514824388019 
 
time = 39.37 secondes

Val loss 0.3769035126830711 micro_f1_score 0.7862815884476535
 
----------
Epoch 40/40
time = 1750.21 secondes

Train loss 0.0016634190706394587 micro_f1_score 0.9985182933779111 
 
time = 37.31 secondes

Val loss 0.3832326128346021 micro_f1_score 0.7849192100538598
 
----------
best_f1_socre 0.7932674716428832 best_epoch 26

average train time 1746.8595989644527

average val time 36.8203330039978
 
time = 39.43 secondes

test_f1_score 0.7854337736522671

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_256_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 768.00 MiB (GPU 1; 79.20 GiB total capacity; 72.05 GiB already allocated; 257.31 MiB free; 76.37 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_512_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 2.82 GiB (GPU 1; 79.20 GiB total capacity; 70.56 GiB already allocated; 2.53 GiB free; 74.09 GiB reserved in total by PyTorch)

