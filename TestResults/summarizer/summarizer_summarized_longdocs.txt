[nltk_data] Downloading package punkt to /vol/fob-
[nltk_data]     vol3/nebenf20/wubingti/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/usr/lib64/python3.6/site-packages/h5py/__init__.py:39: UserWarning: h5py is running against HDF5 1.10.8 when it was built against 1.10.7, this may cause problems
  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)
using bert_summarizer
Some weights of the model checamodel from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_head_bert_summarizer_1
 
time = 35.41 secondes

test_accuracy 0.8572657108306885 macro_avg {'precision': 0.8613127067921289, 'recall': 0.8571923128556467, 'f1-score': 0.8566520917801534, 'support': 2326} weighted_avg {'precision': 0.8646712642442209, 'recall': 0.8572656921754084, 'f1-score': 0.8583683691343899, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_head_bert_summarizer_2
 
time = 35.31 secondes

test_accuracy 0.8594153523445129 macro_avg {'precision': 0.8641219387776813, 'recall': 0.8554200774712747, 'f1-score': 0.8558853187936245, 'support': 2326} weighted_avg {'precision': 0.8643686804613802, 'recall': 0.8594153052450559, 'f1-score': 0.8573959486853996, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_head_bert_summarizer_3
 
time = 37.35 secondes

test_accuracy 0.868873655796051 macro_avg {'precision': 0.8701647655915371, 'recall': 0.8661569786887489, 'f1-score': 0.8657139236847862, 'support': 2326} weighted_avg {'precision': 0.8734292278033292, 'recall': 0.8688736027515047, 'f1-score': 0.8685900057595122, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_head_bert_summarizer_4
 
time = 35.40 secondes

test_accuracy 0.8671539425849915 macro_avg {'precision': 0.8697485360117583, 'recall': 0.8627479604005929, 'f1-score': 0.8635057143562671, 'support': 2326} weighted_avg {'precision': 0.8719713183397947, 'recall': 0.8671539122957868, 'f1-score': 0.8665917222436874, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_head_bert_summarizer_5
 
time = 35.19 secondes

test_accuracy 0.8675838708877563 macro_avg {'precision': 0.8676035957008352, 'recall': 0.8674547292993913, 'f1-score': 0.8658529868594869, 'support': 2326} weighted_avg {'precision': 0.8710610861822693, 'recall': 0.8675838349097162, 'f1-score': 0.8672884614672454, 'support': 2326}

using text rank
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_head_text_rank_1
 
time = 38.43 secondes

test_accuracy 0.7955326437950134 macro_avg {'precision': 0.7971118537791305, 'recall': 0.7898336050679935, 'f1-score': 0.7898838197603837, 'support': 2328} weighted_avg {'precision': 0.80149474358034, 'recall': 0.7955326460481099, 'f1-score': 0.7946536795138477, 'support': 2328}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_head_text_rank_2
 
time = 40.82 secondes

test_accuracy 0.8097079396247864 macro_avg {'precision': 0.8111913401244024, 'recall': 0.8046516497686126, 'f1-score': 0.8045698296568375, 'support': 2328} weighted_avg {'precision': 0.8138262938417689, 'recall': 0.8097079037800687, 'f1-score': 0.8078279534564737, 'support': 2328}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_head_text_rank_3
 
time = 42.90 secondes

test_accuracy 0.8011168241500854 macro_avg {'precision': 0.8028424825205146, 'recall': 0.7967157245647676, 'f1-score': 0.7951377827593188, 'support': 2328} weighted_avg {'precision': 0.8060117381644588, 'recall': 0.8011168384879725, 'f1-score': 0.7983488096686286, 'support': 2328}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_head_text_rank_4
 
time = 36.82 secondes

test_accuracy 0.800687313079834 macro_avg {'precision': 0.8031754156672086, 'recall': 0.7981378372270218, 'f1-score': 0.7983454748435834, 'support': 2328} weighted_avg {'precision': 0.807979555273182, 'recall': 0.8006872852233677, 'f1-score': 0.8018373947832702, 'support': 2328}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_head_text_rank_5
 
time = 33.15 secondes

test_accuracy 0.8045532703399658 macro_avg {'precision': 0.8033784045318508, 'recall': 0.79819344721299, 'f1-score': 0.7978357961320429, 'support': 2328} weighted_avg {'precision': 0.8103838516739919, 'recall': 0.804553264604811, 'f1-score': 0.8041025766601755, 'support': 2328}

##########
Hyperpartisan_BERT_head_bert_summarizer_1
 
time = 0.75 secondes

test_accuracy 0.9473684430122375 macro_avg {'precision': 0.9473684210526316, 'recall': 0.9523809523809523, 'f1-score': 0.9472222222222222, 'support': 38} weighted_avg {'precision': 0.9529085872576177, 'recall': 0.9473684210526315, 'f1-score': 0.9475146198830411, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_BERT_head_bert_summarizer_2
 
time = 0.58 secondes

test_accuracy 0.8157894611358643 macro_avg {'precision': 0.8541666666666667, 'recall': 0.8333333333333333, 'f1-score': 0.8146341463414635, 'support': 38} weighted_avg {'precision': 0.8695175438596493, 'recall': 0.8157894736842105, 'f1-score': 0.8130937098844673, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_BERT_head_bert_summarizer_3
 
time = 0.58 secondes

test_accuracy 0.8421052694320679 macro_avg {'precision': 0.8695652173913043, 'recall': 0.8571428571428572, 'f1-score': 0.8416666666666666, 'support': 38} weighted_avg {'precision': 0.8832951945080091, 'recall': 0.8421052631578947, 'f1-score': 0.8407894736842105, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_BERT_head_bert_summarizer_4
 
time = 0.58 secondes

test_accuracy 0.9210526347160339 macro_avg {'precision': 0.9194444444444444, 'recall': 0.9229691876750701, 'f1-score': 0.9205574912891985, 'support': 38} weighted_avg {'precision': 0.9226608187134503, 'recall': 0.9210526315789473, 'f1-score': 0.9212176783421969, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_BERT_head_bert_summarizer_5
 
time = 0.57 secondes

test_accuracy 0.8947368264198303 macro_avg {'precision': 0.9047619047619048, 'recall': 0.9047619047619048, 'f1-score': 0.8947368421052632, 'support': 38} weighted_avg {'precision': 0.9147869674185463, 'recall': 0.8947368421052632, 'f1-score': 0.8947368421052632, 'support': 38}

using text rank
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_BERT_head_text_rank_1
 
time = 0.48 secondes

test_accuracy 0.7631579041481018 macro_avg {'precision': 0.7666666666666666, 'recall': 0.76890756302521, 'f1-score': 0.7629937629937629, 'support': 38} weighted_avg {'precision': 0.7736842105263158, 'recall': 0.7631578947368421, 'f1-score': 0.7636502899660794, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_BERT_head_text_rank_2
 
time = 0.54 secondes

test_accuracy 0.7631579041481018 macro_avg {'precision': 0.7784090909090908, 'recall': 0.7745098039215685, 'f1-score': 0.762993762993763, 'support': 38} weighted_avg {'precision': 0.7885765550239234, 'recall': 0.7631578947368421, 'f1-score': 0.7623372360214465, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_BERT_head_text_rank_3
 
time = 0.54 secondes

test_accuracy 0.7631579041481018 macro_avg {'precision': 0.7976190476190477, 'recall': 0.7801120448179272, 'f1-score': 0.7616724738675958, 'support': 38} weighted_avg {'precision': 0.8114035087719298, 'recall': 0.7631578947368421, 'f1-score': 0.7596919127086008, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_BERT_head_text_rank_4
 
time = 0.54 secondes

test_accuracy 0.6842105388641357 macro_avg {'precision': 0.6918767507002801, 'recall': 0.6918767507002801, 'f1-score': 0.6842105263157895, 'support': 38} weighted_avg {'precision': 0.6995429750847707, 'recall': 0.6842105263157895, 'f1-score': 0.6842105263157895, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_BERT_head_text_rank_5
 
time = 0.55 secondes

test_accuracy 0.7631579041481018 macro_avg {'precision': 0.7976190476190477, 'recall': 0.7801120448179272, 'f1-score': 0.7616724738675958, 'support': 38} weighted_avg {'precision': 0.8114035087719298, 'recall': 0.7631578947368421, 'f1-score': 0.7596919127086008, 'support': 38}

using bert_summarizer
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
ECtHR_BERT_head_bert_summarizer_1
 
time = 13.47 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
test_f1_score 0.76755917733799

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
ECtHR_BERT_head_bert_summarizer_2
 
time = 13.43 secondes

test_f1_score 0.7598039215686274

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
ECtHR_BERT_head_bert_summarizer_3
 
time = 13.05 secondes

test_f1_score 0.7553816046966733

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
ECtHR_BERT_head_bert_summarizer_4
 
time = 13.39 secondes

test_f1_score 0.7692307692307693

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
ECtHR_BERT_head_bert_summarizer_5
 
time = 13.32 secondes

test_f1_score 0.7728682170542636

using text rank
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
ECtHR_BERT_head_text_rank_1
 
time = 13.04 secondes

test_f1_score 0.7379553466509987

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
ECtHR_BERT_head_text_rank_2
 
time = 12.83 secondes

test_f1_score 0.7200604457876841

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
ECtHR_BERT_head_text_rank_3
 
time = 12.87 secondes

test_f1_score 0.7469325153374233

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
ECtHR_BERT_head_text_rank_4
 
time = 12.88 secondes

test_f1_score 0.7577735124760077

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
ECtHR_BERT_head_text_rank_5
 
time = 12.62 secondes

test_f1_score 0.7397154940407535
