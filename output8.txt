[nltk_data] Downloading package punkt to /vol/fob-
[nltk_data]     vol3/nebenf20/wubingti/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/usr/lib64/python3.6/site-packages/h5py/__init__.py:39: UserWarning: h5py is running against HDF5 1.10.8 when it was built against 1.10.7, this may cause problems
  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)
datasets imported
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
ECtHR_ToBERT_256_25_4
----------
Epoch 1/40
time = 932.52 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Train loss 0.20010861498010052 micro_f1_score 0.7304946455889852 
 
time = 54.12 secondes

Val loss 0.16457779265818048 micro_f1_score 0.7566302652106084
 
----------
Epoch 2/40
time = 912.72 secondes

Train loss 0.1364933298883943 micro_f1_score 0.8221331064932961 
 
time = 52.32 secondes

Val loss 0.15628052973112122 micro_f1_score 0.777306059436511
 
----------
Epoch 3/40
time = 921.18 secondes

Train loss 0.11847368633659842 micro_f1_score 0.8519455487290688 
 
time = 46.78 secondes

Val loss 0.16438355577773736 micro_f1_score 0.7804143126177026
 
----------
Epoch 4/40
time = 880.46 secondes

Train loss 0.10575542732200643 micro_f1_score 0.8717376578873969 
 
time = 52.12 secondes

Val loss 0.15934415753991876 micro_f1_score 0.7867479055597867
 
----------
Epoch 5/40
time = 741.46 secondes

Train loss 0.09436865880464514 micro_f1_score 0.8873741776967583 
 
time = 53.14 secondes

Val loss 0.1632078078071602 micro_f1_score 0.7914601601219978
 
----------
Epoch 6/40
time = 764.35 secondes

Train loss 0.08527079078343672 micro_f1_score 0.9017363851617995 
 
time = 54.25 secondes

Val loss 0.17744694894454519 micro_f1_score 0.7828677839851025
 
----------
Epoch 7/40
time = 825.04 secondes

Train loss 0.07575376666536941 micro_f1_score 0.912992125984252 
 
time = 52.34 secondes

Val loss 0.18107550713371057 micro_f1_score 0.7906976744186046
 
----------
Epoch 8/40
time = 736.76 secondes

Train loss 0.06831450832604959 micro_f1_score 0.9230347420782578 
 
time = 52.89 secondes

Val loss 0.1945515655469699 micro_f1_score 0.7836644591611479
 
----------
Epoch 9/40
time = 726.52 secondes

Train loss 0.06126097197233288 micro_f1_score 0.9321848081440878 
 
time = 52.76 secondes

Val loss 0.19523186303797316 micro_f1_score 0.7972433804860356
 
----------
Epoch 10/40
time = 727.45 secondes

Train loss 0.05475427679159586 micro_f1_score 0.9410618986404893 
 
time = 52.98 secondes

Val loss 0.20649079031875875 micro_f1_score 0.7965367965367965
 
----------
Epoch 11/40
