[nltk_data] Downloading package punkt to /vol/fob-
[nltk_data]     vol3/nebenf20/wubingti/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
datasets imported
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
ECtHR_Bigbird_1024_64_1
----------
Epoch 1/40
time = 673.52 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Train loss 0.2747667538086036 micro_f1_score 0.5970350404312669 
 
time = 20.39 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss 0.21524867855134558 micro_f1_score 0.6876456876456876
 
----------
Epoch 2/40
time = 678.13 secondes

Train loss 0.1748460274860934 micro_f1_score 0.7602787743824488 
 
time = 20.80 secondes

Val loss 0.18726136633118645 micro_f1_score 0.7402496099843995
 
----------
Epoch 3/40
time = 676.63 secondes

Train loss 0.14676110969000572 micro_f1_score 0.8108723958333335 
 
time = 20.79 secondes

Val loss 0.19894481548031823 micro_f1_score 0.7288519637462235
 
----------
Epoch 4/40
time = 679.96 secondes

Train loss 0.1286774612090609 micro_f1_score 0.8376997062256025 
 
time = 21.27 secondes

Val loss 0.1989308208471439 micro_f1_score 0.7403636363636364
 
----------
Epoch 5/40
time = 683.96 secondes

Train loss 0.11146830747927632 micro_f1_score 0.8615802291691621 
 
time = 20.71 secondes

Val loss 0.19647692571409414 micro_f1_score 0.7532086541987533
 
----------
Epoch 6/40
time = 677.88 secondes

Train loss 0.09675895595872724 micro_f1_score 0.8857788526345003 
 
time = 20.23 secondes

Val loss 0.20962173308505386 micro_f1_score 0.7545520757465405
 
----------
Epoch 7/40
time = 676.29 secondes

Train loss 0.08440328408032656 micro_f1_score 0.9041784480050268 
 
time = 20.14 secondes

Val loss 0.2170173567338068 micro_f1_score 0.7605633802816901
 
----------
Epoch 8/40
time = 672.87 secondes

Train loss 0.07175547977708079 micro_f1_score 0.9216330361332709 
 
time = 20.28 secondes

Val loss 0.24299950861051434 micro_f1_score 0.753851666069509
 
----------
Epoch 9/40
time = 678.43 secondes

Train loss 0.060780696792377006 micro_f1_score 0.933872409905532 
 
time = 20.61 secondes

Val loss 0.25103322624183094 micro_f1_score 0.7615942028985507
 
----------
Epoch 10/40
time = 672.14 secondes

Train loss 0.05045988216140383 micro_f1_score 0.9472542792967238 
 
time = 20.11 secondes

Val loss 0.2677754491689752 micro_f1_score 0.7614579574160951
 
----------
Epoch 11/40
time = 684.37 secondes

Train loss 0.042259596122705655 micro_f1_score 0.9546893091470475 
 
time = 20.04 secondes

Val loss 0.29988659808381657 micro_f1_score 0.7542857142857142
 
----------
Epoch 12/40
time = 693.41 secondes

Train loss 0.03618078001539919 micro_f1_score 0.9627835447904651 
 
time = 19.96 secondes

Val loss 0.30817885745744233 micro_f1_score 0.7575971731448764
 
----------
Epoch 13/40
time = 693.70 secondes

Train loss 0.030553089549370648 micro_f1_score 0.9667857005721306 
 
time = 19.96 secondes

Val loss 0.3095569801135141 micro_f1_score 0.7678311499272198
 
----------
Epoch 14/40
