[nltk_data] Downloading package punkt to /vol/fob-
[nltk_data]     vol3/nebenf20/wubingti/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
datasets imported
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
ECtHR_Bigbird_1024_64_1
----------
Epoch 1/40
time = 673.52 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Train loss 0.2747667538086036 micro_f1_score 0.5970350404312669 
 
time = 20.39 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss 0.21524867855134558 micro_f1_score 0.6876456876456876
 
----------
Epoch 2/40
time = 678.13 secondes

Train loss 0.1748460274860934 micro_f1_score 0.7602787743824488 
 
time = 20.80 secondes

Val loss 0.18726136633118645 micro_f1_score 0.7402496099843995
 
----------
Epoch 3/40
time = 676.63 secondes

Train loss 0.14676110969000572 micro_f1_score 0.8108723958333335 
 
time = 20.79 secondes

Val loss 0.19894481548031823 micro_f1_score 0.7288519637462235
 
----------
Epoch 4/40
time = 679.96 secondes

Train loss 0.1286774612090609 micro_f1_score 0.8376997062256025 
 
time = 21.27 secondes

Val loss 0.1989308208471439 micro_f1_score 0.7403636363636364
 
----------
Epoch 5/40
time = 683.96 secondes

Train loss 0.11146830747927632 micro_f1_score 0.8615802291691621 
 
time = 20.71 secondes

Val loss 0.19647692571409414 micro_f1_score 0.7532086541987533
 
----------
Epoch 6/40
time = 677.88 secondes

Train loss 0.09675895595872724 micro_f1_score 0.8857788526345003 
 
time = 20.23 secondes

Val loss 0.20962173308505386 micro_f1_score 0.7545520757465405
 
----------
Epoch 7/40
time = 676.29 secondes

Train loss 0.08440328408032656 micro_f1_score 0.9041784480050268 
 
time = 20.14 secondes

Val loss 0.2170173567338068 micro_f1_score 0.7605633802816901
 
----------
Epoch 8/40
time = 672.87 secondes

Train loss 0.07175547977708079 micro_f1_score 0.9216330361332709 
 
time = 20.28 secondes

Val loss 0.24299950861051434 micro_f1_score 0.753851666069509
 
----------
Epoch 9/40
time = 678.43 secondes

Train loss 0.060780696792377006 micro_f1_score 0.933872409905532 
 
time = 20.61 secondes

Val loss 0.25103322624183094 micro_f1_score 0.7615942028985507
 
----------
Epoch 10/40
time = 672.14 secondes

Train loss 0.05045988216140383 micro_f1_score 0.9472542792967238 
 
time = 20.11 secondes

Val loss 0.2677754491689752 micro_f1_score 0.7614579574160951
 
----------
Epoch 11/40
time = 684.37 secondes

Train loss 0.042259596122705655 micro_f1_score 0.9546893091470475 
 
time = 20.04 secondes

Val loss 0.29988659808381657 micro_f1_score 0.7542857142857142
 
----------
Epoch 12/40
time = 693.41 secondes

Train loss 0.03618078001539919 micro_f1_score 0.9627835447904651 
 
time = 19.96 secondes

Val loss 0.30817885745744233 micro_f1_score 0.7575971731448764
 
----------
Epoch 13/40
time = 693.70 secondes

Train loss 0.030553089549370648 micro_f1_score 0.9667857005721306 
 
time = 19.96 secondes

Val loss 0.3095569801135141 micro_f1_score 0.7678311499272198
 
----------
Epoch 14/40
time = 692.67 secondes

Train loss 0.02648843726424316 micro_f1_score 0.9728964091570325 
 
time = 20.43 secondes

Val loss 0.35009293270404224 micro_f1_score 0.7503506311360449
 
----------
Epoch 15/40
time = 690.30 secondes

Train loss 0.023830213264283213 micro_f1_score 0.9742494182275969 
 
time = 20.09 secondes

Val loss 0.36055789374914327 micro_f1_score 0.7594564818215204
 
----------
Epoch 16/40
time = 691.32 secondes

Train loss 0.021329797982229844 micro_f1_score 0.9784662459361254 
 
time = 20.08 secondes

Val loss 0.40656282850464837 micro_f1_score 0.7476764199655765
 
----------
Epoch 17/40
time = 692.76 secondes

Train loss 0.01719713285817085 micro_f1_score 0.9816461250810853 
 
time = 20.06 secondes

Val loss 0.42822732251198564 micro_f1_score 0.7451248717071502
 
----------
Epoch 18/40
time = 688.47 secondes

Train loss 0.016048157881740334 micro_f1_score 0.9833988474602144 
 
time = 20.24 secondes

Val loss 0.4241260428164826 micro_f1_score 0.7542403464453266
 
----------
Epoch 19/40
time = 695.00 secondes

Train loss 0.013004147251574994 micro_f1_score 0.9866412940057089 
 
time = 20.51 secondes

Val loss 0.44363188181744245 micro_f1_score 0.752411575562701
 
----------
Epoch 20/40
time = 694.54 secondes

Train loss 0.012417138267404438 micro_f1_score 0.9875509194045762 
 
time = 20.12 secondes

Val loss 0.438053159806572 micro_f1_score 0.7532005689900426
 
----------
Epoch 21/40
time = 688.34 secondes

Train loss 0.013395651624297308 micro_f1_score 0.9861417802482296 
 
time = 19.98 secondes

Val loss 0.4393099359557277 micro_f1_score 0.7524680073126143
 
----------
Epoch 22/40
time = 690.69 secondes

Train loss 0.01060419460129426 micro_f1_score 0.98950091296409 
 
time = 20.51 secondes

Val loss 0.47247261663929363 micro_f1_score 0.7467811158798283
 
----------
Epoch 23/40
time = 689.52 secondes

Train loss 0.009934103993768699 micro_f1_score 0.9903732734675241 
 
time = 20.39 secondes

Val loss 0.47411898535783176 micro_f1_score 0.7483777937995675
 
----------
Epoch 24/40
time = 712.18 secondes

Train loss 0.009564219850086899 micro_f1_score 0.99078586658544 
 
time = 29.93 secondes

Val loss 0.4640339759529614 micro_f1_score 0.7582340574632096
 
----------
Epoch 25/40
time = 885.19 secondes

Train loss 0.008335565424023845 micro_f1_score 0.9914757591901971 
 
time = 20.17 secondes

Val loss 0.5200209441732188 micro_f1_score 0.7526652452025586
 
----------
Epoch 26/40
time = 686.69 secondes

Train loss 0.008264003020451275 micro_f1_score 0.9918241624519908 
 
time = 20.12 secondes

Val loss 0.4943874478340149 micro_f1_score 0.7487401007919365
 
----------
Epoch 27/40
time = 686.58 secondes

Train loss 0.00774543031252465 micro_f1_score 0.9923158855751675 
 
time = 20.13 secondes

Val loss 0.49613026248627023 micro_f1_score 0.7545388525780683
 
----------
Epoch 28/40
time = 782.47 secondes

Train loss 0.005496845553532874 micro_f1_score 0.9947114104173801 
 
time = 27.12 secondes

Val loss 0.5115943216398114 micro_f1_score 0.758221900975786
 
----------
Epoch 29/40
time = 735.71 secondes

Train loss 0.006136895617224205 micro_f1_score 0.9936538096142885 
 
time = 20.06 secondes

Val loss 0.5173875953330368 micro_f1_score 0.7573878146661802
 
----------
Epoch 30/40
time = 693.84 secondes

Train loss 0.004701270431476522 micro_f1_score 0.9952866048350312 
 
time = 20.07 secondes

Val loss 0.5441558177842468 micro_f1_score 0.7433116413593637
 
----------
Epoch 31/40
time = 686.32 secondes

Train loss 0.004527878009834206 micro_f1_score 0.9957783440459438 
 
time = 20.03 secondes

Val loss 0.5336140827810179 micro_f1_score 0.7494631352899068
 
----------
Epoch 32/40
time = 691.35 secondes

Train loss 0.00454626490245901 micro_f1_score 0.9964280285757714 
 
time = 20.44 secondes

Val loss 0.5360769885973852 micro_f1_score 0.760014179369018
 
----------
Epoch 33/40
time = 687.52 secondes

Train loss 0.003663634177789661 micro_f1_score 0.9965795074490728 
 
time = 20.03 secondes

Val loss 0.5472595310602032 micro_f1_score 0.755967224795155
 
----------
Epoch 34/40
time = 1190.98 secondes

Train loss 0.0025941232863309826 micro_f1_score 0.9973408296611458 
 
time = 44.47 secondes

Val loss 0.528718503527954 micro_f1_score 0.7637422642883146
 
----------
Epoch 35/40
time = 1446.97 secondes

Train loss 0.0026801873509693986 micro_f1_score 0.9970732448971834 
 
time = 44.36 secondes

Val loss 0.5348291754966876 micro_f1_score 0.7665830046611688
 
----------
Epoch 36/40
time = 1187.97 secondes

Train loss 0.0020920827248201105 micro_f1_score 0.9977596354661097 
 
time = 35.34 secondes

Val loss 0.5502930191940949 micro_f1_score 0.7561761546723954
 
----------
Epoch 37/40
time = 1148.65 secondes

Train loss 0.0018588313004993078 micro_f1_score 0.9979862456780272 
 
time = 35.50 secondes

Val loss 0.559512158886331 micro_f1_score 0.7566594672426208
 
----------
Epoch 38/40
time = 1150.19 secondes

Train loss 0.0013055992944246428 micro_f1_score 0.9988225016143123 
 
time = 35.29 secondes

Val loss 0.5592148352841861 micro_f1_score 0.7651487988526354
 
----------
Epoch 39/40
time = 1146.36 secondes

Train loss 0.0009200753935835343 micro_f1_score 0.9990882844552501 
 
time = 35.50 secondes

Val loss 0.5503614787928394 micro_f1_score 0.7716027249910363
 
----------
Epoch 40/40
time = 1146.78 secondes

Train loss 0.0007333717330053529 micro_f1_score 0.9995061728395062 
 
time = 35.39 secondes

Val loss 0.5782008317650341 micro_f1_score 0.7634024303073623
 
----------
best_f1_socre 0.7716027249910363 best_epoch 39

average train time 785.515813779831

average val time 23.791685903072356
 
time = 36.67 secondes

test_f1_score 0.7527777777777777

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_128_1
----------
Epoch 1/40
Attention type 'block_sparse' is not possible if sequence_length: 1024 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 1408 with config.block_size = 128, config.num_random_blocks = 3. Changing attention type to 'original_full'...
time = 710.55 secondes

Train loss 0.29950906666549476 micro_f1_score 0.5349794238683129 
 
time = 29.57 secondes

Val loss 0.2815650049291673 micro_f1_score 0.47777777777777775
 
----------
Epoch 2/40
time = 713.16 secondes

Train loss 0.2178080552668722 micro_f1_score 0.6625247851949769 
 
time = 29.38 secondes

Val loss 0.22500192532773877 micro_f1_score 0.6624254473161033
 
----------
Epoch 3/40
time = 712.24 secondes

Train loss 0.18171808938453862 micro_f1_score 0.7406185567010308 
 
time = 29.38 secondes

Val loss 0.20416525262789648 micro_f1_score 0.703588143525741
 
----------
Epoch 4/40
time = 712.13 secondes

Train loss 0.16449353134444167 micro_f1_score 0.7754853383153278 
 
time = 29.34 secondes

Val loss 0.19441374574528367 micro_f1_score 0.7233396584440228
 
----------
Epoch 5/40
time = 711.66 secondes

Train loss 0.15525233913716432 micro_f1_score 0.7946417796371535 
 
time = 29.47 secondes

Val loss 0.2016149286608227 micro_f1_score 0.7117887690591298
 
----------
Epoch 6/40
time = 715.15 secondes

Train loss 0.14437316278046047 micro_f1_score 0.8163151166506871 
 
time = 29.37 secondes

Val loss 0.19949738192753713 micro_f1_score 0.7208008898776419
 
----------
Epoch 7/40
time = 712.78 secondes

Train loss 0.13547621957185838 micro_f1_score 0.8311944718657452 
 
time = 29.32 secondes

Val loss 0.1891408358685306 micro_f1_score 0.7443581206067332
 
----------
Epoch 8/40
time = 712.27 secondes

Train loss 0.12492853962724004 micro_f1_score 0.8455587733566072 
 
time = 29.40 secondes

Val loss 0.2106051059042821 micro_f1_score 0.725111441307578
 
----------
Epoch 9/40
time = 712.14 secondes

Train loss 0.11326899221754289 micro_f1_score 0.8608054273655977 
 
time = 29.39 secondes

Val loss 0.20400182597461294 micro_f1_score 0.747643219724438
 
----------
Epoch 10/40
time = 712.98 secondes

Train loss 0.10363794223044638 micro_f1_score 0.8759757155247182 
 
time = 29.40 secondes

Val loss 0.20615474672102538 micro_f1_score 0.7523739956172388
 
----------
Epoch 11/40
time = 711.74 secondes

Train loss 0.09404462047575696 micro_f1_score 0.8879833875328135 
 
time = 29.33 secondes

Val loss 0.22357153758162357 micro_f1_score 0.7515550676911817
 
----------
Epoch 12/40
time = 711.00 secondes

Train loss 0.08371707144369547 micro_f1_score 0.9034453091392594 
 
time = 29.37 secondes

Val loss 0.2281590187158741 micro_f1_score 0.7545945945945947
 
----------
Epoch 13/40
time = 710.93 secondes

Train loss 0.07683055312330793 micro_f1_score 0.9121729326231484 
 
time = 29.28 secondes

Val loss 0.23144635277204825 micro_f1_score 0.7620087336244542
 
----------
Epoch 14/40
time = 714.96 secondes

Train loss 0.07094088247683537 micro_f1_score 0.9222721636701797 
 
time = 29.46 secondes

Val loss 0.2541588548509801 micro_f1_score 0.7443881245474294
 
----------
Epoch 15/40
time = 710.11 secondes

Train loss 0.06362429082913844 micro_f1_score 0.9310424710424711 
 
time = 29.32 secondes

Val loss 0.268356059051928 micro_f1_score 0.7414529914529915
 
----------
Epoch 16/40
time = 712.22 secondes

Train loss 0.056000252343663896 micro_f1_score 0.9405288887178106 
 
time = 29.36 secondes

Val loss 0.2880467437818402 micro_f1_score 0.7395135706732463
 
----------
Epoch 17/40
time = 723.05 secondes

Train loss 0.049847147897355726 micro_f1_score 0.946927481649437 
 
time = 30.94 secondes

Val loss 0.3171956629538145 micro_f1_score 0.7416201117318436
 
----------
Epoch 18/40
time = 727.50 secondes

Train loss 0.042691128473384896 micro_f1_score 0.9537257309492747 
 
time = 31.50 secondes

Val loss 0.34696625441801354 micro_f1_score 0.7277227722772278
 
----------
Epoch 19/40
time = 729.96 secondes

Train loss 0.0394218450663863 micro_f1_score 0.9580025313542745 
 
time = 30.72 secondes

Val loss 0.34942505301022136 micro_f1_score 0.7370990237099023
 
----------
Epoch 20/40
time = 726.79 secondes

Train loss 0.035525006744675 micro_f1_score 0.9629572937394765 
 
time = 32.56 secondes

Val loss 0.3470744993843016 micro_f1_score 0.736436821841092
 
----------
Epoch 21/40
time = 723.27 secondes

Train loss 0.029382888197076615 micro_f1_score 0.969124952235384 
 
time = 31.67 secondes

Val loss 0.37281018996336424 micro_f1_score 0.7375937165298108
 
----------
Epoch 22/40
time = 721.86 secondes

Train loss 0.02981383374027975 micro_f1_score 0.9700003821607368 
 
time = 31.18 secondes

Val loss 0.3554470784595755 micro_f1_score 0.7453637660485021
 
----------
Epoch 23/40
time = 726.76 secondes

Train loss 0.02665256927427542 micro_f1_score 0.9721385829172543 
 
time = 30.91 secondes

Val loss 0.3748141769014421 micro_f1_score 0.7471014492753623
 
----------
Epoch 24/40
time = 720.10 secondes

Train loss 0.022375572353878336 micro_f1_score 0.9773524477657465 
 
time = 31.12 secondes

Val loss 0.37750535323971607 micro_f1_score 0.7507183908045976
 
----------
Epoch 25/40
time = 717.76 secondes

Train loss 0.020943750340810363 micro_f1_score 0.9789116424512833 
 
time = 30.57 secondes

Val loss 0.3764031581947061 micro_f1_score 0.7580760947595119
 
----------
Epoch 26/40
time = 715.47 secondes

Train loss 0.01809346964572313 micro_f1_score 0.9806476190476191 
 
time = 30.19 secondes

Val loss 0.39285375373285325 micro_f1_score 0.7514577259475219
 
----------
Epoch 27/40
time = 716.73 secondes

Train loss 0.016199606098287092 micro_f1_score 0.9831270234241095 
 
time = 31.20 secondes

Val loss 0.4138701186805475 micro_f1_score 0.7447973713033954
 
----------
Epoch 28/40
time = 723.03 secondes

Train loss 0.01467215368319156 micro_f1_score 0.9847185701764414 
 
time = 30.96 secondes

Val loss 0.429845966765138 micro_f1_score 0.741506646971935
 
----------
Epoch 29/40
time = 728.78 secondes

Train loss 0.014713219449113895 micro_f1_score 0.9842189525043836 
 
time = 31.32 secondes

Val loss 0.41786208077043785 micro_f1_score 0.7499999999999999
 
----------
Epoch 30/40
time = 726.90 secondes

Train loss 0.011810814660540174 micro_f1_score 0.9883508451347648 
 
time = 31.62 secondes

Val loss 0.4428071528673172 micro_f1_score 0.7449392712550608
 
----------
Epoch 31/40
time = 728.94 secondes

Train loss 0.011525018282495816 micro_f1_score 0.9883212234184197 
 
time = 32.05 secondes

Val loss 0.4688356169423119 micro_f1_score 0.7466095645967165
 
----------
Epoch 32/40
time = 727.98 secondes

Train loss 0.010062738772227874 micro_f1_score 0.9893390191897654 
 
time = 30.83 secondes

Val loss 0.4884557059553803 micro_f1_score 0.7444523979957052
 
----------
Epoch 33/40
time = 723.28 secondes

Train loss 0.009210721159609232 micro_f1_score 0.9903260207190738 
 
time = 31.24 secondes

Val loss 0.4848633064109771 micro_f1_score 0.7503607503607503
 
----------
Epoch 34/40
time = 723.03 secondes

Train loss 0.00743132512612024 micro_f1_score 0.9924640328842201 
 
time = 31.51 secondes

Val loss 0.4864593172659639 micro_f1_score 0.7562610229276896
 
----------
Epoch 35/40
time = 724.86 secondes

Train loss 0.006265601047322831 micro_f1_score 0.9938346780331863 
 
time = 31.16 secondes

Val loss 0.5167621938420124 micro_f1_score 0.7441696113074204
 
----------
Epoch 36/40
time = 717.78 secondes

Train loss 0.005625582499320641 micro_f1_score 0.9942957103742014 
 
time = 30.41 secondes

Val loss 0.5132132700232209 micro_f1_score 0.7490292975644194
 
----------
Epoch 37/40
time = 722.18 secondes

Train loss 0.005168702146273734 micro_f1_score 0.9950135129991244 
 
time = 30.49 secondes

Val loss 0.5224344891602876 micro_f1_score 0.7516059957173448
 
----------
Epoch 38/40
time = 718.37 secondes

Train loss 0.004401580404606517 micro_f1_score 0.9957793071979925 
 
time = 30.95 secondes

Val loss 0.530449596340539 micro_f1_score 0.7531172069825437
 
----------
Epoch 39/40
time = 721.27 secondes

Train loss 0.004088877162205602 micro_f1_score 0.9963885192929101 
 
time = 30.56 secondes

Val loss 0.511138991987119 micro_f1_score 0.7541800071149057
 
----------
Epoch 40/40
time = 718.08 secondes

Train loss 0.003861534855709865 micro_f1_score 0.9963473099459705 
 
time = 30.49 secondes

Val loss 0.5203768071092543 micro_f1_score 0.7545945945945947
 
----------
best_f1_socre 0.7620087336244542 best_epoch 13

average train time 718.744651567936

average val time 30.406721311807633
 
time = 33.50 secondes

test_f1_score 0.7540521494009866

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_64_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 252.00 MiB (GPU 1; 79.20 GiB total capacity; 69.50 GiB already allocated; 77.31 MiB free; 70.84 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_128_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 1; 79.20 GiB total capacity; 69.71 GiB already allocated; 157.31 MiB free; 70.76 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_64_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 540.00 MiB (GPU 1; 79.20 GiB total capacity; 67.60 GiB already allocated; 491.31 MiB free; 70.44 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_128_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 1008.00 MiB (GPU 1; 79.20 GiB total capacity; 70.35 GiB already allocated; 117.31 MiB free; 70.80 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_256_1
----------
Epoch 1/40
time = 1305.93 secondes

Train loss 0.23526732816889478 micro_f1_score 0.667070952092177 
 
time = 71.47 secondes

Val loss 0.20606710187724378 micro_f1_score 0.6951456310679612
 
----------
Epoch 2/40
time = 1300.19 secondes

Train loss 0.16322092604194138 micro_f1_score 0.7800792580790128 
 
time = 71.12 secondes

Val loss 0.19584343655676137 micro_f1_score 0.7274872198191112
 
----------
Epoch 3/40
time = 1308.19 secondes

Train loss 0.14139240569695158 micro_f1_score 0.8159489375454473 
 
time = 69.98 secondes

Val loss 0.1839171119156431 micro_f1_score 0.7411089866156787
 
----------
Epoch 4/40
time = 1314.97 secondes

Train loss 0.12745516265633408 micro_f1_score 0.8398939588688946 
 
time = 67.68 secondes

Val loss 0.19194785163539355 micro_f1_score 0.7448015122873346
 
----------
Epoch 5/40
time = 1277.37 secondes

Train loss 0.1133392192977103 micro_f1_score 0.8629530683041589 
 
time = 67.63 secondes

Val loss 0.19270809104696648 micro_f1_score 0.7541478129713423
 
----------
Epoch 6/40
time = 1277.64 secondes

Train loss 0.10077206607454935 micro_f1_score 0.8812941083181349 
 
time = 67.62 secondes

Val loss 0.20964271978276675 micro_f1_score 0.7433234421364985
 
----------
Epoch 7/40
time = 1277.05 secondes

Train loss 0.08934343165239772 micro_f1_score 0.8972102750266344 
 
time = 67.66 secondes

Val loss 0.2252380006381723 micro_f1_score 0.7422979340340704
 
----------
Epoch 8/40
time = 1276.39 secondes

Train loss 0.0800280505547988 micro_f1_score 0.9090339261929012 
 
time = 67.58 secondes

Val loss 0.22816724855391707 micro_f1_score 0.7465900933237617
 
----------
Epoch 9/40
time = 1276.01 secondes

Train loss 0.07083073183558546 micro_f1_score 0.9233175614906347 
 
time = 67.64 secondes

Val loss 0.24223888286801634 micro_f1_score 0.7453551912568307
 
----------
Epoch 10/40
time = 1276.28 secondes

Train loss 0.06131914730208951 micro_f1_score 0.9348605267258844 
 
time = 67.70 secondes

Val loss 0.24595553391292446 micro_f1_score 0.749819233550253
 
----------
Epoch 11/40
time = 1275.21 secondes

Train loss 0.05496202261346552 micro_f1_score 0.9441254074189043 
 
time = 67.62 secondes

Val loss 0.2592569916951852 micro_f1_score 0.7587904360056259
 
----------
Epoch 12/40
time = 1277.17 secondes

Train loss 0.04798063223359284 micro_f1_score 0.9508070121244884 
 
time = 67.64 secondes

Val loss 0.2792269598509445 micro_f1_score 0.7428571428571428
 
----------
Epoch 13/40
time = 1277.02 secondes

Train loss 0.04162355997675174 micro_f1_score 0.9574714418030257 
 
time = 67.64 secondes

Val loss 0.28589545324689053 micro_f1_score 0.7538080056677294
 
----------
Epoch 14/40
time = 1277.36 secondes

Train loss 0.037452672597289356 micro_f1_score 0.9627204806285142 
 
time = 67.74 secondes

Val loss 0.29869845118678984 micro_f1_score 0.7520225114315864
 
----------
Epoch 15/40
time = 1278.26 secondes

Train loss 0.03344498646722452 micro_f1_score 0.9666282199154171 
 
time = 67.61 secondes

Val loss 0.30365743793425015 micro_f1_score 0.7515194851626742
 
----------
Epoch 16/40
time = 1278.44 secondes

Train loss 0.02900043921754905 micro_f1_score 0.9722413925312475 
 
time = 67.65 secondes

Val loss 0.30239056429413497 micro_f1_score 0.749819233550253
 
----------
Epoch 17/40
time = 1361.41 secondes

Train loss 0.026565353735350072 micro_f1_score 0.9736499425507469 
 
time = 77.64 secondes

Val loss 0.2982950575771879 micro_f1_score 0.7522189349112426
 
----------
Epoch 18/40
time = 1319.10 secondes

Train loss 0.023290452159844707 micro_f1_score 0.9781362281171165 
 
time = 67.66 secondes

Val loss 0.31139955303219496 micro_f1_score 0.747896084888401
 
----------
Epoch 19/40
time = 1339.52 secondes

Train loss 0.021770752125591677 micro_f1_score 0.9789525955918866 
 
time = 67.57 secondes

Val loss 0.3388934291777064 micro_f1_score 0.7425063199711087
 
----------
Epoch 20/40
time = 2373.14 secondes

Train loss 0.018168047709214326 micro_f1_score 0.9818237360623187 
 
time = 137.33 secondes

Val loss 0.3452908084040783 micro_f1_score 0.7531622696060716
 
----------
Epoch 21/40
time = 2138.02 secondes

Train loss 0.016691907804127916 micro_f1_score 0.9828929280586527 
 
time = 111.62 secondes

Val loss 0.35281780777407473 micro_f1_score 0.7499999999999999
 
----------
Epoch 22/40
time = 2045.24 secondes

Train loss 0.01550072341704288 micro_f1_score 0.9849004804392587 
 
time = 111.59 secondes

Val loss 0.36027909546601966 micro_f1_score 0.7531622696060716
 
----------
Epoch 23/40
time = 2044.20 secondes

Train loss 0.014081834941401843 micro_f1_score 0.9855017169019458 
 
time = 111.70 secondes

Val loss 0.35873269497371113 micro_f1_score 0.7527352297592997
 
----------
Epoch 24/40
time = 2043.98 secondes

Train loss 0.011986393625438968 micro_f1_score 0.9876505564872694 
 
time = 111.61 secondes

Val loss 0.3706738150022069 micro_f1_score 0.7481962481962482
 
----------
Epoch 25/40
time = 2042.97 secondes

Train loss 0.011003124770794383 micro_f1_score 0.9891006097560976 
 
time = 111.68 secondes

Val loss 0.38559199649779524 micro_f1_score 0.7423741271591328
 
----------
Epoch 26/40
time = 2057.90 secondes

Train loss 0.009496604933414969 micro_f1_score 0.990408769125371 
 
time = 111.58 secondes

Val loss 0.3877367589805947 micro_f1_score 0.7487141807494488
 
----------
Epoch 27/40
time = 2057.08 secondes

Train loss 0.009410503555193066 micro_f1_score 0.9906058646788118 
 
time = 114.95 secondes

Val loss 0.39067056994946275 micro_f1_score 0.7514367816091954
 
----------
Epoch 28/40
time = 2070.09 secondes

Train loss 0.008725046035193897 micro_f1_score 0.9907495527047091 
 
time = 113.44 secondes

Val loss 0.4168577072073202 micro_f1_score 0.7423643550125764
 
----------
Epoch 29/40
time = 2068.14 secondes

Train loss 0.008273410020374614 micro_f1_score 0.991480946223473 
 
time = 113.94 secondes

Val loss 0.40472800931969627 micro_f1_score 0.7488151658767772
 
----------
Epoch 30/40
time = 2068.44 secondes

Train loss 0.006601155448237199 micro_f1_score 0.9931900323378353 
 
time = 115.19 secondes

Val loss 0.443072646856308 micro_f1_score 0.7361963190184048
 
----------
Epoch 31/40
time = 2044.65 secondes

Train loss 0.005881692876639201 micro_f1_score 0.994298312300441 
 
time = 111.62 secondes

Val loss 0.4234748643929841 micro_f1_score 0.7532188841201717
 
----------
Epoch 32/40
time = 2041.93 secondes

Train loss 0.005325970588186924 micro_f1_score 0.9948667249705312 
 
time = 111.57 secondes

Val loss 0.42696361720073417 micro_f1_score 0.746031746031746
 
----------
Epoch 33/40
time = 2043.98 secondes

Train loss 0.004885901902751947 micro_f1_score 0.9950192007908445 
 
time = 111.57 secondes

Val loss 0.4203515849152549 micro_f1_score 0.7568740955137483
 
----------
Epoch 34/40
time = 2042.65 secondes

Train loss 0.004514102950837172 micro_f1_score 0.9956673761021587 
 
time = 111.67 secondes

Val loss 0.430829185931409 micro_f1_score 0.7521054558769681
 
----------
Epoch 35/40
time = 2043.57 secondes

Train loss 0.003514032681060426 micro_f1_score 0.9971496978679739 
 
time = 111.66 secondes

Val loss 0.42125124130092684 micro_f1_score 0.7584453323646931
 
----------
Epoch 36/40
time = 2043.45 secondes

Train loss 0.0037567147937249855 micro_f1_score 0.9961240310077519 
 
time = 111.61 secondes

Val loss 0.43685019211690934 micro_f1_score 0.7595392368610512
 
----------
Epoch 37/40
time = 2044.70 secondes

Train loss 0.0024060966644757573 micro_f1_score 0.9976425855513308 
 
time = 112.00 secondes

Val loss 0.42474633687343755 micro_f1_score 0.760144927536232
 
----------
Epoch 38/40
time = 2047.48 secondes

Train loss 0.002428176524594787 micro_f1_score 0.9977210574293529 
 
time = 112.04 secondes

Val loss 0.42897241953455034 micro_f1_score 0.7644703312704769
 
----------
Epoch 39/40
time = 2047.51 secondes

Train loss 0.0020078397929594485 micro_f1_score 0.9984048613748576 
 
time = 111.90 secondes

Val loss 0.4319130474182426 micro_f1_score 0.7655571635311145
 
----------
Epoch 40/40
time = 2045.84 secondes

Train loss 0.0011147823182368853 micro_f1_score 0.9991642607506458 
 
time = 111.63 secondes

Val loss 0.447067526764557 micro_f1_score 0.7547169811320754
 
----------
best_f1_socre 0.7655571635311145 best_epoch 39

average train time 1700.7120189964771

average val time 92.16919352412224
 
time = 114.75 secondes

test_f1_score 0.7466478475652788

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_512_1
----------
Epoch 1/40
time = 2596.52 secondes

Train loss 0.2363037453175665 micro_f1_score 0.6609075043630017 
 
time = 125.51 secondes

Val loss 0.20118295218123763 micro_f1_score 0.7119456652017578
 
----------
Epoch 2/40
time = 2626.91 secondes

Train loss 0.16322361046599376 micro_f1_score 0.7819333742080523 
 
time = 130.22 secondes

Val loss 0.18724405765533447 micro_f1_score 0.7318982387475538
 
----------
Epoch 3/40
time = 2671.89 secondes

Train loss 0.14116224167314737 micro_f1_score 0.8167205169628433 
 
time = 131.71 secondes

Val loss 0.19006939293419728 micro_f1_score 0.7398095238095238
 
----------
Epoch 4/40
time = 2674.34 secondes

Train loss 0.12389018651232257 micro_f1_score 0.8467018892090938 
 
time = 132.20 secondes

Val loss 0.19162800126388424 micro_f1_score 0.7498144023756496
 
----------
Epoch 5/40
time = 2675.20 secondes

Train loss 0.10894957195866753 micro_f1_score 0.8708384587913177 
 
time = 131.98 secondes

Val loss 0.18821529372305165 micro_f1_score 0.7573253193087904
 
----------
Epoch 6/40
time = 2685.82 secondes

Train loss 0.09750085842065714 micro_f1_score 0.8843978643464506 
 
time = 129.74 secondes

Val loss 0.19876127873287827 micro_f1_score 0.7555066079295154
 
----------
Epoch 7/40
time = 2669.80 secondes

Train loss 0.08557999025325518 micro_f1_score 0.9046797836481932 
 
time = 132.98 secondes

Val loss 0.21085514604556757 micro_f1_score 0.7538738738738738
 
----------
Epoch 8/40
time = 2670.73 secondes

Train loss 0.07508876615933872 micro_f1_score 0.9176139692956756 
 
time = 134.20 secondes

Val loss 0.21547672628867823 micro_f1_score 0.7574007220216606
 
----------
Epoch 9/40
time = 2693.05 secondes

Train loss 0.06572651372853297 micro_f1_score 0.9307261701135127 
 
time = 132.71 secondes

Val loss 0.23174037210276868 micro_f1_score 0.7521551724137931
 
----------
Epoch 10/40
time = 2686.57 secondes

Train loss 0.05739611196877049 micro_f1_score 0.9410485436893203 
 
time = 130.74 secondes

Val loss 0.23930167724363138 micro_f1_score 0.7579710144927536
 
----------
Epoch 11/40
time = 2658.24 secondes

Train loss 0.051392058314973704 micro_f1_score 0.9476982591876209 
 
time = 129.73 secondes

Val loss 0.24756714054307 micro_f1_score 0.7532005689900426
 
----------
Epoch 12/40
time = 2630.02 secondes

Train loss 0.04404652084534367 micro_f1_score 0.9568799568799569 
 
time = 128.77 secondes

Val loss 0.25876277214923843 micro_f1_score 0.7611453425154041
 
----------
Epoch 13/40
time = 2694.54 secondes

Train loss 0.038880950386577226 micro_f1_score 0.9612694997310383 
 
time = 135.34 secondes

Val loss 0.28868134270925994 micro_f1_score 0.7424080028581636
 
----------
Epoch 14/40
time = 2986.37 secondes

Train loss 0.03350986240217714 micro_f1_score 0.9667919318966178 
 
time = 154.59 secondes

Val loss 0.28781587386229 micro_f1_score 0.7551519385260217
 
----------
Epoch 15/40
time = 3137.62 secondes

Train loss 0.030592218914639775 micro_f1_score 0.9693044644567926 
 
time = 156.11 secondes

Val loss 0.3115898751821674 micro_f1_score 0.7437609841827768
 
----------
Epoch 16/40
time = 2734.53 secondes

Train loss 0.026959628259620488 micro_f1_score 0.9731571893547769 
 
time = 153.64 secondes

Val loss 0.3042312851939045 micro_f1_score 0.7540069686411149
 
----------
Epoch 17/40
time = 3111.62 secondes

Train loss 0.02407024596505253 micro_f1_score 0.9756228029955677 
 
time = 155.29 secondes

Val loss 0.3223592436215917 micro_f1_score 0.7536945812807883
 
----------
Epoch 18/40
time = 3128.38 secondes

Train loss 0.0221588852250034 micro_f1_score 0.97807705551517 
 
time = 156.46 secondes

Val loss 0.32683169035637966 micro_f1_score 0.7543424317617867
 
----------
Epoch 19/40
time = 3137.41 secondes

Train loss 0.018490880750421736 micro_f1_score 0.9818957960132636 
 
time = 156.44 secondes

Val loss 0.3200327534167493 micro_f1_score 0.7599001070281841
 
----------
Epoch 20/40
time = 3139.26 secondes

Train loss 0.017043585350017508 micro_f1_score 0.9829434883809669 
 
time = 157.75 secondes

Val loss 0.33772655539825314 micro_f1_score 0.7614942528735631
 
----------
Epoch 21/40
time = 2870.26 secondes

Train loss 0.016211189888050644 micro_f1_score 0.9839417172063927 
 
time = 127.85 secondes

Val loss 0.3573837423178016 micro_f1_score 0.7510948905109489
 
----------
Epoch 22/40
time = 2622.35 secondes

Train loss 0.015120665372248638 micro_f1_score 0.9851634310995843 
 
time = 128.91 secondes

Val loss 0.3609097073556947 micro_f1_score 0.7553743513713861
 
----------
Epoch 23/40
time = 2623.46 secondes

Train loss 0.013205127187465478 micro_f1_score 0.9858715107201341 
 
time = 128.42 secondes

Val loss 0.35013758452212224 micro_f1_score 0.7660668380462724
 
----------
Epoch 24/40
time = 2627.11 secondes

Train loss 0.011226232206739427 micro_f1_score 0.9892186369004534 
 
time = 128.31 secondes

Val loss 0.34835617977087613 micro_f1_score 0.7710583153347732
 
----------
Epoch 25/40
time = 2609.08 secondes

Train loss 0.01098339875348789 micro_f1_score 0.9891391334171716 
 
time = 126.41 secondes

Val loss 0.37992256049249995 micro_f1_score 0.7523533671252715
 
----------
Epoch 26/40
time = 2607.62 secondes

Train loss 0.009377040598816281 micro_f1_score 0.9912805087004531 
 
time = 126.51 secondes

Val loss 0.38272341230853657 micro_f1_score 0.7528653295128941
 
----------
Epoch 27/40
time = 2609.07 secondes

Train loss 0.008704520246330732 micro_f1_score 0.9909039010466222 
 
time = 126.51 secondes

Val loss 0.3935484005535235 micro_f1_score 0.7554915376305366
 
----------
Epoch 28/40
time = 2650.60 secondes

Train loss 0.00852271299533911 micro_f1_score 0.9922740247383445 
 
time = 133.01 secondes

Val loss 0.3976423444806552 micro_f1_score 0.7549715909090909
 
----------
Epoch 29/40
time = 2667.60 secondes

Train loss 0.008055082638262927 micro_f1_score 0.9923855935429834 
 
time = 127.67 secondes

Val loss 0.39448484497480707 micro_f1_score 0.7574437182280319
 
----------
Epoch 30/40
time = 2614.76 secondes

Train loss 0.006925393286400248 micro_f1_score 0.9931113225499524 
 
time = 127.27 secondes

Val loss 0.3979289013831342 micro_f1_score 0.7570703408266861
 
----------
Epoch 31/40
time = 2609.73 secondes

Train loss 0.006335357465345501 micro_f1_score 0.9941875925996277 
 
time = 127.82 secondes

Val loss 0.4191802744982672 micro_f1_score 0.7542281396185677
 
----------
Epoch 32/40
time = 2614.17 secondes

Train loss 0.006458582652513534 micro_f1_score 0.9940711462450592 
 
time = 128.52 secondes

Val loss 0.40984233737480447 micro_f1_score 0.7560087399854334
 
----------
Epoch 33/40
time = 2618.71 secondes

Train loss 0.004964846811364279 micro_f1_score 0.9953219488076676 
 
time = 129.14 secondes

Val loss 0.41398610528863844 micro_f1_score 0.7583723442563918
 
----------
Epoch 34/40
time = 2613.14 secondes

Train loss 0.004114989988201168 micro_f1_score 0.9964632059326869 
 
time = 127.74 secondes

Val loss 0.4246144732002352 micro_f1_score 0.7562296858071506
 
----------
Epoch 35/40
time = 2614.26 secondes

Train loss 0.0037546916877856658 micro_f1_score 0.9964648192496294 
 
time = 127.64 secondes

Val loss 0.43190916383364164 micro_f1_score 0.7553342816500711
 
----------
Epoch 36/40
time = 2614.20 secondes

Train loss 0.0036155445336155502 micro_f1_score 0.9970353477765108 
 
time = 127.01 secondes

Val loss 0.438643853928222 micro_f1_score 0.7641643059490085
 
----------
Epoch 37/40
time = 2606.99 secondes

Train loss 0.002522118954751735 micro_f1_score 0.9977198449494565 
 
time = 126.27 secondes

Val loss 0.4388839613463058 micro_f1_score 0.7625394598386532
 
----------
Epoch 38/40
time = 2602.99 secondes

Train loss 0.001840759811583101 micro_f1_score 0.9988221436984688 
 
time = 126.80 secondes

Val loss 0.43739768227592846 micro_f1_score 0.7616011335458731
 
----------
Epoch 39/40
time = 2599.41 secondes

Train loss 0.0020228839646537185 micro_f1_score 0.9982902085945514 
 
time = 125.93 secondes

Val loss 0.4402136388616484 micro_f1_score 0.7615658362989324
 
----------
Epoch 40/40
time = 2607.24 secondes

Train loss 0.0014100572415315525 micro_f1_score 0.9988221436984688 
 
time = 126.66 secondes

Val loss 0.4517062783974116 micro_f1_score 0.7571273908336341
 
----------
best_f1_socre 0.7710583153347732 best_epoch 24

average train time 2715.2891274273397

average val time 133.76227564811705
 
time = 131.63 secondes

test_f1_score 0.7650771388499299

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_256_1
----------
Epoch 1/40
time = 3098.32 secondes

Train loss 0.22437530157936586 micro_f1_score 0.693574889403834 
 
time = 138.16 secondes

Val loss 0.19459027125210057 micro_f1_score 0.7161904761904763
 
----------
Epoch 2/40
time = 3101.10 secondes

Train loss 0.15138667444992174 micro_f1_score 0.8011025090186858 
 
time = 138.70 secondes

Val loss 0.17447984914799206 micro_f1_score 0.7628012048192772
 
----------
Epoch 3/40
time = 3096.38 secondes

Train loss 0.12781350478060075 micro_f1_score 0.8414100421739475 
 
time = 139.63 secondes

Val loss 0.16992495441045918 micro_f1_score 0.7709580838323352
 
----------
Epoch 4/40
time = 3185.26 secondes

Train loss 0.11379080342447705 micro_f1_score 0.8607514748386585 
 
time = 152.47 secondes

Val loss 0.17142946957076183 micro_f1_score 0.770153730783652
 
----------
Epoch 5/40
time = 3098.45 secondes

Train loss 0.10096520972144496 micro_f1_score 0.8807123524104656 
 
time = 141.97 secondes

Val loss 0.1859366497421851 micro_f1_score 0.7656423546834507
 
----------
Epoch 6/40
time = 3105.53 secondes

Train loss 0.08942859775747533 micro_f1_score 0.8963431399505086 
 
time = 139.22 secondes

Val loss 0.19195797569194778 micro_f1_score 0.7706489675516224
 
----------
Epoch 7/40
time = 3126.09 secondes

Train loss 0.07892535471164429 micro_f1_score 0.9125097732603596 
 
time = 141.72 secondes

Val loss 0.19676498201538306 micro_f1_score 0.7749453750910416
 
----------
Epoch 8/40
time = 3136.38 secondes

Train loss 0.07000643235656458 micro_f1_score 0.9231726767028435 
 
time = 142.08 secondes

Val loss 0.20814208191682082 micro_f1_score 0.7600297176820209
 
----------
Epoch 9/40
time = 3079.62 secondes

Train loss 0.0630883288074721 micro_f1_score 0.9325100963031997 
 
time = 137.03 secondes

Val loss 0.2115053177246305 micro_f1_score 0.777056277056277
 
----------
Epoch 10/40
time = 3070.75 secondes

Train loss 0.054187630903110046 micro_f1_score 0.94393751933189 
 
time = 137.00 secondes

Val loss 0.20878798667280402 micro_f1_score 0.7893972403776326
 
----------
Epoch 11/40
time = 3072.62 secondes

Train loss 0.046540183181295526 micro_f1_score 0.9530382479950649 
 
time = 137.07 secondes

Val loss 0.2355352391595723 micro_f1_score 0.7769284225156359
 
----------
Epoch 12/40
time = 3072.29 secondes

Train loss 0.04259873867865551 micro_f1_score 0.9572491422844147 
 
time = 137.09 secondes

Val loss 0.24922891315378126 micro_f1_score 0.7653454928095405
 
----------
Epoch 13/40
time = 3073.48 secondes

Train loss 0.037720280078969694 micro_f1_score 0.9618179720844389 
 
time = 137.01 secondes

Val loss 0.25070803777360523 micro_f1_score 0.770693906305037
 
----------
Epoch 14/40
time = 3072.41 secondes

Train loss 0.0333446326543921 micro_f1_score 0.9667458432304038 
 
time = 137.01 secondes

Val loss 0.2583584237660541 micro_f1_score 0.7771469127040453
 
----------
Epoch 15/40
time = 3072.31 secondes

Train loss 0.029444178840069956 micro_f1_score 0.9704541099827553 
 
time = 137.16 secondes

Val loss 0.2707461947422536 micro_f1_score 0.7758931793576327
 
----------
Epoch 16/40
time = 3070.65 secondes

Train loss 0.02680554166771807 micro_f1_score 0.9732173646499865 
 
time = 137.13 secondes

Val loss 0.28557477366240297 micro_f1_score 0.7711442786069651
 
----------
Epoch 17/40
time = 3072.41 secondes

Train loss 0.022970034025173197 micro_f1_score 0.9774292272379494 
 
time = 136.79 secondes

Val loss 0.28882306569912397 micro_f1_score 0.7713675213675214
 
----------
Epoch 18/40
time = 3071.25 secondes

Train loss 0.02215137508485953 micro_f1_score 0.9775298073983492 
 
time = 136.91 secondes

Val loss 0.29643311449250237 micro_f1_score 0.7812610073969708
 
----------
Epoch 19/40
time = 3070.34 secondes

Train loss 0.019389354355050133 micro_f1_score 0.9801031124689707 
 
time = 136.94 secondes

Val loss 0.31313273738153646 micro_f1_score 0.7685282753775905
 
----------
Epoch 20/40
time = 3069.08 secondes

Train loss 0.01718033181591513 micro_f1_score 0.9830159154230755 
 
time = 136.92 secondes

Val loss 0.3131682309703749 micro_f1_score 0.7725968436154951
 
----------
Epoch 21/40
time = 3071.07 secondes

Train loss 0.015696184647579988 micro_f1_score 0.9841693686820522 
 
time = 137.05 secondes

Val loss 0.3235643848654677 micro_f1_score 0.7676840215439856
 
----------
Epoch 22/40
time = 3071.00 secondes

Train loss 0.015230908711715643 micro_f1_score 0.9848594637885665 
 
time = 136.97 secondes

Val loss 0.317523899381278 micro_f1_score 0.7692307692307693
 
----------
Epoch 23/40
time = 3071.14 secondes

Train loss 0.013039372295302732 micro_f1_score 0.9859916254282453 
 
time = 137.03 secondes

Val loss 0.32425092270628353 micro_f1_score 0.7746427262733604
 
----------
Epoch 24/40
time = 3073.18 secondes

Train loss 0.012011284779087087 micro_f1_score 0.9875404839016956 
 
time = 136.87 secondes

Val loss 0.33735126159230217 micro_f1_score 0.7733531819873464
 
----------
Epoch 25/40
time = 3072.09 secondes

Train loss 0.01122890977309031 micro_f1_score 0.987938971959061 
 
time = 137.10 secondes

Val loss 0.34500192991289935 micro_f1_score 0.7708104143747709
 
----------
Epoch 26/40
time = 3071.15 secondes

Train loss 0.00885457143010278 micro_f1_score 0.9905160921729195 
 
time = 136.94 secondes

Val loss 0.3669190301758344 micro_f1_score 0.7729789590254708
 
----------
Epoch 27/40
time = 3070.68 secondes

Train loss 0.009841618043496923 micro_f1_score 0.9901691815272062 
 
time = 136.93 secondes

Val loss 0.3537699238733068 micro_f1_score 0.7814227792112054
 
----------
Epoch 28/40
time = 3071.16 secondes

Train loss 0.008059482857205293 micro_f1_score 0.9913573196268799 
 
time = 137.14 secondes

Val loss 0.37105664580327563 micro_f1_score 0.7715549005158437
 
----------
Epoch 29/40
time = 3070.74 secondes

Train loss 0.007692236929155492 micro_f1_score 0.992436048500513 
 
time = 136.91 secondes

Val loss 0.369153051347029 micro_f1_score 0.7771679473106476
 
----------
Epoch 30/40
time = 3072.27 secondes

Train loss 0.0073774832848439555 micro_f1_score 0.9928128683880291 
 
time = 136.67 secondes

Val loss 0.36255303428309865 micro_f1_score 0.7807211184694629
 
----------
Epoch 31/40
time = 3068.66 secondes

Train loss 0.005528608528251638 micro_f1_score 0.9947504564820451 
 
time = 136.73 secondes

Val loss 0.38422782047361626 micro_f1_score 0.773577981651376
 
----------
Epoch 32/40
time = 3068.63 secondes

Train loss 0.005655473345475846 micro_f1_score 0.9942939744370054 
 
time = 136.48 secondes

Val loss 0.3885471451966489 micro_f1_score 0.777254617892068
 
----------
Epoch 33/40
time = 3067.51 secondes

Train loss 0.004588250970721732 micro_f1_score 0.9953608639440262 
 
time = 136.45 secondes

Val loss 0.40141951091221123 micro_f1_score 0.7684587813620072
 
----------
Epoch 34/40
time = 3067.95 secondes

Train loss 0.004695274832323907 micro_f1_score 0.9953201689304874 
 
time = 137.03 secondes

Val loss 0.4059839411715015 micro_f1_score 0.7672819399203765
 
----------
Epoch 35/40
time = 3069.50 secondes

Train loss 0.003961529260558162 micro_f1_score 0.9963506424389873 
 
time = 136.98 secondes

Val loss 0.39322246232482255 micro_f1_score 0.7757009345794392
 
----------
Epoch 36/40
time = 3068.93 secondes

Train loss 0.0034413087383114004 micro_f1_score 0.9967310323855862 
 
time = 136.93 secondes

Val loss 0.3895134120324596 micro_f1_score 0.7837445573294629
 
----------
Epoch 37/40
time = 3069.38 secondes

Train loss 0.002817877054084428 micro_f1_score 0.9970355731225297 
 
time = 136.94 secondes

Val loss 0.3940821227724435 micro_f1_score 0.7761521580102414
 
----------
Epoch 38/40
time = 3068.65 secondes

Train loss 0.0018880376339193144 micro_f1_score 0.9980617945502223 
 
time = 136.74 secondes

Val loss 0.4023083136340634 micro_f1_score 0.777209642074507
 
----------
Epoch 39/40
time = 3069.44 secondes

Train loss 0.0019097286858727005 micro_f1_score 0.9982131315819489 
 
time = 136.97 secondes

Val loss 0.39907441379838304 micro_f1_score 0.7774566473988439
 
----------
Epoch 40/40
time = 3070.73 secondes

Train loss 0.0015374300738511847 micro_f1_score 0.9983277591973243 
 
time = 137.22 secondes

Val loss 0.40680825917935765 micro_f1_score 0.7775781530722242
 
----------
best_f1_socre 0.7893972403776326 best_epoch 10

average train time 3080.464173412323

average val time 137.90237558484077
 
time = 140.93 secondes

test_f1_score 0.7717813051146384

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_512_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 1.31 GiB (GPU 1; 79.20 GiB total capacity; 61.01 GiB already allocated; 723.31 MiB free; 63.49 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_256_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 774.00 MiB (GPU 1; 79.20 GiB total capacity; 61.26 GiB already allocated; 111.31 MiB free; 64.08 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_512_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 1.51 GiB (GPU 1; 79.20 GiB total capacity; 60.11 GiB already allocated; 907.31 MiB free; 63.31 GiB reserved in total by PyTorch)
datasets imported
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_64_2
----------
Epoch 1/40
time = 1576.42 secondes

Train loss 0.26038070746638753 micro_f1_score 0.6366005565616856 
 
time = 49.64 secondes

Val loss 0.20460394354628736 micro_f1_score 0.6943883730318934
 
----------
Epoch 2/40
time = 1578.19 secondes

Train loss 0.16927425500240412 micro_f1_score 0.7702452091489799 
 
time = 49.92 secondes

Val loss 0.19439527771023454 micro_f1_score 0.7180685358255451
 
----------
Epoch 3/40
time = 1577.74 secondes

Train loss 0.14237836865825695 micro_f1_score 0.8157018754566858 
 
time = 49.97 secondes

Val loss 0.1912215680616801 micro_f1_score 0.741074783915821
 
----------
Epoch 4/40
time = 1578.67 secondes

Train loss 0.12527214882408713 micro_f1_score 0.8427354548376728 
 
time = 49.94 secondes

Val loss 0.20454249506602523 micro_f1_score 0.7413333333333334
 
----------
Epoch 5/40
time = 1577.76 secondes

Train loss 0.10887373186614331 micro_f1_score 0.866865671641791 
 
time = 49.92 secondes

Val loss 0.1983530699718194 micro_f1_score 0.7536231884057971
 
----------
Epoch 6/40
time = 1578.00 secondes

Train loss 0.09539877698412752 micro_f1_score 0.8881401617250673 
 
time = 50.10 secondes

Val loss 0.2034358646048874 micro_f1_score 0.7760088855979268
 
----------
Epoch 7/40
time = 1576.06 secondes

Train loss 0.08153067190485361 micro_f1_score 0.9066698183823818 
 
time = 50.02 secondes

Val loss 0.23338517546653748 micro_f1_score 0.7522092612230471
 
----------
Epoch 8/40
time = 1576.83 secondes

Train loss 0.06983245846286819 micro_f1_score 0.9215670941507663 
 
time = 50.31 secondes

Val loss 0.25634749905496346 micro_f1_score 0.756946887091101
 
----------
Epoch 9/40
time = 1576.86 secondes

Train loss 0.057329153282953814 micro_f1_score 0.9393209540828218 
 
time = 49.66 secondes

Val loss 0.26485018517638814 micro_f1_score 0.7582496413199427
 
----------
Epoch 10/40
time = 1577.90 secondes

Train loss 0.04797383648330799 micro_f1_score 0.9478452781972353 
 
time = 50.03 secondes

Val loss 0.28787558953293035 micro_f1_score 0.7506315409599422
 
----------
Epoch 11/40
time = 1576.13 secondes

Train loss 0.04039002981766857 micro_f1_score 0.957849677991593 
 
time = 50.03 secondes

Val loss 0.3139912257062607 micro_f1_score 0.7593316743690011
 
----------
Epoch 12/40
time = 1575.40 secondes

Train loss 0.033467252218995144 micro_f1_score 0.96447950740812 
 
time = 49.44 secondes

Val loss 0.34736340981526453 micro_f1_score 0.7445152158527955
 
----------
Epoch 13/40
time = 1573.83 secondes

Train loss 0.027529553350675768 micro_f1_score 0.9699646643109541 
 
time = 50.02 secondes

Val loss 0.3625737482407054 micro_f1_score 0.7423117709437963
 
----------
Epoch 14/40
time = 1573.49 secondes

Train loss 0.024987904457396383 micro_f1_score 0.9726830389640244 
 
time = 49.62 secondes

Val loss 0.364262125531181 micro_f1_score 0.7505376344086021
 
----------
Epoch 15/40
time = 1589.49 secondes

Train loss 0.021720667110613403 micro_f1_score 0.9775143403441684 
 
time = 50.04 secondes

Val loss 0.3730852310774756 micro_f1_score 0.7566787003610108
 
----------
Epoch 16/40
time = 1580.64 secondes

Train loss 0.019957136596460735 micro_f1_score 0.9777947348340328 
 
time = 49.44 secondes

Val loss 0.37988627592071156 micro_f1_score 0.7614475627769572
 
----------
Epoch 17/40
time = 1578.41 secondes

Train loss 0.016347896607633604 micro_f1_score 0.9826422004348987 
 
time = 49.27 secondes

Val loss 0.4164870019818916 micro_f1_score 0.7497322384862548
 
----------
Epoch 18/40
time = 1578.87 secondes

Train loss 0.01563267008703504 micro_f1_score 0.9844464775846294 
 
time = 49.87 secondes

Val loss 0.42455766081321433 micro_f1_score 0.7524752475247525
 
----------
Epoch 19/40
time = 1579.62 secondes

Train loss 0.013699901598004357 micro_f1_score 0.9853945010105631 
 
time = 49.71 secondes

Val loss 0.45102342059377765 micro_f1_score 0.749098774333093
 
----------
Epoch 20/40
time = 1580.63 secondes

Train loss 0.013213090730612356 micro_f1_score 0.9870525514089871 
 
time = 49.77 secondes

Val loss 0.4765105760488354 micro_f1_score 0.7531531531531532
 
----------
Epoch 21/40
time = 1580.83 secondes

Train loss 0.013332737535227778 micro_f1_score 0.9862252663622527 
 
time = 49.80 secondes

Val loss 0.4708457997099298 micro_f1_score 0.7529495888451913
 
----------
Epoch 22/40
time = 1581.84 secondes

Train loss 0.011269021402338628 micro_f1_score 0.9887401095556907 
 
time = 50.16 secondes

Val loss 0.44212440706667355 micro_f1_score 0.759027266028003
 
----------
Epoch 23/40
time = 1587.34 secondes

Train loss 0.009374313356657933 micro_f1_score 0.9903260207190738 
 
time = 50.21 secondes

Val loss 0.4777905653978958 micro_f1_score 0.7521676300578035
 
----------
Epoch 24/40
time = 1588.19 secondes

Train loss 0.008620446323556616 micro_f1_score 0.990900437845041 
 
time = 50.76 secondes

Val loss 0.502041892194357 micro_f1_score 0.7426192278576836
 
----------
Epoch 25/40
time = 1590.36 secondes

Train loss 0.007915088412092855 micro_f1_score 0.992197906755471 
 
time = 50.29 secondes

Val loss 0.5115535891935473 micro_f1_score 0.7536646406864497
 
----------
Epoch 26/40
time = 1587.55 secondes

Train loss 0.007094311734328732 micro_f1_score 0.9920882464815519 
 
time = 50.26 secondes

Val loss 0.5426429536987524 micro_f1_score 0.7456626061277224
 
----------
Epoch 27/40
time = 1589.39 secondes

Train loss 0.007541407367574032 micro_f1_score 0.993122316373447 
 
time = 50.20 secondes

Val loss 0.5173759447013746 micro_f1_score 0.747844827586207
 
----------
Epoch 28/40
time = 1587.74 secondes

Train loss 0.006650118742110355 micro_f1_score 0.9932697060724741 
 
time = 50.04 secondes

Val loss 0.5244463012843835 micro_f1_score 0.7526881720430108
 
----------
Epoch 29/40
time = 1589.46 secondes

Train loss 0.006210327441655315 micro_f1_score 0.9941116134179235 
 
time = 50.37 secondes

Val loss 0.5161806840877063 micro_f1_score 0.7560795873249814
 
----------
Epoch 30/40
time = 1588.02 secondes

Train loss 0.00486953267774863 micro_f1_score 0.9950173063025369 
 
time = 50.23 secondes

Val loss 0.5276941809742177 micro_f1_score 0.7572463768115943
 
----------
Epoch 31/40
time = 1583.76 secondes

Train loss 0.004720384724383654 micro_f1_score 0.9951716534235638 
 
time = 49.64 secondes

Val loss 0.5212514533615503 micro_f1_score 0.7571324067300659
 
----------
Epoch 32/40
time = 1578.01 secondes

Train loss 0.003074280731476671 micro_f1_score 0.9966542468253364 
 
time = 49.63 secondes

Val loss 0.5366463388820164 micro_f1_score 0.7600585223116314
 
----------
Epoch 33/40
time = 1578.57 secondes

Train loss 0.003379380191764286 micro_f1_score 0.9965439975694049 
 
time = 49.87 secondes

Val loss 0.5480495029297031 micro_f1_score 0.7494584837545126
 
----------
Epoch 34/40
time = 1577.74 secondes

Train loss 0.0028147964413644355 micro_f1_score 0.9975683890577508 
 
time = 49.45 secondes

Val loss 0.5602516236363865 micro_f1_score 0.7583988563259471
 
----------
Epoch 35/40
time = 1589.13 secondes

Train loss 0.0030326929034451603 micro_f1_score 0.9969981380856482 
 
time = 52.61 secondes

Val loss 0.555387014248332 micro_f1_score 0.7607843137254902
 
----------
Epoch 36/40
time = 1615.05 secondes

Train loss 0.0015623480659857567 micro_f1_score 0.9983281404362034 
 
time = 52.38 secondes

Val loss 0.5614484549790132 micro_f1_score 0.7537906137184116
 
----------
Epoch 37/40
time = 1626.49 secondes

Train loss 0.001689074352681338 micro_f1_score 0.9982132674396502 
 
time = 54.57 secondes

Val loss 0.5891592158157317 micro_f1_score 0.7522538766678687
 
----------
Epoch 38/40
time = 1626.67 secondes

Train loss 0.0012699913046859152 micro_f1_score 0.9986700611771858 
 
time = 53.28 secondes

Val loss 0.5801729597761983 micro_f1_score 0.756193895870736
 
----------
Epoch 39/40
time = 1625.81 secondes

Train loss 0.0010255267940543114 micro_f1_score 0.9990120079039368 
 
time = 53.50 secondes

Val loss 0.5694238909932433 micro_f1_score 0.7585957292797683
 
----------
Epoch 40/40
time = 1636.50 secondes

Train loss 0.0010268989684592016 micro_f1_score 0.998861047835991 
 
time = 54.80 secondes

Val loss 0.6083341659825356 micro_f1_score 0.751954513148543
 
----------
best_f1_socre 0.7760088855979268 best_epoch 6

average train time 1586.735289078951

average val time 50.46952927708626
 
time = 56.98 secondes

test_f1_score 0.7576866764275255

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_128_2
----------
Epoch 1/40
Attention type 'block_sparse' is not possible if sequence_length: 1024 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 1408 with config.block_size = 128, config.num_random_blocks = 3. Changing attention type to 'original_full'...
time = 950.63 secondes

Train loss 0.26179685393969215 micro_f1_score 0.6394450988090564 
 
time = 43.50 secondes

Val loss 0.21996718913805288 micro_f1_score 0.6754491017964072
 
----------
Epoch 2/40
time = 937.87 secondes

Train loss 0.17010342108236776 micro_f1_score 0.770895031260283 
 
time = 44.14 secondes

Val loss 0.193120219668404 micro_f1_score 0.7232212666145426
 
----------
Epoch 3/40
time = 929.51 secondes

Train loss 0.14608601894363896 micro_f1_score 0.8085123664866183 
 
time = 44.96 secondes

Val loss 0.18506804719323017 micro_f1_score 0.7483044461190657
 
----------
Epoch 4/40
time = 932.18 secondes

Train loss 0.12661143329732857 micro_f1_score 0.8409621108519024 
 
time = 44.97 secondes

Val loss 0.20087776223167045 micro_f1_score 0.7480344440284538
 
----------
Epoch 5/40
time = 931.58 secondes

Train loss 0.11209053971074723 micro_f1_score 0.8629639966508513 
 
time = 43.15 secondes

Val loss 0.19696888327598572 micro_f1_score 0.7518796992481204
 
----------
Epoch 6/40
time = 930.83 secondes

Train loss 0.09915138116307758 micro_f1_score 0.8821850471735512 
 
time = 43.74 secondes

Val loss 0.2044030177544375 micro_f1_score 0.7529751172015867
 
----------
Epoch 7/40
time = 932.36 secondes

Train loss 0.08506376995696678 micro_f1_score 0.9019069292945147 
 
time = 43.18 secondes

Val loss 0.23962537467968267 micro_f1_score 0.7465980139757263
 
----------
Epoch 8/40
time = 933.27 secondes

Train loss 0.07528765561092679 micro_f1_score 0.9157276995305165 
 
time = 41.05 secondes

Val loss 0.2513451566461657 micro_f1_score 0.7492774566473989
 
----------
Epoch 9/40
time = 930.32 secondes

Train loss 0.06303759605439195 micro_f1_score 0.9299863786728935 
 
time = 42.35 secondes

Val loss 0.25624012421877657 micro_f1_score 0.756717501815541
 
----------
Epoch 10/40
time = 934.45 secondes

Train loss 0.05505454774132116 micro_f1_score 0.9397954439795443 
 
time = 42.43 secondes

Val loss 0.2694669994907301 micro_f1_score 0.7565217391304347
 
----------
Epoch 11/40
time = 934.70 secondes

Train loss 0.045864919192988324 micro_f1_score 0.9492468134414832 
 
time = 44.62 secondes

Val loss 0.2784085634057639 micro_f1_score 0.7633262260127933
 
----------
Epoch 12/40
time = 933.87 secondes

Train loss 0.03971794043858494 micro_f1_score 0.9573649376635371 
 
time = 43.52 secondes

Val loss 0.29676633657979185 micro_f1_score 0.7595026642984014
 
----------
Epoch 13/40
time = 933.32 secondes

Train loss 0.0337583606150253 micro_f1_score 0.9641498559077809 
 
time = 44.66 secondes

Val loss 0.3073504805320599 micro_f1_score 0.758303886925795
 
----------
Epoch 14/40
time = 922.95 secondes

Train loss 0.02943284388386166 micro_f1_score 0.9680806222937504 
 
time = 42.53 secondes

Val loss 0.3389123297128521 micro_f1_score 0.7584269662921349
 
----------
Epoch 15/40
time = 934.62 secondes

Train loss 0.02578793782536318 micro_f1_score 0.9723794950267789 
 
time = 44.19 secondes

Val loss 0.3647378619577064 micro_f1_score 0.7509659290481209
 
----------
Epoch 16/40
time = 932.73 secondes

Train loss 0.022260525997629524 micro_f1_score 0.9763616891064871 
 
time = 43.37 secondes

Val loss 0.384125535116821 micro_f1_score 0.7545454545454546
 
----------
Epoch 17/40
time = 936.93 secondes

Train loss 0.019805732624278077 micro_f1_score 0.9789899915959965 
 
time = 43.12 secondes

Val loss 0.3818546802294059 micro_f1_score 0.7635850388143967
 
----------
Epoch 18/40
time = 932.91 secondes

Train loss 0.01788702533999353 micro_f1_score 0.9810946033685978 
 
time = 46.23 secondes

Val loss 0.3938412998543411 micro_f1_score 0.7560014331780724
 
----------
Epoch 19/40
time = 946.23 secondes

Train loss 0.015521142840031995 micro_f1_score 0.9832990162434225 
 
time = 43.91 secondes

Val loss 0.4058228860624501 micro_f1_score 0.7625951431678144
 
----------
Epoch 20/40
time = 943.60 secondes

Train loss 0.013460724142082204 micro_f1_score 0.9855061408192843 
 
time = 45.47 secondes

Val loss 0.4262582367805184 micro_f1_score 0.7627302275189599
 
----------
Epoch 21/40
time = 941.54 secondes

Train loss 0.01356884398689569 micro_f1_score 0.9854045196448306 
 
time = 45.10 secondes

Val loss 0.4199562786055393 micro_f1_score 0.7656082768462361
 
----------
Epoch 22/40
time = 945.57 secondes

Train loss 0.011479865088308994 micro_f1_score 0.9879362179853102 
 
time = 45.48 secondes

Val loss 0.44512106893492526 micro_f1_score 0.7629764065335755
 
----------
Epoch 23/40
time = 944.07 secondes

Train loss 0.010031681631451581 micro_f1_score 0.9886264216972878 
 
time = 45.55 secondes

Val loss 0.44462649634138485 micro_f1_score 0.7706685837526959
 
----------
Epoch 24/40
time = 945.13 secondes

Train loss 0.009669454289391283 micro_f1_score 0.9901789113056718 
 
time = 42.11 secondes

Val loss 0.4590437522188562 micro_f1_score 0.7620764239365536
 
----------
Epoch 25/40
time = 942.89 secondes

Train loss 0.008702143542180787 micro_f1_score 0.9911800486618005 
 
time = 46.13 secondes

Val loss 0.47890863347737517 micro_f1_score 0.7547169811320754
 
----------
Epoch 26/40
time = 952.33 secondes

Train loss 0.008949226538148192 micro_f1_score 0.9912237376999354 
 
time = 49.75 secondes

Val loss 0.4777347697097747 micro_f1_score 0.7643593519882179
 
----------
Epoch 27/40
time = 1096.29 secondes

Train loss 0.008164200345463295 micro_f1_score 0.9911713220184184 
 
time = 50.78 secondes

Val loss 0.4730953685328609 micro_f1_score 0.7654676258992806
 
----------
Epoch 28/40
time = 1093.14 secondes

Train loss 0.007815928253484681 micro_f1_score 0.9916381603952871 
 
time = 51.09 secondes

Val loss 0.496630924158409 micro_f1_score 0.7552498189717596
 
----------
Epoch 29/40
time = 1104.15 secondes

Train loss 0.00625238311402935 micro_f1_score 0.9932324538057942 
 
time = 49.99 secondes

Val loss 0.5036537273496878 micro_f1_score 0.7574007220216606
 
----------
Epoch 30/40
time = 1101.63 secondes

Train loss 0.005036103368059287 micro_f1_score 0.9948686761184387 
 
time = 50.92 secondes

Val loss 0.5144022038725556 micro_f1_score 0.7587230883444692
 
----------
Epoch 31/40
time = 1106.65 secondes

Train loss 0.004116419143751914 micro_f1_score 0.9956288722490402 
 
time = 51.19 secondes

Val loss 0.5117249708683764 micro_f1_score 0.7650471356055112
 
----------
Epoch 32/40
time = 1254.19 secondes

Train loss 0.003952442408948267 micro_f1_score 0.9961240310077519 
 
time = 62.85 secondes

Val loss 0.5477109099509286 micro_f1_score 0.7619728377412437
 
----------
Epoch 33/40
time = 1370.73 secondes

Train loss 0.004240200211182758 micro_f1_score 0.9960080599171198 
 
time = 63.20 secondes

Val loss 0.5617013175467975 micro_f1_score 0.7549295774647887
 
----------
Epoch 34/40
time = 1364.16 secondes

Train loss 0.0032420561054276003 micro_f1_score 0.9967703940119306 
 
time = 61.51 secondes

Val loss 0.5589589574297921 micro_f1_score 0.759377211606511
 
----------
Epoch 35/40
time = 1368.37 secondes

Train loss 0.0030509768465177525 micro_f1_score 0.9969983661993237 
 
time = 62.00 secondes

Val loss 0.5479679847838449 micro_f1_score 0.7635590216235377
 
----------
Epoch 36/40
time = 1363.61 secondes

Train loss 0.0026834117329353584 micro_f1_score 0.9974548907882241 
 
time = 62.23 secondes

Val loss 0.5668988594266234 micro_f1_score 0.7629370629370629
 
----------
Epoch 37/40
time = 1364.12 secondes

Train loss 0.0018271719460891938 micro_f1_score 0.9980249164387724 
 
time = 62.06 secondes

Val loss 0.5690871670109326 micro_f1_score 0.7565649396735273
 
----------
Epoch 38/40
time = 1364.57 secondes

Train loss 0.0014352112607731533 micro_f1_score 0.9984429000037979 
 
time = 61.84 secondes

Val loss 0.549972900601684 micro_f1_score 0.7584453323646931
 
----------
Epoch 39/40
time = 1365.35 secondes

Train loss 0.001229423064080595 micro_f1_score 0.9988223226835846 
 
time = 61.92 secondes

Val loss 0.5606096485843424 micro_f1_score 0.7630434782608696
 
----------
Epoch 40/40
time = 1362.76 secondes

Train loss 0.0009978695606830796 micro_f1_score 0.9991645781119466 
 
time = 61.36 secondes

Val loss 0.5960588276874824 micro_f1_score 0.7558221594918842
 
----------
best_f1_socre 0.7706685837526959 best_epoch 23

average train time 1051.151881825924

average val time 49.05411611795425
 
time = 62.94 secondes

test_f1_score 0.7384937238493723

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_64_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 252.00 MiB (GPU 1; 79.20 GiB total capacity; 50.81 GiB already allocated; 147.31 MiB free; 51.76 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_128_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 79.20 GiB total capacity; 49.75 GiB already allocated; 151.31 MiB free; 51.75 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_64_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 540.00 MiB (GPU 1; 79.20 GiB total capacity; 47.64 GiB already allocated; 141.31 MiB free; 51.76 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_128_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 2.11 GiB (GPU 1; 79.20 GiB total capacity; 48.07 GiB already allocated; 1.88 GiB free; 50.02 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_256_2
----------
Epoch 1/40
time = 3366.15 secondes

Train loss 0.2355213862885763 micro_f1_score 0.6676875957120981 
 
time = 188.25 secondes

Val loss 0.199663301838226 micro_f1_score 0.7095761381475668
 
----------
Epoch 2/40
time = 3352.34 secondes

Train loss 0.16332580119639903 micro_f1_score 0.7807967313585291 
 
time = 184.88 secondes

Val loss 0.20299138446323206 micro_f1_score 0.7117834394904459
 
----------
Epoch 3/40
time = 3356.10 secondes

Train loss 0.14244681730665065 micro_f1_score 0.813637466634312 
 
time = 188.66 secondes

Val loss 0.18614329153397044 micro_f1_score 0.7502885725278953
 
----------
Epoch 4/40
time = 3353.84 secondes

Train loss 0.12469780951201379 micro_f1_score 0.8418054162487463 
 
time = 187.28 secondes

Val loss 0.1941724093478234 micro_f1_score 0.7459016393442622
 
----------
Epoch 5/40
time = 3347.23 secondes

Train loss 0.10916878348635929 micro_f1_score 0.8671834625322997 
 
time = 187.87 secondes

Val loss 0.19094229929271292 micro_f1_score 0.760917838638046
 
----------
Epoch 6/40
time = 3349.92 secondes

Train loss 0.09559730613218234 micro_f1_score 0.8898807172762461 
 
time = 187.56 secondes

Val loss 0.20900883427897438 micro_f1_score 0.7513691128148959
 
----------
Epoch 7/40
time = 3354.09 secondes

Train loss 0.08467405498078143 micro_f1_score 0.9037955803273541 
 
time = 192.85 secondes

Val loss 0.20423128993296233 micro_f1_score 0.7595029239766081
 
----------
Epoch 8/40
time = 3849.63 secondes

Train loss 0.0739824864845555 micro_f1_score 0.9185092585358232 
 
time = 214.65 secondes

Val loss 0.21923707205741133 micro_f1_score 0.756717501815541
 
----------
Epoch 9/40
time = 3861.30 secondes

Train loss 0.06632843512773245 micro_f1_score 0.927962638645651 
 
time = 216.11 secondes

Val loss 0.23074837212191254 micro_f1_score 0.7560262965668372
 
----------
Epoch 10/40
time = 3454.60 secondes

Train loss 0.058173990419117715 micro_f1_score 0.9396400948346225 
 
time = 192.16 secondes

Val loss 0.23539750974197857 micro_f1_score 0.7583212735166425
 
----------
Epoch 11/40
time = 3377.27 secondes

Train loss 0.04953929058964121 micro_f1_score 0.9507638754592923 
 
time = 188.92 secondes

Val loss 0.251059106505308 micro_f1_score 0.7591865858009277
 
----------
Epoch 12/40
time = 3386.51 secondes

Train loss 0.043337497970525614 micro_f1_score 0.9563300197269176 
 
time = 192.10 secondes

Val loss 0.25818360914460947 micro_f1_score 0.7584715212689258
 
----------
Epoch 13/40
time = 3385.32 secondes

Train loss 0.0396034224646854 micro_f1_score 0.9615844025738836 
 
time = 188.83 secondes

Val loss 0.2756466811797658 micro_f1_score 0.7568725455194574
 
----------
Epoch 14/40
time = 3370.89 secondes

Train loss 0.035715704034544056 micro_f1_score 0.9626520868627753 
 
time = 186.07 secondes

Val loss 0.27587152296890977 micro_f1_score 0.7588627588627589
 
----------
Epoch 15/40
time = 3359.79 secondes

Train loss 0.0321209333042052 micro_f1_score 0.9678510998307952 
 
time = 188.72 secondes

Val loss 0.28434522388899913 micro_f1_score 0.7558940877765687
 
----------
Epoch 16/40
time = 3357.47 secondes

Train loss 0.02721370166431911 micro_f1_score 0.9732755645872474 
 
time = 192.21 secondes

Val loss 0.2892772727569596 micro_f1_score 0.7587198849334771
 
----------
Epoch 17/40
time = 3355.30 secondes

Train loss 0.025096151073094087 micro_f1_score 0.9752991715250076 
 
time = 188.76 secondes

Val loss 0.2979155115661074 micro_f1_score 0.7538738738738738
 
----------
Epoch 18/40
time = 3349.39 secondes

Train loss 0.02177204641372989 micro_f1_score 0.9776282075796398 
 
time = 190.20 secondes

Val loss 0.3052512050652113 micro_f1_score 0.7636621717530163
 
----------
Epoch 19/40
time = 3354.88 secondes

Train loss 0.020131457619566202 micro_f1_score 0.980260394792104 
 
time = 190.16 secondes

Val loss 0.3192869202523935 micro_f1_score 0.7567368032484313
 
----------
Epoch 20/40
time = 3359.34 secondes

Train loss 0.017648104354969923 micro_f1_score 0.9827750830691669 
 
time = 190.23 secondes

Val loss 0.32489564343065513 micro_f1_score 0.7626514611546688
 
----------
Epoch 21/40
time = 3374.02 secondes

Train loss 0.015441541743374572 micro_f1_score 0.9838919001450491 
 
time = 191.60 secondes

Val loss 0.3421736545250064 micro_f1_score 0.7609308885754583
 
----------
Epoch 22/40
time = 2974.37 secondes

Train loss 0.015073118352190685 micro_f1_score 0.9843583091713719 
 
time = 164.93 secondes

Val loss 0.33637941311128805 micro_f1_score 0.7634408602150538
 
----------
Epoch 23/40
time = 2937.81 secondes

Train loss 0.012749072736567557 micro_f1_score 0.9876091349269892 
 
time = 167.71 secondes

Val loss 0.34022119616875884 micro_f1_score 0.763420955201777
 
----------
Epoch 24/40
time = 2945.94 secondes

Train loss 0.012419750811720922 micro_f1_score 0.9877041379572881 
 
time = 165.54 secondes

Val loss 0.3544712423301134 micro_f1_score 0.7574007220216606
 
----------
Epoch 25/40
time = 2946.95 secondes

Train loss 0.011876993979115525 micro_f1_score 0.9878289974136619 
 
time = 165.03 secondes

Val loss 0.3550220632650813 micro_f1_score 0.7599856063332134
 
----------
Epoch 26/40
time = 2939.67 secondes

Train loss 0.010316305364946717 micro_f1_score 0.9894159750247467 
 
time = 166.99 secondes

Val loss 0.3684467563375098 micro_f1_score 0.7590404582885787
 
----------
Epoch 27/40
time = 2934.20 secondes

Train loss 0.00882416262620691 micro_f1_score 0.9913619239697098 
 
time = 164.90 secondes

Val loss 0.37586964740127815 micro_f1_score 0.7663685152057246
 
----------
Epoch 28/40
time = 2938.38 secondes

Train loss 0.007702634368051968 micro_f1_score 0.9926540554942337 
 
time = 164.17 secondes

Val loss 0.39077698500429997 micro_f1_score 0.7603539823008849
 
----------
Epoch 29/40
time = 2949.26 secondes

Train loss 0.008222417187770023 micro_f1_score 0.9915115526626318 
 
time = 166.90 secondes

Val loss 0.39059480927029594 micro_f1_score 0.7557803468208093
 
----------
Epoch 30/40
time = 2943.77 secondes

Train loss 0.007216345058672281 micro_f1_score 0.9922038410344172 
 
time = 167.51 secondes

Val loss 0.3926751139711161 micro_f1_score 0.7639885222381635
 
----------
Epoch 31/40
time = 2962.14 secondes

Train loss 0.005801925940993913 micro_f1_score 0.9941449319443388 
 
time = 167.71 secondes

Val loss 0.38969952466546515 micro_f1_score 0.7656813266041816
 
----------
Epoch 32/40
time = 2948.72 secondes

Train loss 0.005726428113619247 micro_f1_score 0.994226240218795 
 
time = 166.26 secondes

Val loss 0.41174171596276954 micro_f1_score 0.7588961510530138
 
----------
Epoch 33/40
time = 2944.11 secondes

Train loss 0.00498996888278713 micro_f1_score 0.9951705517739666 
 
time = 167.58 secondes

Val loss 0.4019321835920459 micro_f1_score 0.7671331180480805
 
----------
Epoch 34/40
time = 2942.00 secondes

Train loss 0.0046609491835215115 micro_f1_score 0.9956686930091184 
 
time = 165.85 secondes

Val loss 0.4126453101634979 micro_f1_score 0.7609475951184493
 
----------
Epoch 35/40
time = 2939.73 secondes

Train loss 0.0032918014785962867 micro_f1_score 0.9970362489550878 
 
time = 165.36 secondes

Val loss 0.4199564061203941 micro_f1_score 0.7568940493468795
 
----------
Epoch 36/40
time = 2912.40 secondes

Train loss 0.002809842319966101 micro_f1_score 0.9971122425716239 
 
time = 163.28 secondes

Val loss 0.41495755998814693 micro_f1_score 0.7645327446651949
 
----------
Epoch 37/40
time = 2902.29 secondes

Train loss 0.0023473704371418853 micro_f1_score 0.9979481723535222 
 
time = 163.02 secondes

Val loss 0.42667779741717166 micro_f1_score 0.7608461814270348
 
----------
Epoch 38/40
time = 2907.58 secondes

Train loss 0.002189637856691161 micro_f1_score 0.9979104137380798 
 
time = 163.40 secondes

Val loss 0.41780654670762235 micro_f1_score 0.7622478386167147
 
----------
Epoch 39/40
time = 2916.34 secondes

Train loss 0.0015073836235387716 micro_f1_score 0.9984798966329711 
 
time = 163.19 secondes

Val loss 0.4255054343919285 micro_f1_score 0.7647268832559801
 
----------
Epoch 40/40
time = 2915.49 secondes

Train loss 0.0016046155108490787 micro_f1_score 0.9987084029782708 
 
time = 163.67 secondes

Val loss 0.4283312190262998 micro_f1_score 0.765697883028346
 
----------
best_f1_socre 0.7671331180480805 best_epoch 33

average train time 3186.9135423600674

average val time 179.27574899196625
 
time = 171.38 secondes

test_f1_score 0.7497389488339714

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_512_2
----------
Epoch 1/40
time = 3670.26 secondes

Train loss 0.23078533707706778 micro_f1_score 0.687064059223758 
 
time = 184.30 secondes

Val loss 0.2098164670780057 micro_f1_score 0.6969577242196761
 
----------
Epoch 2/40
time = 3640.65 secondes

Train loss 0.16223938061713097 micro_f1_score 0.7850834151128557 
 
time = 182.63 secondes

Val loss 0.19448344116328192 micro_f1_score 0.7274155538098979
 
----------
Epoch 3/40
time = 3634.54 secondes

Train loss 0.14162417143784664 micro_f1_score 0.8175046554934823 
 
time = 181.37 secondes

Val loss 0.18863637122463006 micro_f1_score 0.7386146192116342
 
----------
Epoch 4/40
time = 3614.45 secondes

Train loss 0.1256295078375318 micro_f1_score 0.8441043311030089 
 
time = 179.05 secondes

Val loss 0.1969842568772738 micro_f1_score 0.7361827560795873
 
----------
Epoch 5/40
time = 3624.26 secondes

Train loss 0.11088825933098256 micro_f1_score 0.8659999203092004 
 
time = 180.89 secondes

Val loss 0.19707018979748742 micro_f1_score 0.7454407294832828
 
----------
Epoch 6/40
time = 3613.26 secondes

Train loss 0.10004805683552682 micro_f1_score 0.8812995245641838 
 
time = 179.80 secondes

Val loss 0.20425305161319796 micro_f1_score 0.7558182489841152
 
----------
Epoch 7/40
Exception
CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 79.20 GiB total capacity; 24.26 GiB already allocated; 18.31 MiB free; 24.85 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_256_2
----------
Epoch 1/40
time = 867.78 secondes

Train loss 0.21772205471187026 micro_f1_score 0.6973866389754241 
 
time = 30.15 secondes

Val loss 0.18171963743010505 micro_f1_score 0.7465940054495913
 
----------
Epoch 2/40
time = 870.21 secondes

Train loss 0.14972492432943335 micro_f1_score 0.8033032425211513 
 
time = 30.18 secondes

Val loss 0.18427569809995714 micro_f1_score 0.7435223432219301
 
----------
Epoch 3/40
time = 869.49 secondes

Train loss 0.12856896661624714 micro_f1_score 0.8380723855228953 
 
time = 29.85 secondes

Val loss 0.17060717134202114 micro_f1_score 0.7758749069247952
 
----------
Epoch 4/40
time = 868.21 secondes

Train loss 0.1113124052700293 micro_f1_score 0.8647342995169082 
 
time = 31.66 secondes

Val loss 0.16979001376961098 micro_f1_score 0.7786088257292445
 
----------
Epoch 5/40
time = 867.33 secondes

Train loss 0.09742518263860597 micro_f1_score 0.888319268081079 
 
time = 29.66 secondes

Val loss 0.18017622797948415 micro_f1_score 0.7800369685767098
 
----------
Epoch 6/40
time = 870.60 secondes

Train loss 0.08742315997411539 micro_f1_score 0.9000902916813881 
 
time = 29.91 secondes

Val loss 0.188859617978823 micro_f1_score 0.7741702741702742
 
----------
Epoch 7/40
time = 869.00 secondes

Train loss 0.07623199649104798 micro_f1_score 0.9162723397598842 
 
time = 29.92 secondes

Val loss 0.20351144024094597 micro_f1_score 0.7701729106628242
 
----------
Epoch 8/40
time = 867.28 secondes

Train loss 0.06587926322627846 micro_f1_score 0.9289945440374123 
 
time = 29.89 secondes

Val loss 0.21670322059119335 micro_f1_score 0.7757274662881476
 
----------
Epoch 9/40
time = 866.81 secondes

Train loss 0.05649473694245423 micro_f1_score 0.9411764705882353 
 
time = 30.79 secondes

Val loss 0.21465428747603152 micro_f1_score 0.7869318181818182
 
----------
Epoch 10/40
time = 867.49 secondes

Train loss 0.050994494966826995 micro_f1_score 0.9480775932164013 
 
time = 29.81 secondes

Val loss 0.21859457465957421 micro_f1_score 0.7813057438458795
 
----------
Epoch 11/40
time = 870.43 secondes

Train loss 0.04524884399211219 micro_f1_score 0.9539224037791373 
 
time = 29.84 secondes

Val loss 0.23416697746906123 micro_f1_score 0.778056651129437
 
----------
Epoch 12/40
time = 867.08 secondes

Train loss 0.03971052280132164 micro_f1_score 0.9603639728562616 
 
time = 31.07 secondes

Val loss 0.2420643174135294 micro_f1_score 0.7869085734614015
 
----------
Epoch 13/40
time = 866.86 secondes

Train loss 0.03532690984314425 micro_f1_score 0.9642087438423645 
 
time = 29.58 secondes

Val loss 0.2555764043917421 micro_f1_score 0.7752048450302815
 
----------
Epoch 14/40
time = 877.89 secondes

Train loss 0.03081960804668163 micro_f1_score 0.9689818195795058 
 
time = 31.82 secondes

Val loss 0.2641264575793118 micro_f1_score 0.7780530973451327
 
----------
Epoch 15/40
time = 879.38 secondes

Train loss 0.027849100955115915 micro_f1_score 0.9727052861914085 
 
time = 32.82 secondes

Val loss 0.2664614683780514 micro_f1_score 0.7868147617341453
 
----------
Epoch 16/40
time = 876.70 secondes

Train loss 0.024807513191423436 micro_f1_score 0.9760036740786101 
 
time = 31.86 secondes

Val loss 0.28565406664961673 micro_f1_score 0.7754816112084063
 
----------
Epoch 17/40
time = 880.16 secondes

Train loss 0.02039647864729485 micro_f1_score 0.9794232387363268 
 
time = 31.75 secondes

Val loss 0.28671950285063413 micro_f1_score 0.7861923212398731
 
----------
Epoch 18/40
time = 879.43 secondes

Train loss 0.019596208137856494 micro_f1_score 0.9806515754053228 
 
time = 31.07 secondes

Val loss 0.28335499641348105 micro_f1_score 0.7888848791050164
 
----------
Epoch 19/40
time = 877.22 secondes

Train loss 0.018483346778540747 micro_f1_score 0.9821708089947695 
 
time = 30.67 secondes

Val loss 0.3008203067984737 micro_f1_score 0.7826398852223816
 
----------
Epoch 20/40
time = 876.80 secondes

Train loss 0.01654667365862904 micro_f1_score 0.9838469469584146 
 
time = 33.37 secondes

Val loss 0.2993594731708042 micro_f1_score 0.7882479398065209
 
----------
Epoch 21/40
time = 878.96 secondes

Train loss 0.01499418314957471 micro_f1_score 0.9848103198229142 
 
time = 30.79 secondes

Val loss 0.3180460341152598 micro_f1_score 0.7811400422237861
 
----------
Epoch 22/40
time = 878.97 secondes

Train loss 0.013123985185646577 micro_f1_score 0.986877241168841 
 
time = 30.60 secondes

Val loss 0.3139876002659563 micro_f1_score 0.780752532561505
 
----------
Epoch 23/40
time = 876.94 secondes

Train loss 0.012761175603320537 micro_f1_score 0.986388073359515 
 
time = 31.03 secondes

Val loss 0.32392441957700446 micro_f1_score 0.7827348567283279
 
----------
Epoch 24/40
time = 876.88 secondes

Train loss 0.010229138940256005 micro_f1_score 0.9900560064007314 
 
time = 33.39 secondes

Val loss 0.31661946619631814 micro_f1_score 0.790316573556797
 
----------
Epoch 25/40
time = 878.01 secondes

Train loss 0.009712558877806447 micro_f1_score 0.9902956958556912 
 
time = 31.39 secondes

Val loss 0.34132199233672655 micro_f1_score 0.7761304670126019
 
----------
Epoch 26/40
time = 877.40 secondes

Train loss 0.009487748635499654 micro_f1_score 0.9908193973562912 
 
time = 31.18 secondes

Val loss 0.32988816696661905 micro_f1_score 0.7839233038348083
 
----------
Epoch 27/40
time = 880.28 secondes

Train loss 0.008975151044170808 micro_f1_score 0.9910534130277534 
 
time = 33.63 secondes

Val loss 0.3636014228228663 micro_f1_score 0.7735708982925019
 
----------
Epoch 28/40
time = 878.34 secondes

Train loss 0.007591702035383892 micro_f1_score 0.9923989054423836 
 
time = 31.62 secondes

Val loss 0.3582256331306989 micro_f1_score 0.7861866274797943
 
----------
Epoch 29/40
time = 877.11 secondes

Train loss 0.007531545859950842 micro_f1_score 0.9924400714204308 
 
time = 33.99 secondes

Val loss 0.3501547264759658 micro_f1_score 0.7888647866955893
 
----------
Epoch 30/40
time = 875.73 secondes

Train loss 0.006066443058420895 micro_f1_score 0.9939509225794179 
 
time = 33.03 secondes

Val loss 0.3531698990063589 micro_f1_score 0.7862567811934901
 
----------
Epoch 31/40
time = 877.13 secondes

Train loss 0.00624648920529908 micro_f1_score 0.9940251931346806 
 
time = 34.25 secondes

Val loss 0.3516444467374536 micro_f1_score 0.7862932940309506
 
----------
Epoch 32/40
time = 876.33 secondes

Train loss 0.004644903002649248 micro_f1_score 0.9952851711026617 
 
time = 31.12 secondes

Val loss 0.36580054107748095 micro_f1_score 0.7846924177396281
 
----------
Epoch 33/40
time = 877.56 secondes

Train loss 0.004538974980377961 micro_f1_score 0.9952866048350312 
 
time = 30.99 secondes

Val loss 0.36251382517521497 micro_f1_score 0.7889775199419868
 
----------
Epoch 34/40
time = 877.29 secondes

Train loss 0.004425227284442732 micro_f1_score 0.9955896889970344 
 
time = 31.46 secondes

Val loss 0.3755475645426844 micro_f1_score 0.7832422586520947
 
----------
Epoch 35/40
time = 878.54 secondes

Train loss 0.002869785135250847 micro_f1_score 0.9970369244795624 
 
time = 30.74 secondes

Val loss 0.36825150880412977 micro_f1_score 0.7968127490039841
 
----------
Epoch 36/40
time = 878.47 secondes

Train loss 0.002658630447603263 micro_f1_score 0.9973384030418251 
 
time = 37.25 secondes

Val loss 0.3676569012344861 micro_f1_score 0.7968470082407739
 
----------
Epoch 37/40
time = 877.62 secondes

Train loss 0.002798645164515259 micro_f1_score 0.9974914481185861 
 
time = 30.73 secondes

Val loss 0.3877834305167198 micro_f1_score 0.7852804573061807
 
----------
Epoch 38/40
time = 876.09 secondes

Train loss 0.0016882955681196748 micro_f1_score 0.9984425451092117 
 
time = 34.48 secondes

Val loss 0.3840518096675638 micro_f1_score 0.7898498927805575
 
----------
Epoch 39/40
time = 877.70 secondes

Train loss 0.00157487474596335 micro_f1_score 0.9987461529693377 
 
time = 34.28 secondes

Val loss 0.38403427124512 micro_f1_score 0.789665211062591
 
----------
Epoch 40/40
time = 878.94 secondes

Train loss 0.001245587506488297 micro_f1_score 0.9990121580547112 
 
time = 30.98 secondes

Val loss 0.3932526923838209 micro_f1_score 0.7917271407837446
 
----------
best_f1_socre 0.7968470082407739 best_epoch 36

average train time 874.7614018678665

average val time 31.565739154815674
 
time = 33.13 secondes

test_f1_score 0.7608391608391609

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_512_2
----------
Epoch 1/40
time = 1707.44 secondes

Train loss 0.22775330113263817 micro_f1_score 0.681545809813287 
 
time = 37.96 secondes

Val loss 0.19011288075173488 micro_f1_score 0.7198428290766208
 
----------
Epoch 2/40
time = 1708.40 secondes

Train loss 0.1533682675243498 micro_f1_score 0.7997406596968961 
 
time = 38.31 secondes

Val loss 0.17908229361303518 micro_f1_score 0.7561260210035007
 
----------
Epoch 3/40
time = 1708.93 secondes

Train loss 0.1275878915479323 micro_f1_score 0.8419414065618264 
 
time = 39.34 secondes

Val loss 0.16699423934104013 micro_f1_score 0.7757437070938216
 
----------
Epoch 4/40
time = 1707.35 secondes

Train loss 0.1120512650860054 micro_f1_score 0.8669310071371926 
 
time = 37.98 secondes

Val loss 0.17034544453757708 micro_f1_score 0.7874074074074074
 
----------
Epoch 5/40
time = 1706.61 secondes

Train loss 0.10185589490978568 micro_f1_score 0.880528883991815 
 
time = 36.38 secondes

Val loss 0.1763499097623786 micro_f1_score 0.7832116788321167
 
----------
Epoch 6/40
time = 1706.80 secondes

Train loss 0.09037993533352205 micro_f1_score 0.899482109227872 
 
time = 38.28 secondes

Val loss 0.180785707396562 micro_f1_score 0.7766497461928934
 
----------
Epoch 7/40
time = 1709.58 secondes

Train loss 0.08038442934116533 micro_f1_score 0.9101726094954794 
 
time = 36.95 secondes

Val loss 0.18831456073972044 micro_f1_score 0.791651673263764
 
----------
Epoch 8/40
time = 1706.25 secondes

Train loss 0.0714118233688914 micro_f1_score 0.9241008874357777 
 
time = 36.85 secondes

Val loss 0.19139016792178154 micro_f1_score 0.7931158121190391
 
----------
Epoch 9/40
time = 1705.78 secondes

Train loss 0.06203503064257471 micro_f1_score 0.9351192319603593 
 
time = 37.09 secondes

Val loss 0.2072946827431194 micro_f1_score 0.7810480976310121
 
----------
Epoch 10/40
time = 1706.08 secondes

Train loss 0.05394854877009853 micro_f1_score 0.944493800455792 
 
time = 36.83 secondes

Val loss 0.20543500243640336 micro_f1_score 0.7984274481772694
 
----------
Epoch 11/40
time = 1706.74 secondes

Train loss 0.04835435138125946 micro_f1_score 0.9510936537276649 
 
time = 36.84 secondes

Val loss 0.2285622321679944 micro_f1_score 0.7843275679491705
 
----------
Epoch 12/40
time = 1706.88 secondes

Train loss 0.04291725437668664 micro_f1_score 0.9575557009273867 
 
time = 36.79 secondes

Val loss 0.23905216053616804 micro_f1_score 0.7800917755030004
 
----------
Epoch 13/40
time = 1705.18 secondes

Train loss 0.038550552397731454 micro_f1_score 0.9608318890814559 
 
time = 37.02 secondes

Val loss 0.23539287169448664 micro_f1_score 0.7959405581732512
 
----------
Epoch 14/40
time = 1706.21 secondes

Train loss 0.034043970019011216 micro_f1_score 0.9672735653376354 
 
time = 37.41 secondes

Val loss 0.2496856870587732 micro_f1_score 0.7919746568109821
 
----------
Epoch 15/40
time = 1707.01 secondes

Train loss 0.03028154614117198 micro_f1_score 0.9704269111273062 
 
time = 36.89 secondes

Val loss 0.2703820826088796 micro_f1_score 0.783916083916084
 
----------
Epoch 16/40
time = 1706.97 secondes

Train loss 0.026403265828812108 micro_f1_score 0.974527652413371 
 
time = 36.77 secondes

Val loss 0.29072225570190147 micro_f1_score 0.7743792731198272
 
----------
Epoch 17/40
time = 1706.83 secondes

Train loss 0.023811273808750484 micro_f1_score 0.9754864409376437 
 
time = 36.78 secondes

Val loss 0.2741035722562524 micro_f1_score 0.7864112757499095
 
----------
Epoch 18/40
time = 1707.21 secondes

Train loss 0.021921895940154625 micro_f1_score 0.9781545982279255 
 
time = 36.85 secondes

Val loss 0.2758983373641968 micro_f1_score 0.7929799426934098
 
----------
Epoch 19/40
time = 1707.11 secondes

Train loss 0.019802010443946767 micro_f1_score 0.9799831919932768 
 
time = 37.52 secondes

Val loss 0.28602351358190914 micro_f1_score 0.7909806728704366
 
----------
Epoch 20/40
time = 1707.90 secondes

Train loss 0.01725597860971214 micro_f1_score 0.9830055375214817 
 
time = 37.32 secondes

Val loss 0.2848552195507972 micro_f1_score 0.7979651162790697
 
----------
Epoch 21/40
time = 1707.86 secondes

Train loss 0.015345385223983678 micro_f1_score 0.9844286695672085 
 
time = 36.77 secondes

Val loss 0.29414544144614796 micro_f1_score 0.7961666052340584
 
----------
Epoch 22/40
time = 1706.63 secondes

Train loss 0.013752606594966523 micro_f1_score 0.9857306371613888 
 
time = 35.63 secondes

Val loss 0.30231349841981636 micro_f1_score 0.7930910399424254
 
----------
Epoch 23/40
time = 1703.92 secondes

Train loss 0.013128681009612255 micro_f1_score 0.9863984455366327 
 
time = 35.52 secondes

Val loss 0.3139091055901324 micro_f1_score 0.7896440129449837
 
----------
Epoch 24/40
time = 1701.95 secondes

Train loss 0.010585597538869609 micro_f1_score 0.988985021153333 
 
time = 35.59 secondes

Val loss 0.32423298525028543 micro_f1_score 0.7904451682953312
 
----------
Epoch 25/40
time = 1702.67 secondes

Train loss 0.010541619426940006 micro_f1_score 0.9892661388550548 
 
time = 35.79 secondes

Val loss 0.33177988243396167 micro_f1_score 0.7883636363636363
 
----------
Epoch 26/40
time = 1702.20 secondes

Train loss 0.009784227154880493 micro_f1_score 0.990590834634871 
 
time = 35.58 secondes

Val loss 0.33420381108756925 micro_f1_score 0.7837837837837838
 
----------
Epoch 27/40
time = 1704.30 secondes

Train loss 0.009415281359588744 micro_f1_score 0.9908620164483705 
 
time = 35.63 secondes

Val loss 0.34221998028090744 micro_f1_score 0.7908473364318913
 
----------
Epoch 28/40
time = 1703.53 secondes

Train loss 0.008604867205989698 micro_f1_score 0.9917298677541064 
 
time = 35.81 secondes

Val loss 0.3418105407083621 micro_f1_score 0.7840290381125226
 
----------
Epoch 29/40
time = 1703.79 secondes

Train loss 0.00733285809225751 micro_f1_score 0.9923896499238964 
 
time = 36.03 secondes

Val loss 0.36478366824935693 micro_f1_score 0.7814327485380118
 
----------
Epoch 30/40
time = 1702.13 secondes

Train loss 0.006919996983852544 micro_f1_score 0.9928117749971475 
 
time = 36.49 secondes

Val loss 0.35300366543844097 micro_f1_score 0.7906306306306305
 
----------
Epoch 31/40
time = 1701.50 secondes

Train loss 0.0059321685184316655 micro_f1_score 0.9937250427837993 
 
time = 35.86 secondes

Val loss 0.366286606817949 micro_f1_score 0.785244704163623
 
----------
Epoch 32/40
time = 1702.12 secondes

Train loss 0.004933621762265338 micro_f1_score 0.9945238819592334 
 
time = 35.83 secondes

Val loss 0.37823236825280504 micro_f1_score 0.7802237459400937
 
----------
Epoch 33/40
time = 1702.71 secondes

Train loss 0.004904019510948208 micro_f1_score 0.9950570342205323 
 
time = 35.96 secondes

Val loss 0.359065194965386 micro_f1_score 0.7894736842105264
 
----------
Epoch 34/40
time = 1703.25 secondes

Train loss 0.004273539194557973 micro_f1_score 0.9960474308300394 
 
time = 36.03 secondes

Val loss 0.38300656081467377 micro_f1_score 0.7894546767786205
 
----------
Epoch 35/40
time = 1704.44 secondes

Train loss 0.003481355519132292 micro_f1_score 0.9966539923954372 
 
time = 36.03 secondes

Val loss 0.373415363677701 micro_f1_score 0.7866138898884492
 
----------
Epoch 36/40
time = 1704.62 secondes

Train loss 0.0029825579417242275 micro_f1_score 0.9974164133738601 
 
time = 35.57 secondes

Val loss 0.36626274761606437 micro_f1_score 0.7935251798561153
 
----------
Epoch 37/40
time = 1704.77 secondes

Train loss 0.0027574109152598654 micro_f1_score 0.997226970560304 
 
time = 35.70 secondes

Val loss 0.37137024238949917 micro_f1_score 0.7952415284787311
 
----------
Epoch 38/40
time = 1705.15 secondes

Train loss 0.0018442060467579952 micro_f1_score 0.9982140821522211 
 
time = 35.63 secondes

Val loss 0.3667045537931997 micro_f1_score 0.7976794778825237
 
----------
Epoch 39/40
time = 1704.82 secondes

Train loss 0.0020161502818400795 micro_f1_score 0.9980629723878612 
 
time = 35.56 secondes

Val loss 0.3736267546039136 micro_f1_score 0.7919034090909091
 
----------
Epoch 40/40
time = 1703.24 secondes

Train loss 0.0013791221572515694 micro_f1_score 0.9989364934670313 
 
time = 35.94 secondes

Val loss 0.37629041179526046 micro_f1_score 0.7956989247311829
 
----------
best_f1_socre 0.7984274481772694 best_epoch 10

average train time 1705.4209457576276

average val time 36.5772416472435
 
time = 37.98 secondes

test_f1_score 0.7899511514305653

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_256_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 768.00 MiB (GPU 1; 79.20 GiB total capacity; 72.05 GiB already allocated; 651.31 MiB free; 75.99 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_4096_512_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 1.51 GiB (GPU 1; 79.20 GiB total capacity; 72.41 GiB already allocated; 897.31 MiB free; 75.75 GiB reserved in total by PyTorch)
datasets imported
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_64_3
----------
Epoch 1/40
time = 538.52 secondes

Train loss 0.2645446994685912 micro_f1_score 0.6252789079931749 
 
time = 20.05 secondes

Val loss 0.21807634464052858 micro_f1_score 0.6693977877918885
 
----------
Epoch 2/40
time = 539.83 secondes

Train loss 0.1683363185742417 micro_f1_score 0.7732850201762332 
 
time = 19.75 secondes

Val loss 0.19657986427916854 micro_f1_score 0.728049728049728
 
----------
Epoch 3/40
time = 542.22 secondes

Train loss 0.14371423950536294 micro_f1_score 0.8118964537856042 
 
time = 19.74 secondes

Val loss 0.18694168707874956 micro_f1_score 0.7423613730667674
 
----------
Epoch 4/40
time = 542.03 secondes

Train loss 0.12455643119944913 micro_f1_score 0.8447499598006112 
 
time = 19.59 secondes

Val loss 0.19934284125195176 micro_f1_score 0.744288872512896
 
----------
Epoch 5/40
time = 541.17 secondes

Train loss 0.108563216468571 micro_f1_score 0.868913260219342 
 
time = 19.65 secondes

Val loss 0.19396121567878566 micro_f1_score 0.7574007220216606
 
----------
Epoch 6/40
time = 539.26 secondes

Train loss 0.09443836838577513 micro_f1_score 0.8902448688485617 
 
time = 19.64 secondes

Val loss 0.20559108281721833 micro_f1_score 0.7631866133139323
 
----------
Epoch 7/40
time = 542.66 secondes

Train loss 0.08149006231562109 micro_f1_score 0.9068083431719796 
 
time = 19.74 secondes

Val loss 0.22704233546726038 micro_f1_score 0.7574774774774775
 
----------
Epoch 8/40
time = 540.43 secondes

Train loss 0.07121761945319606 micro_f1_score 0.9211874682207533 
 
time = 19.84 secondes

Val loss 0.2418288458444056 micro_f1_score 0.7532188841201717
 
----------
Epoch 9/40
time = 538.95 secondes

Train loss 0.058094957663031584 micro_f1_score 0.9386503067484662 
 
time = 19.77 secondes

Val loss 0.253148720706584 micro_f1_score 0.7680284191829485
 
----------
Epoch 10/40
time = 540.26 secondes

Train loss 0.049031753534333665 micro_f1_score 0.94743773482589 
 
time = 19.95 secondes

Val loss 0.2667106934746758 micro_f1_score 0.7679083094555875
 
----------
Epoch 11/40
time = 541.30 secondes

Train loss 0.040744923664936546 micro_f1_score 0.9564179564179563 
 
time = 19.69 secondes

Val loss 0.2972197612045241 micro_f1_score 0.7562434048540274
 
----------
Epoch 12/40
time = 542.10 secondes

Train loss 0.0352199266584976 micro_f1_score 0.9620700245700246 
 
time = 19.65 secondes

Val loss 0.3165959726835861 micro_f1_score 0.7576403695806682
 
----------
Epoch 13/40
time = 541.43 secondes

Train loss 0.029764777078683413 micro_f1_score 0.9694460417864673 
 
time = 19.75 secondes

Val loss 0.32483951918414383 micro_f1_score 0.7611677805135421
 
----------
Epoch 14/40
time = 542.00 secondes

Train loss 0.026124763669475477 micro_f1_score 0.9740801715226464 
 
time = 19.62 secondes

Val loss 0.33898465420867574 micro_f1_score 0.7604426990360585
 
----------
Epoch 15/40
time = 542.14 secondes

Train loss 0.022118636845246058 micro_f1_score 0.9765394772092311 
 
time = 19.69 secondes

Val loss 0.36402298143652617 micro_f1_score 0.759928443649374
 
----------
Epoch 16/40
time = 542.13 secondes

Train loss 0.019008905761078134 micro_f1_score 0.9798539699529798 
 
time = 19.58 secondes

Val loss 0.38894807828254385 micro_f1_score 0.749483826565726
 
----------
Epoch 17/40
time = 540.25 secondes

Train loss 0.01648112109443802 micro_f1_score 0.9826245999085506 
 
time = 19.59 secondes

Val loss 0.38940498428266557 micro_f1_score 0.7609162491052254
 
----------
Epoch 18/40
time = 540.65 secondes

Train loss 0.01590142329771445 micro_f1_score 0.9842615937046376 
 
time = 20.02 secondes

Val loss 0.41105588110255414 micro_f1_score 0.7612722824187347
 
----------
Epoch 19/40
time = 540.97 secondes

Train loss 0.01297877447959927 micro_f1_score 0.9863984455366327 
 
time = 19.65 secondes

Val loss 0.4093846847287944 micro_f1_score 0.7709011943539631
 
----------
Epoch 20/40
time = 540.89 secondes

Train loss 0.013354783780337416 micro_f1_score 0.9863942985632075 
 
time = 19.70 secondes

Val loss 0.451233798607451 micro_f1_score 0.75445337059029
 
----------
Epoch 21/40
time = 541.21 secondes

Train loss 0.011820886863685562 micro_f1_score 0.9877821337494767 
 
time = 19.74 secondes

Val loss 0.4175581182124185 micro_f1_score 0.7660500544069642
 
----------
Epoch 22/40
time = 542.31 secondes

Train loss 0.011270659117708255 micro_f1_score 0.988734967270513 
 
time = 19.65 secondes

Val loss 0.4408093753163932 micro_f1_score 0.763026409707352
 
----------
Epoch 23/40
time = 540.49 secondes

Train loss 0.008861580991258567 micro_f1_score 0.9909855083488646 
 
time = 19.72 secondes

Val loss 0.4554064392310674 micro_f1_score 0.7585959885386819
 
----------
Epoch 24/40
time = 541.91 secondes

Train loss 0.008717373180017505 micro_f1_score 0.9907551835647708 
 
time = 19.71 secondes

Val loss 0.46735428908809284 micro_f1_score 0.7610175564313866
 
----------
Epoch 25/40
time = 540.88 secondes

Train loss 0.008442805072204587 micro_f1_score 0.9913202375513933 
 
time = 19.59 secondes

Val loss 0.47430877949370714 micro_f1_score 0.7600574712643678
 
----------
Epoch 26/40
time = 541.21 secondes

Train loss 0.009185855548777318 micro_f1_score 0.9913678366353575 
 
time = 20.20 secondes

Val loss 0.4765047984289341 micro_f1_score 0.7663344407530454
 
----------
Epoch 27/40
time = 541.39 secondes

Train loss 0.007202125899097122 micro_f1_score 0.9925075115049633 
 
time = 19.67 secondes

Val loss 0.482967738184284 micro_f1_score 0.7660500544069642
 
----------
Epoch 28/40
time = 542.70 secondes

Train loss 0.007092948606585704 micro_f1_score 0.9931496422590959 
 
time = 19.73 secondes

Val loss 0.5026010335957418 micro_f1_score 0.7611070648215585
 
----------
Epoch 29/40
time = 541.24 secondes

Train loss 0.005825635862915865 micro_f1_score 0.9940661848611639 
 
time = 19.77 secondes

Val loss 0.5058768512528451 micro_f1_score 0.7622803872355685
 
----------
Epoch 30/40
time = 541.75 secondes

Train loss 0.004740301816550404 micro_f1_score 0.9957052183497397 
 
time = 19.60 secondes

Val loss 0.5027550676318465 micro_f1_score 0.7634795111430626
 
----------
Epoch 31/40
time = 542.12 secondes

Train loss 0.003926469864539261 micro_f1_score 0.9961246200607903 
 
time = 19.72 secondes

Val loss 0.5177014514071042 micro_f1_score 0.7619047619047619
 
----------
Epoch 32/40
time = 542.04 secondes

Train loss 0.004442132712790169 micro_f1_score 0.9958184444613396 
 
time = 19.76 secondes

Val loss 0.5250131758998652 micro_f1_score 0.7620738636363636
 
----------
Epoch 33/40
time = 541.78 secondes

Train loss 0.0036518824551020367 micro_f1_score 0.9969219076572298 
 
time = 19.63 secondes

Val loss 0.5268841902496385 micro_f1_score 0.7591508052708638
 
----------
Epoch 34/40
time = 541.46 secondes

Train loss 0.004496566376557796 micro_f1_score 0.9960492326394165 
 
time = 20.18 secondes

Val loss 0.5340065455338994 micro_f1_score 0.7619394823186292
 
----------
Epoch 35/40
time = 543.27 secondes

Train loss 0.0029817398177935197 micro_f1_score 0.9977956825782913 
 
time = 19.76 secondes

Val loss 0.5499300629389091 micro_f1_score 0.7593850554165178
 
----------
Epoch 36/40
time = 543.82 secondes

Train loss 0.002182372836407897 micro_f1_score 0.9977212305355109 
 
time = 19.73 secondes

Val loss 0.5405793497796918 micro_f1_score 0.7611567297393786
 
----------
Epoch 37/40
Exception
CUDA out of memory. Tried to allocate 48.00 MiB (GPU 1; 79.20 GiB total capacity; 36.47 GiB already allocated; 46.31 MiB free; 36.57 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_1024_128_3
----------
Epoch 1/40
Attention type 'block_sparse' is not possible if sequence_length: 1024 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 1408 with config.block_size = 128, config.num_random_blocks = 3. Changing attention type to 'original_full'...
time = 458.88 secondes

Train loss 0.2567840816872614 micro_f1_score 0.6385217275982495 
 
time = 16.95 secondes

Val loss 0.2245809557985087 micro_f1_score 0.6481632653061223
 
----------
Epoch 2/40
time = 458.31 secondes

Train loss 0.16896172983420862 micro_f1_score 0.769895126465145 
 
time = 17.01 secondes

Val loss 0.19371901440327285 micro_f1_score 0.7301103920822231
 
----------
Epoch 3/40
time = 458.52 secondes

Train loss 0.14575650799717452 micro_f1_score 0.8115494103294022 
 
time = 17.00 secondes

Val loss 0.1873829925402266 micro_f1_score 0.7481259370314843
 
----------
Epoch 4/40
time = 457.77 secondes

Train loss 0.1272198222292302 micro_f1_score 0.8394545235126111 
 
time = 17.04 secondes

Val loss 0.1901981858689277 micro_f1_score 0.7510204081632653
 
----------
Epoch 5/40
time = 458.44 secondes

Train loss 0.11296128281434109 micro_f1_score 0.8609160548994574 
 
time = 17.13 secondes

Val loss 0.18984945228353875 micro_f1_score 0.7610226009633198
 
----------
Epoch 6/40
time = 458.09 secondes

Train loss 0.10061228985285706 micro_f1_score 0.8786908573461402 
 
time = 17.17 secondes

Val loss 0.2003627623446652 micro_f1_score 0.7527887729399064
 
----------
Epoch 7/40
time = 458.42 secondes

Train loss 0.087686680702845 micro_f1_score 0.8986271193108061 
 
time = 17.07 secondes

Val loss 0.20828176509650026 micro_f1_score 0.7550229115262601
 
----------
Epoch 8/40
time = 458.32 secondes

Train loss 0.07874206943051504 micro_f1_score 0.909276009240769 
 
time = 17.08 secondes

Val loss 0.23007006051599002 micro_f1_score 0.7576601671309192
 
----------
Epoch 9/40
time = 458.78 secondes

Train loss 0.0683259568273652 micro_f1_score 0.9219125979973609 
 
time = 17.84 secondes

Val loss 0.2563998308338103 micro_f1_score 0.7483870967741937
 
----------
Epoch 10/40
time = 458.23 secondes

Train loss 0.05986913718987961 micro_f1_score 0.9326525086070172 
 
time = 17.63 secondes

Val loss 0.2785747655835308 micro_f1_score 0.7414772727272728
 
----------
Epoch 11/40
time = 458.12 secondes

Train loss 0.05064805533846018 micro_f1_score 0.9455403419200124 
 
time = 17.50 secondes

Val loss 0.28616157271822945 micro_f1_score 0.7523709167544783
 
----------
Epoch 12/40
time = 459.03 secondes

Train loss 0.04342069927359688 micro_f1_score 0.9523076923076922 
 
time = 17.31 secondes

Val loss 0.3241760791813741 micro_f1_score 0.7381882770870337
 
----------
Epoch 13/40
time = 458.55 secondes

Train loss 0.03704650438420038 micro_f1_score 0.9603781561046847 
 
time = 18.47 secondes

Val loss 0.3584364617212874 micro_f1_score 0.7311304347826086
 
----------
Epoch 14/40
time = 457.97 secondes

Train loss 0.032544408023865004 micro_f1_score 0.9657234874626179 
 
time = 17.83 secondes

Val loss 0.3582024220071855 micro_f1_score 0.7356643356643355
 
----------
Epoch 15/40
time = 458.19 secondes

Train loss 0.027721921591400713 micro_f1_score 0.9704418408760241 
 
time = 17.79 secondes

Val loss 0.3647922531503146 micro_f1_score 0.7412831241283125
 
----------
Epoch 16/40
time = 458.30 secondes

Train loss 0.023624440236226864 micro_f1_score 0.9734296746568796 
 
time = 17.72 secondes

Val loss 0.3686754718178608 micro_f1_score 0.7519826964671954
 
----------
Epoch 17/40
time = 458.45 secondes

Train loss 0.022035850260064827 micro_f1_score 0.9769113149847095 
 
time = 17.82 secondes

Val loss 0.3962201752135011 micro_f1_score 0.7500898957209636
 
----------
Epoch 18/40
time = 458.66 secondes

Train loss 0.01959165417075866 micro_f1_score 0.9790337979759405 
 
time = 17.45 secondes

Val loss 0.4125624245307485 micro_f1_score 0.7482695810564662
 
----------
Epoch 19/40
time = 458.36 secondes

Train loss 0.01704081145184941 micro_f1_score 0.9818667684672647 
 
time = 17.83 secondes

Val loss 0.4094763282136839 micro_f1_score 0.7500898957209636
 
----------
Epoch 20/40
time = 458.28 secondes

Train loss 0.015895835849192196 micro_f1_score 0.9836716007935298 
 
time = 17.69 secondes

Val loss 0.42335039792490786 micro_f1_score 0.7537473233404711
 
----------
Epoch 21/40
time = 458.04 secondes

Train loss 0.013818019560773161 micro_f1_score 0.9854607899255867 
 
time = 17.74 secondes

Val loss 0.46196949115542113 micro_f1_score 0.7391455366446683
 
----------
Epoch 22/40
time = 458.41 secondes

Train loss 0.013646675788494338 micro_f1_score 0.985917637207886 
 
time = 18.12 secondes

Val loss 0.4505369646138832 micro_f1_score 0.7488584474885844
 
----------
Epoch 23/40
time = 458.07 secondes

Train loss 0.01208766880078107 micro_f1_score 0.9868260737130672 
 
time = 17.17 secondes

Val loss 0.459931132001955 micro_f1_score 0.7507930912936199
 
----------
Epoch 24/40
time = 458.03 secondes

Train loss 0.010828459504331034 micro_f1_score 0.9884208120667327 
 
time = 17.61 secondes

Val loss 0.48998516651450613 micro_f1_score 0.7463432037103104
 
----------
Epoch 25/40
time = 458.82 secondes

Train loss 0.009334452756494914 micro_f1_score 0.9902927404925959 
 
time = 17.56 secondes

Val loss 0.4912801518059168 micro_f1_score 0.7500912075884714
 
----------
Epoch 26/40
time = 458.01 secondes

Train loss 0.00957377663797013 micro_f1_score 0.9903370615536788 
 
time = 18.05 secondes

Val loss 0.4719065818630281 micro_f1_score 0.7567959405581732
 
----------
Epoch 27/40
time = 458.57 secondes

Train loss 0.00828577274860272 micro_f1_score 0.9913248611216803 
 
time = 17.64 secondes

Val loss 0.5027217116878658 micro_f1_score 0.7551537070524412
 
----------
Epoch 28/40
time = 458.23 secondes

Train loss 0.008390967147919058 micro_f1_score 0.9911860800850999 
 
time = 17.83 secondes

Val loss 0.4790531903749607 micro_f1_score 0.7646632874728458
 
----------
Epoch 29/40
time = 458.47 secondes

Train loss 0.006488247844987069 micro_f1_score 0.9935375959857067 
 
time = 17.76 secondes

Val loss 0.5221791225867193 micro_f1_score 0.7540751240255139
 
----------
Epoch 30/40
time = 459.28 secondes

Train loss 0.006013217763845475 micro_f1_score 0.9936533272526888 
 
time = 17.85 secondes

Val loss 0.5046686781722991 micro_f1_score 0.7578872740163063
 
----------
Epoch 31/40
time = 457.68 secondes

Train loss 0.005738748779251248 micro_f1_score 0.9948671153188092 
 
time = 17.01 secondes

Val loss 0.5277922993800679 micro_f1_score 0.7512653651482285
 
----------
Epoch 32/40
time = 458.26 secondes

Train loss 0.005086477733026628 micro_f1_score 0.9949446957314986 
 
time = 17.73 secondes

Val loss 0.5379531352979238 micro_f1_score 0.7573236889692585
 
----------
Epoch 33/40
time = 458.06 secondes

Train loss 0.00409678807567665 micro_f1_score 0.9962384589080132 
 
time = 17.22 secondes

Val loss 0.5498144230881675 micro_f1_score 0.7540628385698809
 
----------
Epoch 34/40
time = 458.33 secondes

Train loss 0.0035290361591840927 micro_f1_score 0.996085287522329 
 
time = 17.91 secondes

Val loss 0.548111238196248 micro_f1_score 0.7596460176991151
 
----------
Epoch 35/40
time = 458.34 secondes

Train loss 0.003291559354453909 micro_f1_score 0.9966178985369561 
 
time = 17.02 secondes

Val loss 0.5437652121801846 micro_f1_score 0.7632782272247626
 
----------
Epoch 36/40
time = 457.83 secondes

Train loss 0.002626026585007698 micro_f1_score 0.997492972726582 
 
time = 17.81 secondes

Val loss 0.5252822929474174 micro_f1_score 0.7654140824516601
 
----------
Epoch 37/40
time = 458.03 secondes

Train loss 0.0020478641473977125 micro_f1_score 0.9977961851204499 
 
time = 18.04 secondes

Val loss 0.5472620586391355 micro_f1_score 0.760662671836447
 
----------
Epoch 38/40
time = 458.02 secondes

Train loss 0.0018205496183859929 micro_f1_score 0.9981378026070764 
 
time = 17.85 secondes

Val loss 0.5443072387429534 micro_f1_score 0.7666905958363245
 
----------
Epoch 39/40
time = 458.14 secondes

Train loss 0.0011171477754270437 micro_f1_score 0.9989744368898849 
 
time = 17.02 secondes

Val loss 0.5588955798598586 micro_f1_score 0.769394261424017
 
----------
Epoch 40/40
time = 458.02 secondes

Train loss 0.0013597933539950555 micro_f1_score 0.9987466291921456 
 
time = 17.91 secondes

Val loss 0.5633743038920106 micro_f1_score 0.7653532126375577
 
----------
best_f1_socre 0.769394261424017 best_epoch 39

average train time 458.30705746412275

average val time 17.55489726662636
 
time = 18.90 secondes

test_f1_score 0.7521136286777139

----------
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_64_3
----------
Epoch 1/40
time = 1723.45 secondes

Train loss 0.24238248079478203 micro_f1_score 0.6686380240041747 
 
time = 30.61 secondes

Val loss 0.20479827239865162 micro_f1_score 0.6915887850467289
 
----------
Epoch 2/40
time = 1739.78 secondes

Train loss 0.1530774692291612 micro_f1_score 0.7988797337338149 
 
time = 30.22 secondes

Val loss 0.18384846275458572 micro_f1_score 0.7445086705202312
 
----------
Epoch 3/40
time = 1734.54 secondes

Train loss 0.1306423542568007 micro_f1_score 0.8351145038167939 
 
time = 30.11 secondes

Val loss 0.16807785813437134 micro_f1_score 0.7743879472693033
 
----------
Epoch 4/40
time = 1736.49 secondes

Train loss 0.11449129623067271 micro_f1_score 0.8607765987043441 
 
time = 30.11 secondes

Val loss 0.18420698353257337 micro_f1_score 0.7663003663003662
 
----------
Epoch 5/40
time = 1740.28 secondes

Train loss 0.09655834966347561 micro_f1_score 0.8853611341468229 
 
time = 31.31 secondes

Val loss 0.18550661584881487 micro_f1_score 0.7828571428571429
 
----------
Epoch 6/40
time = 1731.84 secondes

Train loss 0.08329606131893826 micro_f1_score 0.9067923046721633 
 
time = 31.24 secondes

Val loss 0.1943922548509035 micro_f1_score 0.7877697841726617
 
----------
Epoch 7/40
Exception
CUDA out of memory. Tried to allocate 384.00 MiB (GPU 1; 79.20 GiB total capacity; 73.85 GiB already allocated; 86.31 MiB free; 75.57 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_2048_128_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 1.12 GiB (GPU 1; 79.20 GiB total capacity; 73.60 GiB already allocated; 1.01 GiB free; 75.62 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_64_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 1; 79.20 GiB total capacity; 73.06 GiB already allocated; 133.31 MiB free; 76.49 GiB reserved in total by PyTorch)
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Bigbird_4096_128_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 2.62 GiB (GPU 1; 79.20 GiB total capacity; 72.99 GiB already allocated; 2.52 GiB free; 74.10 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_256_3
----------
Epoch 1/40
time = 536.04 secondes

Train loss 0.22944014991994377 micro_f1_score 0.6758732737611698 
 
time = 23.35 secondes

Val loss 0.20446506814389934 micro_f1_score 0.7061611374407584
 
----------
Epoch 2/40
time = 540.27 secondes

Train loss 0.1590426026096752 micro_f1_score 0.7840653728294177 
 
time = 22.89 secondes

Val loss 0.18190000719222865 micro_f1_score 0.7410889150019584
 
----------
Epoch 3/40
time = 543.24 secondes

Train loss 0.13752392684822684 micro_f1_score 0.824298160696999 
 
time = 23.62 secondes

Val loss 0.18266534829725983 micro_f1_score 0.7520976353928299
 
----------
Epoch 4/40
time = 536.84 secondes

Train loss 0.12131627888192197 micro_f1_score 0.8481817818138178 
 
time = 21.60 secondes

Val loss 0.19033011000175945 micro_f1_score 0.7392442947998504
 
----------
Epoch 5/40
time = 518.38 secondes

Train loss 0.10759907065143993 micro_f1_score 0.8707153354251898 
 
time = 21.75 secondes

Val loss 0.19266518816107608 micro_f1_score 0.7608616283315078
 
----------
Epoch 6/40
time = 514.48 secondes

Train loss 0.09474748752105075 micro_f1_score 0.8903281860905967 
 
time = 21.75 secondes

Val loss 0.19554085492110643 micro_f1_score 0.7675009067827349
 
----------
Epoch 7/40
time = 513.76 secondes

Train loss 0.08338645693619509 micro_f1_score 0.9057431638329546 
 
time = 21.62 secondes

Val loss 0.21600141205260012 micro_f1_score 0.7518796992481204
 
----------
Epoch 8/40
time = 515.70 secondes

Train loss 0.07239544676230834 micro_f1_score 0.9209807136722106 
 
time = 22.00 secondes

Val loss 0.2301105314590892 micro_f1_score 0.7554915376305366
 
----------
Epoch 9/40
time = 514.21 secondes

Train loss 0.06272157550546097 micro_f1_score 0.9333333333333332 
 
time = 21.56 secondes

Val loss 0.2557342461020243 micro_f1_score 0.7541995200548508
 
----------
Epoch 10/40
time = 517.32 secondes

Train loss 0.055549995542864675 micro_f1_score 0.9418464995544535 
 
time = 21.44 secondes

Val loss 0.2342261915079883 micro_f1_score 0.768562115175661
 
----------
Epoch 11/40
time = 513.82 secondes

Train loss 0.04843648599737601 micro_f1_score 0.9501180294880229 
 
time = 21.53 secondes

Val loss 0.24821690335625507 micro_f1_score 0.7606961566352428
 
----------
Epoch 12/40
time = 516.25 secondes

Train loss 0.041830625186615567 micro_f1_score 0.9575149527300791 
 
time = 21.51 secondes

Val loss 0.261160100581216 micro_f1_score 0.7627365356622998
 
----------
Epoch 13/40
time = 516.09 secondes

Train loss 0.03734646884909084 micro_f1_score 0.9626607129109247 
 
time = 21.59 secondes

Val loss 0.26416010134777085 micro_f1_score 0.7639391745112236
 
----------
Epoch 14/40
time = 516.08 secondes

Train loss 0.03193841107684683 micro_f1_score 0.9679132040627886 
 
time = 21.79 secondes

Val loss 0.2934687990145605 micro_f1_score 0.75421600287047
 
----------
Epoch 15/40
time = 517.25 secondes

Train loss 0.028540496835862662 micro_f1_score 0.9705036247171186 
 
time = 21.52 secondes

Val loss 0.28780947222572856 micro_f1_score 0.7665135994348286
 
----------
Epoch 16/40
time = 511.95 secondes

Train loss 0.026222198570633792 micro_f1_score 0.9741742662272971 
 
time = 21.58 secondes

Val loss 0.2924406285412976 micro_f1_score 0.7599271402550092
 
----------
Epoch 17/40
time = 514.05 secondes

Train loss 0.022186019016127732 micro_f1_score 0.9783847890125866 
 
time = 21.58 secondes

Val loss 0.3150977592487804 micro_f1_score 0.7518905293482174
 
----------
Epoch 18/40
time = 514.41 secondes

Train loss 0.020552597800794117 micro_f1_score 0.9790519877675841 
 
time = 21.59 secondes

Val loss 0.3253446113867838 micro_f1_score 0.7593070401769259
 
----------
Epoch 19/40
time = 515.67 secondes

Train loss 0.019390528242477122 micro_f1_score 0.9802362475629802 
 
time = 21.54 secondes

Val loss 0.3299242283721439 micro_f1_score 0.7610872675250357
 
----------
Epoch 20/40
time = 516.20 secondes

Train loss 0.016315062558026734 micro_f1_score 0.9831660113753483 
 
time = 22.03 secondes

Val loss 0.34739115895306477 micro_f1_score 0.7572274468826192
 
----------
Epoch 21/40
time = 509.86 secondes

Train loss 0.015119985667862804 micro_f1_score 0.9841584914303164 
 
time = 21.65 secondes

Val loss 0.32368876894966503 micro_f1_score 0.7627541919372101
 
----------
Epoch 22/40
time = 512.95 secondes

Train loss 0.013069496987553246 micro_f1_score 0.9872684302813143 
 
time = 21.76 secondes

Val loss 0.3470008672505129 micro_f1_score 0.7609791216702664
 
----------
Epoch 23/40
time = 512.57 secondes

Train loss 0.012367845051407998 micro_f1_score 0.9871041587180467 
 
time = 21.96 secondes

Val loss 0.37553654440113754 micro_f1_score 0.7493707299532543
 
----------
Epoch 24/40
time = 509.91 secondes

Train loss 0.011152681413312182 micro_f1_score 0.9882200449849415 
 
time = 21.81 secondes

Val loss 0.3643533577684496 micro_f1_score 0.7699496764917326
 
----------
Epoch 25/40
time = 508.94 secondes

Train loss 0.01042219050171437 micro_f1_score 0.9895305897133285 
 
time = 22.00 secondes

Val loss 0.3949886724108555 micro_f1_score 0.7527648947556189
 
----------
Epoch 26/40
time = 509.97 secondes

Train loss 0.008543555458419252 micro_f1_score 0.9915089669877775 
 
time = 21.63 secondes

Val loss 0.3711900322652254 micro_f1_score 0.7615918218327856
 
----------
Epoch 27/40
time = 509.09 secondes

Train loss 0.0088862099863593 micro_f1_score 0.9908396366262496 
 
time = 21.76 secondes

Val loss 0.39200237234596347 micro_f1_score 0.7489300998573467
 
----------
Epoch 28/40
time = 508.61 secondes

Train loss 0.007676852321349764 micro_f1_score 0.9924646064850053 
 
time = 21.49 secondes

Val loss 0.39367640482597666 micro_f1_score 0.757260666905701
 
----------
Epoch 29/40
time = 508.66 secondes

Train loss 0.006889088111413068 micro_f1_score 0.993075635367524 
 
time = 21.65 secondes

Val loss 0.38697735818683127 micro_f1_score 0.7614545454545455
 
----------
Epoch 30/40
time = 505.94 secondes

Train loss 0.00629801380696868 micro_f1_score 0.9936465664827848 
 
time = 21.40 secondes

Val loss 0.39545320353058516 micro_f1_score 0.7684021543985639
 
----------
Epoch 31/40
time = 508.32 secondes

Train loss 0.006288606265304413 micro_f1_score 0.9932717527654237 
 
time = 21.47 secondes

Val loss 0.40911734214083095 micro_f1_score 0.7621097954790097
 
----------
Epoch 32/40
time = 508.12 secondes

Train loss 0.005225601877772296 micro_f1_score 0.9949014534662508 
 
time = 21.49 secondes

Val loss 0.4142958916357306 micro_f1_score 0.7572254335260117
 
----------
Epoch 33/40
time = 507.36 secondes

Train loss 0.004112528616324935 micro_f1_score 0.9957055447877475 
 
time = 21.65 secondes

Val loss 0.4112973060519969 micro_f1_score 0.7654232424677189
 
----------
Epoch 34/40
time = 503.25 secondes

Train loss 0.005125826307525753 micro_f1_score 0.9949859454531641 
 
time = 21.51 secondes

Val loss 0.4147153396586903 micro_f1_score 0.7620427381383558
 
----------
Epoch 35/40
time = 507.80 secondes

Train loss 0.0032942276357160252 micro_f1_score 0.9963923593969545 
 
time = 21.37 secondes

Val loss 0.4198951059188999 micro_f1_score 0.7606557377049181
 
----------
Epoch 36/40
time = 506.14 secondes

Train loss 0.0024240435119773376 micro_f1_score 0.9978339350180506 
 
time = 21.44 secondes

Val loss 0.42643692149002044 micro_f1_score 0.760246644903881
 
----------
Epoch 37/40
time = 504.39 secondes

Train loss 0.002309133942401024 micro_f1_score 0.9978350867864333 
 
time = 21.53 secondes

Val loss 0.42231483452144214 micro_f1_score 0.7675981605942696
 
----------
Epoch 38/40
time = 507.73 secondes

Train loss 0.0022591463984713914 micro_f1_score 0.9980249164387724 
 
time = 21.50 secondes

Val loss 0.4243094678052136 micro_f1_score 0.7700035880875492
 
----------
Epoch 39/40
time = 502.45 secondes

Train loss 0.0017723449926625682 micro_f1_score 0.9985560115519077 
 
time = 21.36 secondes

Val loss 0.4166060677561604 micro_f1_score 0.7678442682047584
 
----------
Epoch 40/40
time = 506.45 secondes

Train loss 0.001270063847586825 micro_f1_score 0.9987459623788714 
 
time = 21.49 secondes

Val loss 0.4283793378071707 micro_f1_score 0.7664023071377073
 
----------
best_f1_socre 0.7700035880875492 best_epoch 38

average train time 514.0125499904156

average val time 21.743916231393815
 
time = 23.09 secondes

test_f1_score 0.7612408504705471

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_1024_512_3
----------
Epoch 1/40
time = 643.87 secondes

Train loss 0.22391148868042068 micro_f1_score 0.6942661439710507 
 
time = 25.24 secondes

Val loss 0.2008829620040831 micro_f1_score 0.7193737769080234
 
----------
Epoch 2/40
time = 643.29 secondes

Train loss 0.15937395751946146 micro_f1_score 0.7890906117108276 
 
time = 24.56 secondes

Val loss 0.18082785117821615 micro_f1_score 0.7389937106918238
 
----------
Epoch 3/40
time = 640.55 secondes

Train loss 0.13714344205653614 micro_f1_score 0.8240961904376342 
 
time = 24.52 secondes

Val loss 0.18998622637791712 micro_f1_score 0.7391471379177872
 
----------
Epoch 4/40
time = 641.38 secondes

Train loss 0.12337626947408861 micro_f1_score 0.8475693049417437 
 
time = 24.46 secondes

Val loss 0.18975087047600356 micro_f1_score 0.7525347352609838
 
----------
Epoch 5/40
time = 639.57 secondes

Train loss 0.11110356063456149 micro_f1_score 0.865730023149996 
 
time = 24.42 secondes

Val loss 0.1946149649190121 micro_f1_score 0.7473997028231797
 
----------
Epoch 6/40
time = 642.47 secondes

Train loss 0.09855386757434488 micro_f1_score 0.8848812266624295 
 
time = 24.49 secondes

Val loss 0.19473635808366244 micro_f1_score 0.7661950856291884
 
----------
Epoch 7/40
time = 639.83 secondes

Train loss 0.08700152531115188 micro_f1_score 0.9000670162021523 
 
time = 24.40 secondes

Val loss 0.2048591272752793 micro_f1_score 0.7634525099313831
 
----------
Epoch 8/40
time = 638.08 secondes

Train loss 0.07676855012405294 micro_f1_score 0.9138161516080836 
 
time = 24.47 secondes

Val loss 0.21748138866463645 micro_f1_score 0.7657430730478588
 
----------
Epoch 9/40
time = 647.78 secondes

Train loss 0.06739579731481032 micro_f1_score 0.9277230817315205 
 
time = 24.95 secondes

Val loss 0.23887447560908365 micro_f1_score 0.7560801144492132
 
----------
Epoch 10/40
time = 665.23 secondes

Train loss 0.0591332204392398 micro_f1_score 0.9362399095269665 
 
time = 24.99 secondes

Val loss 0.2455879493815 micro_f1_score 0.7618379515959313
 
----------
Epoch 11/40
time = 669.03 secondes

Train loss 0.050794149902942884 micro_f1_score 0.946618114201596 
 
time = 24.84 secondes

Val loss 0.25629827954241485 micro_f1_score 0.7640769779044904
 
----------
Epoch 12/40
time = 674.21 secondes

Train loss 0.04426808233901455 micro_f1_score 0.9543382523371707 
 
time = 24.87 secondes

Val loss 0.2826064922770516 micro_f1_score 0.7523264137437365
 
----------
Epoch 13/40
time = 681.27 secondes

Train loss 0.03813280013247251 micro_f1_score 0.9612050699233347 
 
time = 25.07 secondes

Val loss 0.2832422688847683 micro_f1_score 0.757163323782235
 
----------
Epoch 14/40
time = 683.16 secondes

Train loss 0.035176773486147124 micro_f1_score 0.9638925244437602 
 
time = 25.09 secondes

Val loss 0.28763210712397685 micro_f1_score 0.7608232789212207
 
----------
Epoch 15/40
time = 681.27 secondes

Train loss 0.029337925957473886 micro_f1_score 0.9691315365123243 
 
time = 24.87 secondes

Val loss 0.29543512192417365 micro_f1_score 0.7682672233820459
 
----------
Epoch 16/40
time = 683.98 secondes

Train loss 0.02736216329933807 micro_f1_score 0.9717501148369315 
 
time = 24.92 secondes

Val loss 0.30041670640472506 micro_f1_score 0.7633642195295794
 
----------
Epoch 17/40
time = 684.59 secondes

Train loss 0.023990190489247006 micro_f1_score 0.9763628701681799 
 
time = 25.16 secondes

Val loss 0.31584034579210596 micro_f1_score 0.7531440891124686
 
----------
Epoch 18/40
time = 680.80 secondes

Train loss 0.0207594967896102 micro_f1_score 0.9782725116670493 
 
time = 25.20 secondes

Val loss 0.31971692489307435 micro_f1_score 0.7586933614330876
 
----------
Epoch 19/40
time = 683.19 secondes

Train loss 0.018638641008717084 micro_f1_score 0.9806742948987792 
 
time = 24.93 secondes

Val loss 0.33041467884036363 micro_f1_score 0.7583749109052031
 
----------
Epoch 20/40
time = 680.65 secondes

Train loss 0.016478623542934657 micro_f1_score 0.9830819171281269 
 
time = 24.93 secondes

Val loss 0.32064156258692506 micro_f1_score 0.7659574468085107
 
----------
Epoch 21/40
time = 680.69 secondes

Train loss 0.015959645402754095 micro_f1_score 0.9838506471194595 
 
time = 25.23 secondes

Val loss 0.32526739674513455 micro_f1_score 0.7680057908070937
 
----------
Epoch 22/40
time = 684.57 secondes

Train loss 0.015267922562758883 micro_f1_score 0.984162118841354 
 
time = 24.81 secondes

Val loss 0.33750538857745344 micro_f1_score 0.769116555308092
 
----------
Epoch 23/40
time = 680.15 secondes

Train loss 0.0138130071149774 micro_f1_score 0.9861779305078274 
 
time = 24.98 secondes

Val loss 0.3483300322636229 micro_f1_score 0.764386536373507
 
----------
Epoch 24/40
time = 680.63 secondes

Train loss 0.01088281294707469 micro_f1_score 0.9887906054598139 
 
time = 24.78 secondes

Val loss 0.357469773194829 micro_f1_score 0.7575539568345323
 
----------
Epoch 25/40
time = 682.03 secondes

Train loss 0.010162428312888789 micro_f1_score 0.9895650849264986 
 
time = 24.90 secondes

Val loss 0.38478513929199 micro_f1_score 0.7494631352899068
 
----------
Epoch 26/40
time = 679.30 secondes

Train loss 0.010374758431980562 micro_f1_score 0.989684442921853 
 
time = 24.98 secondes

Val loss 0.3783472863132836 micro_f1_score 0.7666900913562895
 
----------
Epoch 27/40
time = 680.60 secondes

Train loss 0.00907716258302612 micro_f1_score 0.9901804064854991 
 
time = 24.86 secondes

Val loss 0.3960912865204889 micro_f1_score 0.760662671836447
 
----------
Epoch 28/40
time = 681.26 secondes

Train loss 0.007096201228869341 micro_f1_score 0.9924983816305549 
 
time = 24.92 secondes

Val loss 0.39534329317632266 micro_f1_score 0.7592926741248647
 
----------
Epoch 29/40
time = 682.44 secondes

Train loss 0.006915606240815848 micro_f1_score 0.9932737982139465 
 
time = 24.74 secondes

Val loss 0.3870402728436423 micro_f1_score 0.7644444444444444
 
----------
Epoch 30/40
time = 677.28 secondes

Train loss 0.006144811838512888 micro_f1_score 0.9935047669692711 
 
time = 24.98 secondes

Val loss 0.4036832992903522 micro_f1_score 0.7592926741248647
 
----------
Epoch 31/40
time = 683.94 secondes

Train loss 0.006480770633487253 micro_f1_score 0.9935326789926195 
 
time = 24.83 secondes

Val loss 0.42526248390557336 micro_f1_score 0.7540511343176088
 
----------
Epoch 32/40
time = 683.15 secondes

Train loss 0.0053741650408220916 micro_f1_score 0.9947110079525131 
 
time = 25.40 secondes

Val loss 0.42628484891086327 micro_f1_score 0.7587687902648532
 
----------
Epoch 33/40
time = 684.62 secondes

Train loss 0.0042068023023275275 micro_f1_score 0.995816536091884 
 
time = 25.23 secondes

Val loss 0.42524108339528566 micro_f1_score 0.7625951431678144
 
----------
Epoch 34/40
time = 680.46 secondes

Train loss 0.003928571390127029 micro_f1_score 0.9961604257745675 
 
time = 24.88 secondes

Val loss 0.42452666016875723 micro_f1_score 0.7583454281567489
 
----------
Epoch 35/40
time = 685.64 secondes

Train loss 0.0034543592614034685 micro_f1_score 0.9966550098829254 
 
time = 25.41 secondes

Val loss 0.43006070837622784 micro_f1_score 0.7676912080057184
 
----------
Epoch 36/40
time = 684.76 secondes

Train loss 0.0035740326829532556 micro_f1_score 0.9965813264453391 
 
time = 24.86 secondes

Val loss 0.4129626489565021 micro_f1_score 0.7680057908070937
 
----------
Epoch 37/40
time = 679.06 secondes

Train loss 0.002862097998392225 micro_f1_score 0.9973400212798298 
 
time = 24.88 secondes

Val loss 0.41762635795796504 micro_f1_score 0.7696750902527075
 
----------
Epoch 38/40
time = 684.47 secondes

Train loss 0.002029934922029497 micro_f1_score 0.9981007369140774 
 
time = 24.75 secondes

Val loss 0.4397442003742593 micro_f1_score 0.7629446988376188
 
----------
Epoch 39/40
time = 682.93 secondes

Train loss 0.0018083608196419034 micro_f1_score 0.9984040127678978 
 
time = 25.01 secondes

Val loss 0.42191102880923476 micro_f1_score 0.767902123065851
 
----------
Epoch 40/40
time = 683.33 secondes

Train loss 0.001423411694456779 micro_f1_score 0.9988602689765216 
 
time = 24.94 secondes

Val loss 0.44011821292462894 micro_f1_score 0.7619047619047619
 
----------
best_f1_socre 0.7696750902527075 best_epoch 37

average train time 672.1382075548172

average val time 24.89387500882149
 
time = 27.38 secondes

test_f1_score 0.7538787023977432

----------
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_Longformer_2048_256_3
----------
Epoch 1/40
time = 910.32 secondes

Train loss 0.22219659399476138 micro_f1_score 0.6986143679386059 
 
time = 30.32 secondes

Val loss 0.19023853845772196 micro_f1_score 0.7328901055924911
 
----------
Epoch 2/40
time = 906.87 secondes

Train loss 0.15333346542944243 micro_f1_score 0.7989786820134553 
 
time = 29.43 secondes

Val loss 0.16687907620531614 micro_f1_score 0.7675758739915481
 
----------
Epoch 3/40
time = 904.62 secondes

Train loss 0.12960268801732644 micro_f1_score 0.834727614778049 
 
time = 29.60 secondes

Val loss 0.17398052772537606 micro_f1_score 0.7715156130997715
 
----------
Epoch 4/40
time = 905.66 secondes

Train loss 0.11530928660419074 micro_f1_score 0.861694539113894 
 
time = 29.59 secondes

Val loss 0.16472775171526144 micro_f1_score 0.786607799852833
 
----------
Epoch 5/40
time = 908.06 secondes

Train loss 0.10162715531449329 micro_f1_score 0.8822532991924366 
 
time = 29.75 secondes

Val loss 0.17107505497873807 micro_f1_score 0.7868131868131868
 
----------
Epoch 6/40
time = 904.96 secondes

Train loss 0.09079020748602915 micro_f1_score 0.8961049159729857 
 
time = 29.64 secondes

Val loss 0.17580614751968227 micro_f1_score 0.7810781078107811
 
----------
Epoch 7/40
time = 904.79 secondes

Train loss 0.07928332410551406 micro_f1_score 0.9124218749999999 
 
time = 29.69 secondes

Val loss 0.18770053433101686 micro_f1_score 0.7849111352919842
 
----------
Epoch 8/40
time = 903.99 secondes

Train loss 0.06951195464326858 micro_f1_score 0.9247286731240518 
 
time = 30.37 secondes

Val loss 0.1974811076385076 micro_f1_score 0.7840789010214864
 
----------
Epoch 9/40
time = 906.74 secondes

Train loss 0.06111407910079301 micro_f1_score 0.9339487259046658 
 
time = 29.72 secondes

Val loss 0.2063321735038132 micro_f1_score 0.7897727272727273
 
----------
Epoch 10/40
time = 906.30 secondes

Train loss 0.053749504489908076 micro_f1_score 0.9447282861124013 
 
time = 29.71 secondes

Val loss 0.22025913616917173 micro_f1_score 0.7813057438458795
 
----------
Epoch 11/40
time = 906.27 secondes

Train loss 0.04714396712244363 micro_f1_score 0.95155749411356 
 
time = 29.81 secondes

Val loss 0.22338261895003866 micro_f1_score 0.7805742644452321
 
----------
Epoch 12/40
time = 907.81 secondes

Train loss 0.0406076539630859 micro_f1_score 0.9591112482641567 
 
time = 29.73 secondes

Val loss 0.2321673615179101 micro_f1_score 0.7811831789023521
 
----------
Epoch 13/40
time = 906.61 secondes

Train loss 0.03731447186694388 micro_f1_score 0.9618908307029024 
 
time = 29.56 secondes

Val loss 0.25875979063452265 micro_f1_score 0.7813471502590673
 
----------
Epoch 14/40
time = 908.86 secondes

Train loss 0.031870685183434735 micro_f1_score 0.9676849183477425 
 
time = 29.86 secondes

Val loss 0.2557211189118565 micro_f1_score 0.7886235955056179
 
----------
Epoch 15/40
time = 908.65 secondes

Train loss 0.02941363671005846 micro_f1_score 0.9695058642568737 
 
time = 29.92 secondes

Val loss 0.2702222021571437 micro_f1_score 0.7822160472386245
 
----------
Epoch 16/40
time = 909.64 secondes

Train loss 0.02548067254079338 micro_f1_score 0.9747905588921617 
 
time = 30.21 secondes

Val loss 0.2751024102822679 micro_f1_score 0.781665500349895
 
----------
Epoch 17/40
time = 908.04 secondes

Train loss 0.022835414415552608 micro_f1_score 0.9770735254717343 
 
time = 29.67 secondes

Val loss 0.2835524399138865 micro_f1_score 0.7842727905358386
 
----------
Epoch 18/40
time = 905.66 secondes

Train loss 0.020540404645516326 micro_f1_score 0.9793420045906657 
 
time = 29.72 secondes

Val loss 0.30215407321687604 micro_f1_score 0.7775438596491229
 
----------
Epoch 19/40
time = 905.04 secondes

Train loss 0.019096978572312625 micro_f1_score 0.9810095143479424 
 
time = 29.54 secondes

Val loss 0.3186041766502818 micro_f1_score 0.7745786516853933
 
----------
Epoch 20/40
time = 901.83 secondes

Train loss 0.016314753754901372 micro_f1_score 0.983686723973257 
 
time = 29.41 secondes

Val loss 0.30968268020231215 micro_f1_score 0.7875574407917993
 
----------
Epoch 21/40
time = 901.38 secondes

Train loss 0.01502346935449168 micro_f1_score 0.9850221610881859 
 
time = 29.35 secondes

Val loss 0.3095683192864793 micro_f1_score 0.7809048806555041
 
----------
Epoch 22/40
time = 903.40 secondes

Train loss 0.01410332005712212 micro_f1_score 0.9857562912895712 
 
time = 29.68 secondes

Val loss 0.31881049776174986 micro_f1_score 0.7840508115737473
 
----------
Epoch 23/40
time = 903.04 secondes

Train loss 0.012982875608098651 micro_f1_score 0.9873080001524565 
 
time = 29.41 secondes

Val loss 0.32038955695805005 micro_f1_score 0.7864285714285714
 
----------
Epoch 24/40
time = 909.21 secondes

Train loss 0.011772851444395922 micro_f1_score 0.9883384146341463 
 
time = 29.78 secondes

Val loss 0.3300546460220071 micro_f1_score 0.7865008880994672
 
----------
Epoch 25/40
time = 902.51 secondes

Train loss 0.011293341126528921 micro_f1_score 0.9879453727016096 
 
time = 29.70 secondes

Val loss 0.3389621824026108 micro_f1_score 0.7902197023387667
 
----------
Epoch 26/40
