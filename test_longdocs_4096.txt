[nltk_data] Downloading package punkt to /vol/fob-
[nltk_data]     vol3/nebenf20/wubingti/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/usr/lib64/python3.6/site-packages/h5py/__init__.py:39: UserWarning: h5py is running against HDF5 1.10.8 when it was built against 1.10.7, this may cause problems
  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)
datasets imported
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_256_25_1
 
time = 122.66 secondes

test_accuracy 0.8667240142822266 macro_avg {'precision': 0.8693085977534614, 'recall': 0.861587956716648, 'f1-score': 0.8617875052042834, 'support': 2326} weighted_avg {'precision': 0.8733254242038059, 'recall': 0.8667239896818573, 'f1-score': 0.8659922488600186, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_256_50_1
 
time = 138.15 secondes

test_accuracy 0.8701633810997009 macro_avg {'precision': 0.8707106367704588, 'recall': 0.8656484622449654, 'f1-score': 0.8656669635773998, 'support': 2326} weighted_avg {'precision': 0.8755633366125626, 'recall': 0.8701633705932932, 'f1-score': 0.8700047271828678, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_512_25_1
 
time = 145.73 secondes

test_accuracy 0.8903697729110718 macro_avg {'precision': 0.891210464879903, 'recall': 0.8876931101321055, 'f1-score': 0.8866175063346405, 'support': 2326} weighted_avg {'precision': 0.8962436822107039, 'recall': 0.8903697334479793, 'f1-score': 0.8899793972829253, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_512_50_1
 
time = 157.40 secondes

test_accuracy 0.8817713260650635 macro_avg {'precision': 0.882591141942429, 'recall': 0.877440939846954, 'f1-score': 0.8774484194953047, 'support': 2326} weighted_avg {'precision': 0.8849593715173025, 'recall': 0.8817712811693895, 'f1-score': 0.880553447986379, 'support': 2326}

datasets imported
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_ToBERT_256_25_1
 
time = 2.41 secondes

test_accuracy 0.9210526347160339 macro_avg {'precision': 0.9194444444444444, 'recall': 0.9229691876750701, 'f1-score': 0.9205574912891985, 'support': 38} weighted_avg {'precision': 0.9226608187134503, 'recall': 0.9210526315789473, 'f1-score': 0.9212176783421969, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_ToBERT_256_50_1
 
time = 2.58 secondes

test_accuracy 0.8421052694320679 macro_avg {'precision': 0.8695652173913043, 'recall': 0.8571428571428572, 'f1-score': 0.8416666666666666, 'support': 38} weighted_avg {'precision': 0.8832951945080091, 'recall': 0.8421052631578947, 'f1-score': 0.8407894736842105, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_ToBERT_512_25_1
 
time = 2.68 secondes

test_accuracy 0.8421052694320679 macro_avg {'precision': 0.8515406162464986, 'recall': 0.8515406162464986, 'f1-score': 0.8421052631578947, 'support': 38} weighted_avg {'precision': 0.8609759693351023, 'recall': 0.8421052631578947, 'f1-score': 0.8421052631578947, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_ToBERT_512_50_1
 
time = 1.67 secondes

test_accuracy 0.9473684430122375 macro_avg {'precision': 0.9467787114845938, 'recall': 0.9467787114845938, 'f1-score': 0.9467787114845938, 'support': 38} weighted_avg {'precision': 0.9473684210526315, 'recall': 0.9473684210526315, 'f1-score': 0.9473684210526315, 'support': 38}

datasets imported
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_256_25_2
 
time = 109.18 secondes

test_accuracy 0.8822012543678284 macro_avg {'precision': 0.8816225075431605, 'recall': 0.8776826108066406, 'f1-score': 0.8775445552845389, 'support': 2326} weighted_avg {'precision': 0.8838992502099481, 'recall': 0.882201203783319, 'f1-score': 0.8805394476806142, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_256_50_2
 
time = 134.88 secondes

test_accuracy 0.8667240142822266 macro_avg {'precision': 0.8646939320089437, 'recall': 0.861290945139032, 'f1-score': 0.8604385918632623, 'support': 2326} weighted_avg {'precision': 0.8703708215205558, 'recall': 0.8667239896818573, 'f1-score': 0.8656312337139519, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_512_25_2
 
time = 146.34 secondes

test_accuracy 0.889509916305542 macro_avg {'precision': 0.8903845641053106, 'recall': 0.8859290505256109, 'f1-score': 0.8859901742747871, 'support': 2326} weighted_avg {'precision': 0.8920705221622124, 'recall': 0.8895098882201203, 'f1-score': 0.8883401091341502, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_512_50_2
 
time = 146.89 secondes

test_accuracy 0.8740326762199402 macro_avg {'precision': 0.8764683179768612, 'recall': 0.8718895200735739, 'f1-score': 0.8718562207236987, 'support': 2326} weighted_avg {'precision': 0.8778856033515064, 'recall': 0.8740326741186586, 'f1-score': 0.8734542910470221, 'support': 2326}

datasets imported
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_ToBERT_256_25_2
 
time = 2.41 secondes

test_accuracy 0.9210526347160339 macro_avg {'precision': 0.9194444444444444, 'recall': 0.9229691876750701, 'f1-score': 0.9205574912891985, 'support': 38} weighted_avg {'precision': 0.9226608187134503, 'recall': 0.9210526315789473, 'f1-score': 0.9212176783421969, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_ToBERT_256_50_2
 
time = 2.60 secondes

test_accuracy 0.8947368264198303 macro_avg {'precision': 0.9047619047619048, 'recall': 0.9047619047619048, 'f1-score': 0.8947368421052632, 'support': 38} weighted_avg {'precision': 0.9147869674185463, 'recall': 0.8947368421052632, 'f1-score': 0.8947368421052632, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_ToBERT_512_25_2
 
time = 2.68 secondes

test_accuracy 0.9736841917037964 macro_avg {'precision': 0.9722222222222222, 'recall': 0.9761904761904762, 'f1-score': 0.9735191637630662, 'support': 38} weighted_avg {'precision': 0.9751461988304094, 'recall': 0.9736842105263158, 'f1-score': 0.9737392261140657, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_ToBERT_512_50_2
 
time = 2.74 secondes

test_accuracy 0.9473684430122375 macro_avg {'precision': 0.9467787114845938, 'recall': 0.9467787114845938, 'f1-score': 0.9467787114845938, 'support': 38} weighted_avg {'precision': 0.9473684210526315, 'recall': 0.9473684210526315, 'f1-score': 0.9473684210526315, 'support': 38}

datasets imported
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_256_25_3
 
time = 126.54 secondes

test_accuracy 0.8757523894309998 macro_avg {'precision': 0.8768881286583182, 'recall': 0.8714173331413873, 'f1-score': 0.8710346232699745, 'support': 2326} weighted_avg {'precision': 0.8796611089796906, 'recall': 0.8757523645743767, 'f1-score': 0.8738950085271139, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_256_50_3
 
time = 127.39 secondes

test_accuracy 0.860275149345398 macro_avg {'precision': 0.8649147576354878, 'recall': 0.8564752619791935, 'f1-score': 0.8572613867597021, 'support': 2326} weighted_avg {'precision': 0.8692382305996258, 'recall': 0.8602751504729149, 'f1-score': 0.8609190884463948, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_512_25_3
 
time = 152.33 secondes

test_accuracy 0.8865004777908325 macro_avg {'precision': 0.8878768649944476, 'recall': 0.8822624138202464, 'f1-score': 0.8824068374505835, 'support': 2326} weighted_avg {'precision': 0.8899624506510281, 'recall': 0.8865004299226139, 'f1-score': 0.8851041482528065, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_512_50_3
 
time = 119.10 secondes

test_accuracy 0.8826311826705933 macro_avg {'precision': 0.8824940479508905, 'recall': 0.8768971240401582, 'f1-score': 0.8773849925577105, 'support': 2326} weighted_avg {'precision': 0.886354334689773, 'recall': 0.8826311263972485, 'f1-score': 0.8819607338576478, 'support': 2326}

datasets imported
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_ToBERT_256_25_3
 
time = 1.42 secondes

test_accuracy 0.8947368264198303 macro_avg {'precision': 0.9047619047619048, 'recall': 0.9047619047619048, 'f1-score': 0.8947368421052632, 'support': 38} weighted_avg {'precision': 0.9147869674185463, 'recall': 0.8947368421052632, 'f1-score': 0.8947368421052632, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_ToBERT_256_50_3
 
time = 1.52 secondes

test_accuracy 0.8157894611358643 macro_avg {'precision': 0.8323863636363636, 'recall': 0.8277310924369747, 'f1-score': 0.8156618156618156, 'support': 38} weighted_avg {'precision': 0.8434509569377989, 'recall': 0.8157894736842105, 'f1-score': 0.8151511835722363, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_ToBERT_512_25_3
 
time = 1.56 secondes

test_accuracy 0.9736841917037964 macro_avg {'precision': 0.9722222222222222, 'recall': 0.9761904761904762, 'f1-score': 0.9735191637630662, 'support': 38} weighted_avg {'precision': 0.9751461988304094, 'recall': 0.9736842105263158, 'f1-score': 0.9737392261140657, 'support': 38}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_ToBERT_512_50_3
 
time = 1.60 secondes

test_accuracy 0.9210526347160339 macro_avg {'precision': 0.9194444444444444, 'recall': 0.9229691876750701, 'f1-score': 0.9205574912891985, 'support': 38} weighted_avg {'precision': 0.9226608187134503, 'recall': 0.9210526315789473, 'f1-score': 0.9212176783421969, 'support': 38}

datasets imported
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_256_25_4
 
time = 75.44 secondes

test_accuracy 0.8714531660079956 macro_avg {'precision': 0.87106853617057, 'recall': 0.8665758782733347, 'f1-score': 0.8658201224394191, 'support': 2326} weighted_avg {'precision': 0.8739138744446984, 'recall': 0.8714531384350817, 'f1-score': 0.8690794249625463, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_256_50_4
 
time = 79.83 secondes

test_accuracy 0.8710232377052307 macro_avg {'precision': 0.8727499360676753, 'recall': 0.8679720062462284, 'f1-score': 0.8680671793853134, 'support': 2326} weighted_avg {'precision': 0.8746447069098134, 'recall': 0.8710232158211522, 'f1-score': 0.870235135699434, 'support': 2326}

Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_ToBERT_512_25_4
