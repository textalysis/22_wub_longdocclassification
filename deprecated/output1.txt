[nltk_data] Downloading package punkt to /vol/fob-
[nltk_data]     vol3/nebenf20/wubingti/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (11). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
20newsgroups_BERT_head_bert_summarizer_1
----------
Epoch 1/40
time = 244.95 secondes

Train loss 1.2754744588860911 accuracy 0.6783890128135681 macro_avg {'precision': 0.701831888648878, 'recall': 0.6631310265176371, 'f1-score': 0.6556714988901758, 'support': 10180} weighted_avg {'precision': 0.7021710379191725, 'recall': 0.6783889980353635, 'f1-score': 0.6694774040631106, 'support': 10180}
 
time = 5.44 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss 0.6540595223786125 accuracy 0.8196286559104919 macro_avg {'precision': 0.7949882564702239, 'recall': 0.8123239378445957, 'f1-score': 0.7975679977204734, 'support': 1131} weighted_avg {'precision': 0.8063637400211182, 'recall': 0.8196286472148541, 'f1-score': 0.8070193178037786, 'support': 1131}
 
----------
Epoch 2/40
time = 241.68 secondes

Train loss 0.4649639995812528 accuracy 0.8643418550491333 macro_avg {'precision': 0.8558183058680966, 'recall': 0.8533910854778937, 'f1-score': 0.8521519436788239, 'support': 10180} weighted_avg {'precision': 0.8625143041698158, 'recall': 0.8643418467583497, 'f1-score': 0.8615607348951108, 'support': 10180}
 
time = 5.25 secondes

Val loss 0.4910782116504622 accuracy 0.8673740029335022 macro_avg {'precision': 0.8638307981720242, 'recall': 0.8626439046827914, 'f1-score': 0.8596931454302978, 'support': 1131} weighted_avg {'precision': 0.871308687893527, 'recall': 0.8673740053050398, 'f1-score': 0.865944120793141, 'support': 1131}
 
----------
Epoch 3/40
time = 242.49 secondes

Train loss 0.28218848636341715 accuracy 0.9212180972099304 macro_avg {'precision': 0.916672368081802, 'recall': 0.9153017667749884, 'f1-score': 0.9156778795223903, 'support': 10180} weighted_avg {'precision': 0.9211728054230192, 'recall': 0.9212180746561887, 'f1-score': 0.9209356895743103, 'support': 10180}
 
time = 5.22 secondes

Val loss 0.5317556057387675 accuracy 0.8691423535346985 macro_avg {'precision': 0.8763683054713514, 'recall': 0.8701091711957861, 'f1-score': 0.8685482331086772, 'support': 1131} weighted_avg {'precision': 0.8779667557960947, 'recall': 0.8691423519009726, 'f1-score': 0.8681341286629264, 'support': 1131}
 
----------
Epoch 4/40
time = 233.28 secondes

Train loss 0.21210408750431703 accuracy 0.9425343871116638 macro_avg {'precision': 0.9395207154244976, 'recall': 0.9386983107232787, 'f1-score': 0.9389846515422547, 'support': 10180} weighted_avg {'precision': 0.9423959910309652, 'recall': 0.9425343811394892, 'f1-score': 0.9423535657893622, 'support': 10180}
 
time = 5.21 secondes

Val loss 0.4916821052943369 accuracy 0.8921308517456055 macro_avg {'precision': 0.8948644783213154, 'recall': 0.8882307149408559, 'f1-score': 0.8883902336313989, 'support': 1131} weighted_avg {'precision': 0.8972361388669531, 'recall': 0.8921308576480991, 'f1-score': 0.8918426776225501, 'support': 1131}
 
----------
Epoch 5/40
time = 241.45 secondes

Train loss 0.1699967440232354 accuracy 0.9573674201965332 macro_avg {'precision': 0.9554035625732006, 'recall': 0.9552763949195278, 'f1-score': 0.9552710258300829, 'support': 10180} weighted_avg {'precision': 0.9575352666898409, 'recall': 0.9573673870333989, 'f1-score': 0.9573815884466447, 'support': 10180}
 
time = 5.22 secondes

Val loss 0.7318680806374046 accuracy 0.8656056523323059 macro_avg {'precision': 0.879168753297295, 'recall': 0.872606243777974, 'f1-score': 0.8684757790153016, 'support': 1131} weighted_avg {'precision': 0.8880865381175838, 'recall': 0.865605658709107, 'f1-score': 0.869605571564478, 'support': 1131}
 
----------
Epoch 6/40
time = 241.65 secondes

Train loss 0.16014801397043524 accuracy 0.9621807932853699 macro_avg {'precision': 0.9613539716191483, 'recall': 0.9611087609395129, 'f1-score': 0.9611608237472611, 'support': 10180} weighted_avg {'precision': 0.9622626140180315, 'recall': 0.962180746561886, 'f1-score': 0.9621494902354872, 'support': 10180}
 
time = 5.18 secondes

Val loss 0.5683415353865507 accuracy 0.8894783854484558 macro_avg {'precision': 0.8930818464253013, 'recall': 0.8904117992518883, 'f1-score': 0.8894887533745894, 'support': 1131} weighted_avg {'precision': 0.8949500811005436, 'recall': 0.8894783377541998, 'f1-score': 0.8900980453465683, 'support': 1131}
 
----------
Epoch 7/40
time = 240.88 secondes

Train loss 0.14736697123385234 accuracy 0.965618908405304 macro_avg {'precision': 0.9643803853974733, 'recall': 0.9642651035727863, 'f1-score': 0.9642807683068153, 'support': 10180} weighted_avg {'precision': 0.9657680449431586, 'recall': 0.9656188605108055, 'f1-score': 0.9656501485948229, 'support': 10180}
 
time = 5.20 secondes

Val loss 0.6968451252963039 accuracy 0.8921308517456055 macro_avg {'precision': 0.9063356426085759, 'recall': 0.8937872470991044, 'f1-score': 0.8941785159595023, 'support': 1131} weighted_avg {'precision': 0.9050582714174104, 'recall': 0.8921308576480991, 'f1-score': 0.8924090392865673, 'support': 1131}
 
----------
Epoch 8/40
time = 240.76 secondes

Train loss 0.149434083597179 accuracy 0.9678782224655151 macro_avg {'precision': 0.966407037534658, 'recall': 0.9662779013196555, 'f1-score': 0.9662543204348495, 'support': 10180} weighted_avg {'precision': 0.9680710107746947, 'recall': 0.9678781925343811, 'f1-score': 0.9678924182298309, 'support': 10180}
 
time = 5.20 secondes

Val loss 0.690487482419408 accuracy 0.8912467360496521 macro_avg {'precision': 0.8935376720260338, 'recall': 0.8917592254472702, 'f1-score': 0.8893270631436223, 'support': 1131} weighted_avg {'precision': 0.8967139918551417, 'recall': 0.8912466843501327, 'f1-score': 0.890649100548282, 'support': 1131}
 
----------
Epoch 9/40
time = 236.63 secondes

Train loss 0.1305105469352328 accuracy 0.9694499373435974 macro_avg {'precision': 0.9684114293144042, 'recall': 0.9683969323748114, 'f1-score': 0.9683363516156712, 'support': 10180} weighted_avg {'precision': 0.9696923620931471, 'recall': 0.9694499017681729, 'f1-score': 0.9695074270832026, 'support': 10180}
 
time = 5.81 secondes

Val loss 0.6587127394048647 accuracy 0.8983200788497925 macro_avg {'precision': 0.9027547626076584, 'recall': 0.8973897386957432, 'f1-score': 0.897430440845185, 'support': 1131} weighted_avg {'precision': 0.9026821868605123, 'recall': 0.8983200707338639, 'f1-score': 0.8980069402790262, 'support': 1131}
 
----------
Epoch 10/40
time = 242.07 secondes

Train loss 0.12203179339174104 accuracy 0.9747544527053833 macro_avg {'precision': 0.9741849789466108, 'recall': 0.9742464078369247, 'f1-score': 0.9741379815263496, 'support': 10180} weighted_avg {'precision': 0.9748566625632311, 'recall': 0.97475442043222, 'f1-score': 0.974729239817267, 'support': 10180}
 
time = 5.20 secondes

Val loss 0.6077434339685182 accuracy 0.9053934812545776 macro_avg {'precision': 0.9059668006678369, 'recall': 0.9057159120374572, 'f1-score': 0.9053275117016077, 'support': 1131} weighted_avg {'precision': 0.905638077277579, 'recall': 0.905393457117595, 'f1-score': 0.905009388790889, 'support': 1131}
 
----------
Epoch 11/40
time = 234.30 secondes

Train loss 0.11160660652471911 accuracy 0.9771119952201843 macro_avg {'precision': 0.9758678485958214, 'recall': 0.9755373107683823, 'f1-score': 0.9756665161411693, 'support': 10180} weighted_avg {'precision': 0.9771658589687906, 'recall': 0.9771119842829077, 'f1-score': 0.9771017497937003, 'support': 10180}
 
time = 5.37 secondes

Val loss 0.8079591478400661 accuracy 0.8850575089454651 macro_avg {'precision': 0.89045258794388, 'recall': 0.8816807144210552, 'f1-score': 0.8838152781676787, 'support': 1131} weighted_avg {'precision': 0.888387720266583, 'recall': 0.8850574712643678, 'f1-score': 0.8847206569580652, 'support': 1131}
 
----------
Epoch 12/40
time = 241.06 secondes

Train loss 0.12632377943055748 accuracy 0.9751473665237427 macro_avg {'precision': 0.9745381440421189, 'recall': 0.974233942298171, 'f1-score': 0.974341916805068, 'support': 10180} weighted_avg {'precision': 0.9752101804693061, 'recall': 0.975147347740668, 'f1-score': 0.9751355613847025, 'support': 10180}
 
time = 5.19 secondes

Val loss 0.931098633782543 accuracy 0.8815208077430725 macro_avg {'precision': 0.8970737576636652, 'recall': 0.8838306399262079, 'f1-score': 0.8855271950866992, 'support': 1131} weighted_avg {'precision': 0.8945029412498278, 'recall': 0.8815207780725022, 'f1-score': 0.8828067038302735, 'support': 1131}
 
----------
Epoch 13/40
time = 241.19 secondes

Train loss 0.12037763775834738 accuracy 0.9781925678253174 macro_avg {'precision': 0.9777754007727875, 'recall': 0.9778157440297324, 'f1-score': 0.9777460254093409, 'support': 10180} weighted_avg {'precision': 0.9783010497579141, 'recall': 0.9781925343811395, 'f1-score': 0.978199843562036, 'support': 10180}
 
time = 5.16 secondes

Val loss 0.7450073800018159 accuracy 0.8983200788497925 macro_avg {'precision': 0.9031276071646765, 'recall': 0.8997059987468212, 'f1-score': 0.8992933579348271, 'support': 1131} weighted_avg {'precision': 0.9030972305887893, 'recall': 0.8983200707338639, 'f1-score': 0.8987001115748277, 'support': 1131}
 
----------
Epoch 14/40
time = 239.61 secondes

Train loss 0.10893101184164476 accuracy 0.9792731404304504 macro_avg {'precision': 0.9790850147760404, 'recall': 0.9788636696428803, 'f1-score': 0.9789395029663099, 'support': 10180} weighted_avg {'precision': 0.9793480193179145, 'recall': 0.9792730844793713, 'f1-score': 0.9792736544676294, 'support': 10180}
 
time = 5.22 secondes

Val loss 0.7789769347461554 accuracy 0.8930150270462036 macro_avg {'precision': 0.8949167840628757, 'recall': 0.8927886650873067, 'f1-score': 0.8929805169151314, 'support': 1131} weighted_avg {'precision': 0.8958347116712647, 'recall': 0.8930150309460654, 'f1-score': 0.8935877235575865, 'support': 1131}
 
----------
Epoch 15/40
time = 239.75 secondes

Train loss 0.1039695132742762 accuracy 0.9785854816436768 macro_avg {'precision': 0.9775905443922571, 'recall': 0.9771738390935745, 'f1-score': 0.9773274070419842, 'support': 10180} weighted_avg {'precision': 0.9786166067701758, 'recall': 0.9785854616895874, 'f1-score': 0.9785522414127941, 'support': 10180}
 
time = 5.24 secondes

Val loss 0.7463385245732432 accuracy 0.8894783854484558 macro_avg {'precision': 0.8925916386870073, 'recall': 0.8893862030398137, 'f1-score': 0.8890732073971339, 'support': 1131} weighted_avg {'precision': 0.8953705992520125, 'recall': 0.8894783377541998, 'f1-score': 0.890483427827065, 'support': 1131}
 
----------
Epoch 16/40
time = 234.54 secondes

Train loss 0.09988137630955389 accuracy 0.9818271398544312 macro_avg {'precision': 0.9812619160272096, 'recall': 0.9805326417034317, 'f1-score': 0.9808469918242386, 'support': 10180} weighted_avg {'precision': 0.981887306036467, 'recall': 0.981827111984283, 'f1-score': 0.9818159544252804, 'support': 10180}
 
time = 5.14 secondes

Val loss 0.8152506748258634 accuracy 0.9027409553527832 macro_avg {'precision': 0.9053181558290925, 'recall': 0.9047501594067157, 'f1-score': 0.9026635673762324, 'support': 1131} weighted_avg {'precision': 0.9077908002751652, 'recall': 0.9027409372236959, 'f1-score': 0.9027471000730999, 'support': 1131}
 
----------
Epoch 17/40
time = 240.22 secondes

Train loss 0.0886922204541746 accuracy 0.9833988547325134 macro_avg {'precision': 0.9833530276776065, 'recall': 0.9832863350954402, 'f1-score': 0.9833001621057201, 'support': 10180} weighted_avg {'precision': 0.983478715885643, 'recall': 0.9833988212180746, 'f1-score': 0.9834188809711347, 'support': 10180}
 
time = 5.18 secondes

Val loss 0.9432713099723373 accuracy 0.8629531860351562 macro_avg {'precision': 0.8925231818827033, 'recall': 0.8708255109893142, 'f1-score': 0.8651558557737312, 'support': 1131} weighted_avg {'precision': 0.8911623871890729, 'recall': 0.8629531388152077, 'f1-score': 0.858229461132215, 'support': 1131}
 
----------
Epoch 18/40
time = 239.92 secondes

Train loss 0.08340722320919704 accuracy 0.9829077124595642 macro_avg {'precision': 0.9828359763840873, 'recall': 0.9827592498019817, 'f1-score': 0.9827678107485369, 'support': 10180} weighted_avg {'precision': 0.9829458406047644, 'recall': 0.9829076620825148, 'f1-score': 0.9828955247546344, 'support': 10180}
 
time = 5.19 secondes

Val loss 0.7524104989094833 accuracy 0.9009726047515869 macro_avg {'precision': 0.9062670231700627, 'recall': 0.902652014316344, 'f1-score': 0.9029042550174194, 'support': 1131} weighted_avg {'precision': 0.908235142433074, 'recall': 0.900972590627763, 'f1-score': 0.9031068418532432, 'support': 1131}
 
----------
Epoch 19/40
time = 239.44 secondes

Train loss 0.07779163807117727 accuracy 0.9850687980651855 macro_avg {'precision': 0.9852810570702337, 'recall': 0.9852136161175589, 'f1-score': 0.985234775592328, 'support': 10180} weighted_avg {'precision': 0.9850836687980802, 'recall': 0.9850687622789784, 'f1-score': 0.9850630670903067, 'support': 10180}
 
time = 5.17 secondes

Val loss 0.7767728598312676 accuracy 0.9009726047515869 macro_avg {'precision': 0.9071257440423356, 'recall': 0.9027701594776321, 'f1-score': 0.9019082942159622, 'support': 1131} weighted_avg {'precision': 0.907619920272567, 'recall': 0.900972590627763, 'f1-score': 0.9012437294872699, 'support': 1131}
 
----------
Epoch 20/40
time = 237.85 secondes

Train loss 0.09211054434319745 accuracy 0.9833988547325134 macro_avg {'precision': 0.9829935806955795, 'recall': 0.9827889344432448, 'f1-score': 0.9828622685112158, 'support': 10180} weighted_avg {'precision': 0.9834683570672319, 'recall': 0.9833988212180746, 'f1-score': 0.9834041303593538, 'support': 10180}
 
time = 5.83 secondes

Val loss 0.9471402184580269 accuracy 0.8877100348472595 macro_avg {'precision': 0.9036951114490577, 'recall': 0.8892797593250327, 'f1-score': 0.8920724560364528, 'support': 1131} weighted_avg {'precision': 0.9013062299579369, 'recall': 0.887709991158267, 'f1-score': 0.8897129095969999, 'support': 1131}
 
----------
Epoch 21/40
time = 240.38 secondes

Train loss 0.08230172854989065 accuracy 0.9855599403381348 macro_avg {'precision': 0.9853529726921477, 'recall': 0.9847886783824255, 'f1-score': 0.9850168000144036, 'support': 10180} weighted_avg {'precision': 0.985656927796175, 'recall': 0.9855599214145383, 'f1-score': 0.9855614404358672, 'support': 10180}
 
time = 5.17 secondes

Val loss 1.019574353450083 accuracy 0.8806366324424744 macro_avg {'precision': 0.8892261595337432, 'recall': 0.8877893183665175, 'f1-score': 0.8825889632998815, 'support': 1131} weighted_avg {'precision': 0.8940175626876322, 'recall': 0.8806366047745358, 'f1-score': 0.8812119119993267, 'support': 1131}
 
----------
Epoch 22/40
time = 234.49 secondes

Train loss 0.06578444373345052 accuracy 0.9886051416397095 macro_avg {'precision': 0.9885225606992091, 'recall': 0.9883520781995255, 'f1-score': 0.9884163852974203, 'support': 10180} weighted_avg {'precision': 0.9886529886639435, 'recall': 0.9886051080550098, 'f1-score': 0.9886079658618405, 'support': 10180}
 
time = 5.24 secondes

Val loss 0.9699526440645699 accuracy 0.8770999312400818 macro_avg {'precision': 0.885809689469802, 'recall': 0.8811506978038862, 'f1-score': 0.8790562477744459, 'support': 1131} weighted_avg {'precision': 0.8892440962344761, 'recall': 0.8770999115826702, 'f1-score': 0.8785500672995814, 'support': 1131}
 
----------
Epoch 23/40
time = 242.15 secondes

Train loss 0.06905906675108402 accuracy 0.9882122278213501 macro_avg {'precision': 0.9882703538466666, 'recall': 0.9878282144715602, 'f1-score': 0.9880174015394866, 'support': 10180} weighted_avg {'precision': 0.988279082046684, 'recall': 0.9882121807465619, 'f1-score': 0.9882139098080215, 'support': 10180}
 
time = 5.21 secondes

Val loss 0.7738221737497337 accuracy 0.9018567800521851 macro_avg {'precision': 0.9061321033835062, 'recall': 0.9021250376474766, 'f1-score': 0.9020173358755507, 'support': 1131} weighted_avg {'precision': 0.9073615529607723, 'recall': 0.9018567639257294, 'f1-score': 0.9025338808742872, 'support': 1131}
 
----------
Epoch 24/40
time = 241.70 secondes

Train loss 0.057717197855387696 accuracy 0.9887033700942993 macro_avg {'precision': 0.9884049368444655, 'recall': 0.988468787191073, 'f1-score': 0.9884267185417895, 'support': 10180} weighted_avg {'precision': 0.9887345992854603, 'recall': 0.9887033398821218, 'f1-score': 0.9887088195073683, 'support': 10180}
 
time = 5.17 secondes

Val loss 0.7822903664934311 accuracy 0.9098143577575684 macro_avg {'precision': 0.9088361470325133, 'recall': 0.9096673959831382, 'f1-score': 0.9083583989826248, 'support': 1131} weighted_avg {'precision': 0.9109906919673354, 'recall': 0.9098143236074271, 'f1-score': 0.9095932691179359, 'support': 1131}
 
----------
Epoch 25/40
time = 239.78 secondes

Train loss 0.05219531873503047 accuracy 0.9897839426994324 macro_avg {'precision': 0.9895297385685862, 'recall': 0.9894011442199225, 'f1-score': 0.9894540586456971, 'support': 10180} weighted_avg {'precision': 0.9898196453079874, 'recall': 0.9897838899803536, 'f1-score': 0.9897905370220123, 'support': 10180}
 
time = 5.22 secondes

Val loss 0.8607579740879808 accuracy 0.9000884294509888 macro_avg {'precision': 0.904187043489561, 'recall': 0.9015147059042737, 'f1-score': 0.898864201270627, 'support': 1131} weighted_avg {'precision': 0.9111607564842067, 'recall': 0.9000884173297966, 'f1-score': 0.9024275629772084, 'support': 1131}
 
----------
Epoch 26/40
time = 240.11 secondes

Train loss 0.05470014235608653 accuracy 0.9901768565177917 macro_avg {'precision': 0.9901064327313437, 'recall': 0.9898311876640238, 'f1-score': 0.9899394403749197, 'support': 10180} weighted_avg {'precision': 0.990222380565333, 'recall': 0.9901768172888016, 'f1-score': 0.9901725769910653, 'support': 10180}
 
time = 5.18 secondes

Val loss 0.7342938976459702 accuracy 0.9071618318557739 macro_avg {'precision': 0.9140264598453436, 'recall': 0.9089259672909172, 'f1-score': 0.9082717527159719, 'support': 1131} weighted_avg {'precision': 0.9133669422687386, 'recall': 0.9071618037135278, 'f1-score': 0.9069003833849142, 'support': 1131}
 
----------
Epoch 27/40
time = 235.31 secondes

Train loss 0.03424232543807729 accuracy 0.9924361705780029 macro_avg {'precision': 0.9925458932285359, 'recall': 0.9923988265726462, 'f1-score': 0.9924621334963737, 'support': 10180} weighted_avg {'precision': 0.9924603191387785, 'recall': 0.9924361493123772, 'f1-score': 0.9924383884596892, 'support': 10180}
 
time = 5.19 secondes

Val loss 0.9124069905736972 accuracy 0.8983200788497925 macro_avg {'precision': 0.9058407441129136, 'recall': 0.9004744557423466, 'f1-score': 0.9004246044233832, 'support': 1131} weighted_avg {'precision': 0.9052085765572453, 'recall': 0.8983200707338639, 'f1-score': 0.8988943672658435, 'support': 1131}
 
----------
Epoch 28/40
time = 240.49 secondes

Train loss 0.04638121871697822 accuracy 0.9909626841545105 macro_avg {'precision': 0.9911999303815036, 'recall': 0.9908896737399215, 'f1-score': 0.9910300915019304, 'support': 10180} weighted_avg {'precision': 0.9909958675470699, 'recall': 0.9909626719056974, 'f1-score': 0.9909641016803119, 'support': 10180}
 
time = 5.23 secondes

Val loss 0.772117155141375 accuracy 0.9027409553527832 macro_avg {'precision': 0.9043704735060476, 'recall': 0.904551688076895, 'f1-score': 0.9028955258287171, 'support': 1131} weighted_avg {'precision': 0.9063909168521148, 'recall': 0.9027409372236959, 'f1-score': 0.9028494046745401, 'support': 1131}
 
----------
Epoch 29/40
time = 242.02 secondes

Train loss 0.03580565280650288 accuracy 0.9932220578193665 macro_avg {'precision': 0.9932058683609875, 'recall': 0.993019832646544, 'f1-score': 0.9930990143870229, 'support': 10180} weighted_avg {'precision': 0.9932654478115206, 'recall': 0.9932220039292731, 'f1-score': 0.9932291933455317, 'support': 10180}
 
time = 5.29 secondes

Val loss 0.811938214811421 accuracy 0.9036251306533813 macro_avg {'precision': 0.9038415868763193, 'recall': 0.9058537698212266, 'f1-score': 0.9039953855827036, 'support': 1131} weighted_avg {'precision': 0.9066226231895609, 'recall': 0.9036251105216623, 'f1-score': 0.9042850952379607, 'support': 1131}
 
----------
Epoch 30/40
time = 240.39 secondes

Train loss 0.04750907334692645 accuracy 0.9915521144866943 macro_avg {'precision': 0.9915411912254483, 'recall': 0.9912795661122363, 'f1-score': 0.9913806664719175, 'support': 10180} weighted_avg {'precision': 0.9916318560692058, 'recall': 0.9915520628683694, 'f1-score': 0.991561777734496, 'support': 10180}
 
time = 5.20 secondes

Val loss 0.7963354651968677 accuracy 0.9115827083587646 macro_avg {'precision': 0.9094994159908676, 'recall': 0.91188145488087, 'f1-score': 0.9093453591019094, 'support': 1131} weighted_avg {'precision': 0.9136860700034635, 'recall': 0.9115826702033598, 'f1-score': 0.9113197330832334, 'support': 1131}
 
----------
Epoch 31/40
time = 238.69 secondes

Train loss 0.04391564947850505 accuracy 0.9928290843963623 macro_avg {'precision': 0.9930058709377917, 'recall': 0.9928403960147577, 'f1-score': 0.9929019022044047, 'support': 10180} weighted_avg {'precision': 0.9928917429124163, 'recall': 0.9928290766208252, 'f1-score': 0.9928382599342858, 'support': 10180}
 
time = 5.78 secondes

Val loss 0.8067651749122273 accuracy 0.9045093059539795 macro_avg {'precision': 0.9070754974579671, 'recall': 0.9057546438039361, 'f1-score': 0.90483126410968, 'support': 1131} weighted_avg {'precision': 0.9100887440992637, 'recall': 0.9045092838196287, 'f1-score': 0.9056229907102371, 'support': 1131}
 
----------
Epoch 32/40
time = 235.07 secondes

Train loss 0.04151187469106049 accuracy 0.9922397136688232 macro_avg {'precision': 0.9925599118827954, 'recall': 0.9923397571179894, 'f1-score': 0.9924343538691043, 'support': 10180} weighted_avg {'precision': 0.992282922200766, 'recall': 0.9922396856581532, 'f1-score': 0.9922450899029838, 'support': 10180}
 
time = 5.25 secondes

Val loss 0.9184843371082716 accuracy 0.8947833776473999 macro_avg {'precision': 0.9048945267082728, 'recall': 0.8962988732727704, 'f1-score': 0.8974625323171976, 'support': 1131} weighted_avg {'precision': 0.9040308981988416, 'recall': 0.8947833775419982, 'f1-score': 0.8960754993543906, 'support': 1131}
 
----------
Epoch 33/40
time = 241.92 secondes

Train loss 0.035072007098670054 accuracy 0.9943025708198547 macro_avg {'precision': 0.9944256244484881, 'recall': 0.9941652782444084, 'f1-score': 0.9942729694366189, 'support': 10180} weighted_avg {'precision': 0.9943683453539209, 'recall': 0.9943025540275049, 'f1-score': 0.9943122255335798, 'support': 10180}
 
time = 5.19 secondes

Val loss 0.8176990939211695 accuracy 0.9098143577575684 macro_avg {'precision': 0.9104904384322353, 'recall': 0.9107494905391793, 'f1-score': 0.9091169127657295, 'support': 1131} weighted_avg {'precision': 0.9125833962041228, 'recall': 0.9098143236074271, 'f1-score': 0.9097516773988978, 'support': 1131}
 
----------
Epoch 34/40
time = 240.43 secondes

Train loss 0.030135209365323466 accuracy 0.9939096570014954 macro_avg {'precision': 0.9940923802726296, 'recall': 0.9939381576633239, 'f1-score': 0.9939997181032567, 'support': 10180} weighted_avg {'precision': 0.9939585979508212, 'recall': 0.993909626719057, 'f1-score': 0.9939175867013097, 'support': 10180}
 
time = 5.22 secondes

Val loss 0.8748405659075296 accuracy 0.9000884294509888 macro_avg {'precision': 0.9033410279097056, 'recall': 0.9028714954424615, 'f1-score': 0.9014233588124361, 'support': 1131} weighted_avg {'precision': 0.905857965743779, 'recall': 0.9000884173297966, 'f1-score': 0.9012419138425413, 'support': 1131}
 
----------
Epoch 35/40
time = 241.31 secondes

Train loss 0.029428113425719386 accuracy 0.9946955442428589 macro_avg {'precision': 0.9947897343313878, 'recall': 0.9945127052292531, 'f1-score': 0.9946338281004017, 'support': 10180} weighted_avg {'precision': 0.9947456346677007, 'recall': 0.9946954813359529, 'f1-score': 0.9947024709285917, 'support': 10180}
 
time = 5.30 secondes

Val loss 0.7662999795629987 accuracy 0.9115827083587646 macro_avg {'precision': 0.9123744334433528, 'recall': 0.9128225611571633, 'f1-score': 0.9121297855356882, 'support': 1131} weighted_avg {'precision': 0.913229661592086, 'recall': 0.9115826702033598, 'f1-score': 0.9119031556986644, 'support': 1131}
 
----------
Epoch 36/40
time = 240.97 secondes

Train loss 0.017839458019596043 accuracy 0.9959725141525269 macro_avg {'precision': 0.9961238303916831, 'recall': 0.9959211773506402, 'f1-score': 0.9960066132830429, 'support': 10180} weighted_avg {'precision': 0.9960219284554163, 'recall': 0.9959724950884087, 'f1-score': 0.9959806287311922, 'support': 10180}
 
time = 7.10 secondes

Val loss 0.8724513886842596 accuracy 0.9036251306533813 macro_avg {'precision': 0.9046747591319019, 'recall': 0.9049792512897785, 'f1-score': 0.9033386601830518, 'support': 1131} weighted_avg {'precision': 0.908106563383422, 'recall': 0.9036251105216623, 'f1-score': 0.9043520528260459, 'support': 1131}
 
----------
Epoch 37/40
time = 244.06 secondes

Train loss 0.01743872369148831 accuracy 0.995874285697937 macro_avg {'precision': 0.9961045831193773, 'recall': 0.9959045097855658, 'f1-score': 0.9959899367035229, 'support': 10180} weighted_avg {'precision': 0.995927010813156, 'recall': 0.9958742632612967, 'f1-score': 0.99588510163527, 'support': 10180}
 
time = 5.54 secondes

Val loss 0.889854495900655 accuracy 0.9071618318557739 macro_avg {'precision': 0.909658690263347, 'recall': 0.9077122237214088, 'f1-score': 0.9067808222266305, 'support': 1131} weighted_avg {'precision': 0.912376359132009, 'recall': 0.9071618037135278, 'f1-score': 0.907875706092507, 'support': 1131}
 
----------
Epoch 38/40
time = 250.16 secondes

Train loss 0.015314323553605132 accuracy 0.996365487575531 macro_avg {'precision': 0.9965381763564306, 'recall': 0.99632114591566, 'f1-score': 0.9964153730151868, 'support': 10180} weighted_avg {'precision': 0.9964143827045193, 'recall': 0.9963654223968565, 'f1-score': 0.9963748308734635, 'support': 10180}
 
time = 7.07 secondes

Val loss 0.7844555207126357 accuracy 0.9142352342605591 macro_avg {'precision': 0.9146000711846153, 'recall': 0.9144171573352923, 'f1-score': 0.9132359739461533, 'support': 1131} weighted_avg {'precision': 0.9168835850408318, 'recall': 0.9142351900972591, 'f1-score': 0.9143322645236514, 'support': 1131}
 
----------
Epoch 39/40
time = 249.79 secondes

Train loss 0.010156716713642353 accuracy 0.9974460005760193 macro_avg {'precision': 0.9975799673445099, 'recall': 0.9974546492263441, 'f1-score': 0.9975037535640962, 'support': 10180} weighted_avg {'precision': 0.997495305655022, 'recall': 0.9974459724950884, 'f1-score': 0.997456152391225, 'support': 10180}
 
time = 7.27 secondes

Val loss 0.8065748114480248 accuracy 0.9160035848617554 macro_avg {'precision': 0.9165889151337947, 'recall': 0.9163583366606041, 'f1-score': 0.9153374353980087, 'support': 1131} weighted_avg {'precision': 0.9191001009011879, 'recall': 0.9160035366931919, 'f1-score': 0.9164064599415629, 'support': 1131}
 
----------
Epoch 40/40
time = 282.30 secondes

Train loss 0.008151907847998956 accuracy 0.9975442290306091 macro_avg {'precision': 0.9977139055125337, 'recall': 0.9975152884782315, 'f1-score': 0.9976008759907776, 'support': 10180} weighted_avg {'precision': 0.9975922377828589, 'recall': 0.9975442043222004, 'f1-score': 0.9975537233529747, 'support': 10180}
 
time = 7.25 secondes

Val loss 0.804712722515881 accuracy 0.9151194095611572 macro_avg {'precision': 0.9153822919878583, 'recall': 0.915580352739763, 'f1-score': 0.9144660783299431, 'support': 1131} weighted_avg {'precision': 0.9178939280205993, 'recall': 0.9151193633952255, 'f1-score': 0.9154583443944836, 'support': 1131}
 
----------
best_accuracy 0.9160035848617554 best_epoch 39 macro_avg {'precision': 0.9165889151337947, 'recall': 0.9163583366606041, 'f1-score': 0.9153374353980087, 'support': 1131} weighted_avg {'precision': 0.9191001009011879, 'recall': 0.9160035366931919, 'f1-score': 0.9164064599415629, 'support': 1131}

average train time 241.38100823163987

average val time 5.4669983148574826
 
time = 53.46 secondes

test_accuracy 0.845418393611908 macro_avg {'precision': 0.8405713605197402, 'recall': 0.8373682971594523, 'f1-score': 0.8380341431731265, 'support': 7530} weighted_avg {'precision': 0.8478652957999896, 'recall': 0.8454183266932271, 'f1-score': 0.8457130807898442, 'support': 7530}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_head_text_rank_1
----------
Epoch 1/40
time = 267.92 secondes

Train loss 1.2901145130676601 accuracy 0.6720039248466492 macro_avg {'precision': 0.7004832424076479, 'recall': 0.6552844427950648, 'f1-score': 0.6485620025943819, 'support': 10180} weighted_avg {'precision': 0.7014853522100034, 'recall': 0.6720039292730845, 'f1-score': 0.6635122176931935, 'support': 10180}
 
time = 7.43 secondes

Val loss 0.6584891693692811 accuracy 0.8019452095031738 macro_avg {'precision': 0.7849212335179093, 'recall': 0.7945690512097936, 'f1-score': 0.7831104030145923, 'support': 1131} weighted_avg {'precision': 0.7904660566670981, 'recall': 0.801945181255526, 'f1-score': 0.7899104219244996, 'support': 1131}
 
----------
Epoch 2/40
time = 247.90 secondes

Train loss 0.48496347675728646 accuracy 0.8553045392036438 macro_avg {'precision': 0.8456177852988912, 'recall': 0.8440849489695162, 'f1-score': 0.8422094881293398, 'support': 10180} weighted_avg {'precision': 0.8525677272790382, 'recall': 0.8553045186640471, 'f1-score': 0.8519623143410425, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.5016119628095291 accuracy 0.8620690107345581 macro_avg {'precision': 0.8648704025028234, 'recall': 0.8564332000402427, 'f1-score': 0.8585833877215496, 'support': 1131} weighted_avg {'precision': 0.8695228420599422, 'recall': 0.8620689655172413, 'f1-score': 0.863746119910853, 'support': 1131}
 
----------
Epoch 3/40
time = 227.45 secondes

Train loss 0.3050526772249999 accuracy 0.91277015209198 macro_avg {'precision': 0.9067210130726362, 'recall': 0.9055151724070978, 'f1-score': 0.9058408400367712, 'support': 10180} weighted_avg {'precision': 0.9127634831459945, 'recall': 0.9127701375245579, 'f1-score': 0.9125101334322827, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.48222303617430823 accuracy 0.8815208077430725 macro_avg {'precision': 0.8799166300485682, 'recall': 0.8764343793848651, 'f1-score': 0.8754881131295917, 'support': 1131} weighted_avg {'precision': 0.8809780799672697, 'recall': 0.8815207780725022, 'f1-score': 0.8786434451640709, 'support': 1131}
 
----------
Epoch 4/40
time = 220.70 secondes

Train loss 0.22725786962670683 accuracy 0.9388015866279602 macro_avg {'precision': 0.9349841388262906, 'recall': 0.9339821258558235, 'f1-score': 0.9343333298424747, 'support': 10180} weighted_avg {'precision': 0.9387889371956779, 'recall': 0.9388015717092338, 'f1-score': 0.9386571937388186, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.5502137617326118 accuracy 0.8815208077430725 macro_avg {'precision': 0.8853744654597024, 'recall': 0.8814566861071456, 'f1-score': 0.878857495553372, 'support': 1131} weighted_avg {'precision': 0.8861868232047749, 'recall': 0.8815207780725022, 'f1-score': 0.8785573751710786, 'support': 1131}
 
----------
Epoch 5/40
time = 226.44 secondes

Train loss 0.19641230086243333 accuracy 0.9507858753204346 macro_avg {'precision': 0.948511981459759, 'recall': 0.9484693777094252, 'f1-score': 0.9484055555860393, 'support': 10180} weighted_avg {'precision': 0.9508358712468861, 'recall': 0.9507858546168959, 'f1-score': 0.9507263598337382, 'support': 10180}
 
time = 4.88 secondes

Val loss 0.5521061757618797 accuracy 0.8832891583442688 macro_avg {'precision': 0.8901687593752688, 'recall': 0.8819169472205081, 'f1-score': 0.8825710852638039, 'support': 1131} weighted_avg {'precision': 0.8878001382409405, 'recall': 0.883289124668435, 'f1-score': 0.8824425151021483, 'support': 1131}
 
----------
Epoch 6/40
time = 225.59 secondes

Train loss 0.16167345823971568 accuracy 0.9603143930435181 macro_avg {'precision': 0.9584992858866789, 'recall': 0.9581818407284111, 'f1-score': 0.9581880224590977, 'support': 10180} weighted_avg {'precision': 0.9605859438917795, 'recall': 0.9603143418467583, 'f1-score': 0.9602990828018237, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.6517427161067042 accuracy 0.8877100348472595 macro_avg {'precision': 0.8918089334082125, 'recall': 0.8835994715497657, 'f1-score': 0.8847546998978622, 'support': 1131} weighted_avg {'precision': 0.8922136177631051, 'recall': 0.887709991158267, 'f1-score': 0.8874180681409564, 'support': 1131}
 
----------
Epoch 7/40
time = 223.70 secondes

Train loss 0.1637788680134439 accuracy 0.9611984491348267 macro_avg {'precision': 0.959826983379559, 'recall': 0.9595446275194824, 'f1-score': 0.9596372693833917, 'support': 10180} weighted_avg {'precision': 0.9612768790557801, 'recall': 0.9611984282907662, 'f1-score': 0.9611888275978184, 'support': 10180}
 
time = 4.91 secondes

Val loss 0.6350101954869362 accuracy 0.890362560749054 macro_avg {'precision': 0.8974570061114584, 'recall': 0.8911933766228829, 'f1-score': 0.8916498405166781, 'support': 1131} weighted_avg {'precision': 0.8976327515896442, 'recall': 0.8903625110521662, 'f1-score': 0.8912939016041167, 'support': 1131}
 
----------
Epoch 8/40
time = 221.25 secondes

Train loss 0.14298796218323187 accuracy 0.9681729078292847 macro_avg {'precision': 0.9672610486536491, 'recall': 0.9670765013080856, 'f1-score': 0.9671317229086431, 'support': 10180} weighted_avg {'precision': 0.968218439394306, 'recall': 0.9681728880157171, 'f1-score': 0.9681571905919177, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.7898611696461596 accuracy 0.8691423535346985 macro_avg {'precision': 0.887899102273823, 'recall': 0.872929445336946, 'f1-score': 0.875007796009038, 'support': 1131} weighted_avg {'precision': 0.8865159873834253, 'recall': 0.8691423519009726, 'f1-score': 0.8718047395706737, 'support': 1131}
 
----------
Epoch 9/40
time = 227.80 secondes

Train loss 0.1332599866179566 accuracy 0.9714145660400391 macro_avg {'precision': 0.9705973446711086, 'recall': 0.9702540668509286, 'f1-score': 0.9704079087676598, 'support': 10180} weighted_avg {'precision': 0.9714010547171026, 'recall': 0.9714145383104126, 'f1-score': 0.9713903787746004, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.7185338385182624 accuracy 0.8983200788497925 macro_avg {'precision': 0.902110171606789, 'recall': 0.896917204141268, 'f1-score': 0.8981652471547363, 'support': 1131} weighted_avg {'precision': 0.9010892209862555, 'recall': 0.8983200707338639, 'f1-score': 0.898337725655038, 'support': 1131}
 
----------
Epoch 10/40
time = 225.42 secondes

Train loss 0.1414438217231935 accuracy 0.9705305099487305 macro_avg {'precision': 0.9699559341235299, 'recall': 0.96940057895849, 'f1-score': 0.9696152410344488, 'support': 10180} weighted_avg {'precision': 0.9706528522389238, 'recall': 0.9705304518664047, 'f1-score': 0.9705283726821283, 'support': 10180}
 
time = 7.33 secondes

Val loss 0.6151147103920894 accuracy 0.8992042541503906 macro_avg {'precision': 0.8994826102984316, 'recall': 0.9005370862342794, 'f1-score': 0.8989742882011498, 'support': 1131} weighted_avg {'precision': 0.9021506827867637, 'recall': 0.8992042440318302, 'f1-score': 0.8995782607227174, 'support': 1131}
 
----------
Epoch 11/40
time = 298.47 secondes

Train loss 0.11268642879636911 accuracy 0.9763261675834656 macro_avg {'precision': 0.9760817847336245, 'recall': 0.9758410864094953, 'f1-score': 0.9759299242931926, 'support': 10180} weighted_avg {'precision': 0.9764596170913139, 'recall': 0.9763261296660117, 'f1-score': 0.9763605584996281, 'support': 10180}
 
time = 8.73 secondes

Val loss 0.6982385573886551 accuracy 0.8947833776473999 macro_avg {'precision': 0.8989023000980456, 'recall': 0.8956732095720019, 'f1-score': 0.8940497072807911, 'support': 1131} weighted_avg {'precision': 0.9016936925423146, 'recall': 0.8947833775419982, 'f1-score': 0.8949007012654155, 'support': 1131}
 
----------
Epoch 12/40
time = 263.74 secondes

Train loss 0.09843571792536958 accuracy 0.9789783954620361 macro_avg {'precision': 0.9785610831417326, 'recall': 0.9783508537114176, 'f1-score': 0.9784126099916428, 'support': 10180} weighted_avg {'precision': 0.9791499345750116, 'recall': 0.9789783889980354, 'f1-score': 0.9790200333994942, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.7867459102343558 accuracy 0.8850575089454651 macro_avg {'precision': 0.8925941427747249, 'recall': 0.8850328606236928, 'f1-score': 0.8838185314873982, 'support': 1131} weighted_avg {'precision': 0.8945909884218335, 'recall': 0.8850574712643678, 'f1-score': 0.8851050000586898, 'support': 1131}
 
----------
Epoch 13/40
time = 223.93 secondes

Train loss 0.10724455605449175 accuracy 0.9783890247344971 macro_avg {'precision': 0.9783251064651622, 'recall': 0.9782299679710753, 'f1-score': 0.9782361880042879, 'support': 10180} weighted_avg {'precision': 0.9784970709930761, 'recall': 0.9783889980353635, 'f1-score': 0.9784011348564265, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.7212337041263466 accuracy 0.9027409553527832 macro_avg {'precision': 0.9079014071521889, 'recall': 0.9015326466345399, 'f1-score': 0.9030796564084236, 'support': 1131} weighted_avg {'precision': 0.9054230981023329, 'recall': 0.9027409372236959, 'f1-score': 0.9026222752660402, 'support': 1131}
 
----------
Epoch 14/40
time = 219.60 secondes

Train loss 0.1060652055026164 accuracy 0.9798625111579895 macro_avg {'precision': 0.9793581434077694, 'recall': 0.9797176864457041, 'f1-score': 0.979474340530917, 'support': 10180} weighted_avg {'precision': 0.9799293410832106, 'recall': 0.9798624754420432, 'f1-score': 0.9798349626955717, 'support': 10180}
 
time = 5.11 secondes

Val loss 0.8513213630887011 accuracy 0.8841733336448669 macro_avg {'precision': 0.9014140173000753, 'recall': 0.8891240563334666, 'f1-score': 0.8898796275869479, 'support': 1131} weighted_avg {'precision': 0.9005492163358568, 'recall': 0.8841732979664014, 'f1-score': 0.8867798437841086, 'support': 1131}
 
----------
Epoch 15/40
time = 225.16 secondes

Train loss 0.0936630969154409 accuracy 0.9824165105819702 macro_avg {'precision': 0.9820840284896712, 'recall': 0.982112758816913, 'f1-score': 0.9820740465033415, 'support': 10180} weighted_avg {'precision': 0.9825242847456129, 'recall': 0.9824165029469548, 'f1-score': 0.9824464880037547, 'support': 10180}
 
time = 5.64 secondes

Val loss 0.8696198455983623 accuracy 0.8850575089454651 macro_avg {'precision': 0.8979367759472334, 'recall': 0.8805349088146277, 'f1-score': 0.8834531094296064, 'support': 1131} weighted_avg {'precision': 0.8955735832829699, 'recall': 0.8850574712643678, 'f1-score': 0.885304344833838, 'support': 1131}
 
----------
Epoch 16/40
time = 232.40 secondes

Train loss 0.09373994883854568 accuracy 0.981925368309021 macro_avg {'precision': 0.9818545297736586, 'recall': 0.9815963040558252, 'f1-score': 0.981681152942382, 'support': 10180} weighted_avg {'precision': 0.982020217541798, 'recall': 0.9819253438113948, 'f1-score': 0.9819284786058913, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.7434266330370954 accuracy 0.9045093059539795 macro_avg {'precision': 0.9055724898863552, 'recall': 0.9087095008155461, 'f1-score': 0.9045722361835002, 'support': 1131} weighted_avg {'precision': 0.910377981351077, 'recall': 0.9045092838196287, 'f1-score': 0.9046385195759782, 'support': 1131}
 
----------
Epoch 17/40
time = 219.41 secondes

Train loss 0.0852073819993534 accuracy 0.983693540096283 macro_avg {'precision': 0.9835227497759416, 'recall': 0.9834703744234401, 'f1-score': 0.9834741211115523, 'support': 10180} weighted_avg {'precision': 0.9837900121857261, 'recall': 0.9836935166994106, 'f1-score': 0.9837186851012693, 'support': 10180}
 
time = 4.91 secondes

Val loss 0.8058772287798822 accuracy 0.8912467360496521 macro_avg {'precision': 0.9001820522899079, 'recall': 0.8883072944353467, 'f1-score': 0.8903394504428587, 'support': 1131} weighted_avg {'precision': 0.9017106802879441, 'recall': 0.8912466843501327, 'f1-score': 0.8930389770539916, 'support': 1131}
 
----------
Epoch 18/40
time = 225.88 secondes

Train loss 0.0862807130051663 accuracy 0.9838899970054626 macro_avg {'precision': 0.9836736472559766, 'recall': 0.9834974424516462, 'f1-score': 0.9835531182558933, 'support': 10180} weighted_avg {'precision': 0.9839724486366203, 'recall': 0.9838899803536346, 'f1-score': 0.9839003568462777, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.7889530622114634 accuracy 0.8938992023468018 macro_avg {'precision': 0.8961824480957692, 'recall': 0.8956483254896697, 'f1-score': 0.8933493777603797, 'support': 1131} weighted_avg {'precision': 0.9000784786420278, 'recall': 0.8938992042440318, 'f1-score': 0.8944611908085989, 'support': 1131}
 
----------
Epoch 19/40
time = 228.42 secondes

Train loss 0.08333922663364413 accuracy 0.9833988547325134 macro_avg {'precision': 0.9833815352336271, 'recall': 0.9830934702812859, 'f1-score': 0.983188727499145, 'support': 10180} weighted_avg {'precision': 0.9835161420745046, 'recall': 0.9833988212180746, 'f1-score': 0.9834069064108586, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.8940603551128962 accuracy 0.890362560749054 macro_avg {'precision': 0.8959684831578176, 'recall': 0.8912495076587799, 'f1-score': 0.8895505308226286, 'support': 1131} weighted_avg {'precision': 0.8956305395390088, 'recall': 0.8903625110521662, 'f1-score': 0.8888008280682489, 'support': 1131}
 
----------
Epoch 20/40
time = 227.92 secondes

Train loss 0.07538770590480949 accuracy 0.9852652549743652 macro_avg {'precision': 0.984924792626364, 'recall': 0.9847436628782493, 'f1-score': 0.9848163840766606, 'support': 10180} weighted_avg {'precision': 0.9853022070941118, 'recall': 0.9852652259332023, 'f1-score': 0.985266480370869, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.8035848262317276 accuracy 0.9036251306533813 macro_avg {'precision': 0.9046379088273312, 'recall': 0.9043215814051022, 'f1-score': 0.9035060805555899, 'support': 1131} weighted_avg {'precision': 0.9059441591669839, 'recall': 0.9036251105216623, 'f1-score': 0.9037694866848515, 'support': 1131}
 
----------
Epoch 21/40
time = 229.20 secondes

Train loss 0.07649617259461425 accuracy 0.9852652549743652 macro_avg {'precision': 0.9849618108779958, 'recall': 0.9841575337330266, 'f1-score': 0.9845255383534376, 'support': 10180} weighted_avg {'precision': 0.9852913460855407, 'recall': 0.9852652259332023, 'f1-score': 0.9852510306207231, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.71709914195157 accuracy 0.9062776565551758 macro_avg {'precision': 0.9091232145368975, 'recall': 0.9059608042946733, 'f1-score': 0.9061926862559659, 'support': 1131} weighted_avg {'precision': 0.9095317157509181, 'recall': 0.9062776304155614, 'f1-score': 0.9065305663651001, 'support': 1131}
 
----------
Epoch 22/40
time = 216.56 secondes

Train loss 0.07508612635005481 accuracy 0.9858546257019043 macro_avg {'precision': 0.9851640974738652, 'recall': 0.9849592286813431, 'f1-score': 0.9850329060585074, 'support': 10180} weighted_avg {'precision': 0.985925618855856, 'recall': 0.9858546168958743, 'f1-score': 0.9858598801858665, 'support': 10180}
 
time = 4.98 secondes

Val loss 0.7771122515981224 accuracy 0.8912467360496521 macro_avg {'precision': 0.8916989219961419, 'recall': 0.8910002393936196, 'f1-score': 0.8896867376986837, 'support': 1131} weighted_avg {'precision': 0.8949465485330963, 'recall': 0.8912466843501327, 'f1-score': 0.8914766632258937, 'support': 1131}
 
----------
Epoch 23/40
time = 227.26 secondes

Train loss 0.06649474734402981 accuracy 0.9868369698524475 macro_avg {'precision': 0.9869598334560823, 'recall': 0.9866141262910209, 'f1-score': 0.9867399756537052, 'support': 10180} weighted_avg {'precision': 0.9869732011776734, 'recall': 0.9868369351669941, 'f1-score': 0.9868562958675936, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.8698853232045609 accuracy 0.8832891583442688 macro_avg {'precision': 0.8936118409954126, 'recall': 0.8868135990105023, 'f1-score': 0.8860164051735173, 'support': 1131} weighted_avg {'precision': 0.8960081927482021, 'recall': 0.883289124668435, 'f1-score': 0.8850063218565538, 'support': 1131}
 
----------
Epoch 24/40
time = 225.72 secondes

Train loss 0.06443913673503525 accuracy 0.9882122278213501 macro_avg {'precision': 0.9884347237536624, 'recall': 0.9878422348110023, 'f1-score': 0.9881050835950571, 'support': 10180} weighted_avg {'precision': 0.9882887699425753, 'recall': 0.9882121807465619, 'f1-score': 0.988216700675349, 'support': 10180}
 
time = 4.91 secondes

Val loss 0.9008349262796503 accuracy 0.8797524571418762 macro_avg {'precision': 0.9013076739343182, 'recall': 0.8860740963559991, 'f1-score': 0.8859691391537234, 'support': 1131} weighted_avg {'precision': 0.9026598609297724, 'recall': 0.8797524314765695, 'f1-score': 0.8831171379518498, 'support': 1131}
 
----------
Epoch 25/40
time = 227.82 secondes

Train loss 0.06199731292798212 accuracy 0.9878193140029907 macro_avg {'precision': 0.9878420547210801, 'recall': 0.9875495747907845, 'f1-score': 0.9876574337046028, 'support': 10180} weighted_avg {'precision': 0.9878944489600443, 'recall': 0.9878192534381139, 'f1-score': 0.987817068005623, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.7781249771592934 accuracy 0.9089301824569702 macro_avg {'precision': 0.9090593540135347, 'recall': 0.9104686617119768, 'f1-score': 0.9086618690075611, 'support': 1131} weighted_avg {'precision': 0.9117031386422307, 'recall': 0.9089301503094607, 'f1-score': 0.9092412723019458, 'support': 1131}
 
----------
Epoch 26/40
time = 223.52 secondes

Train loss 0.04905723339006256 accuracy 0.990667998790741 macro_avg {'precision': 0.9905403380862923, 'recall': 0.9905982765528446, 'f1-score': 0.9905401349249285, 'support': 10180} weighted_avg {'precision': 0.9907890921282178, 'recall': 0.9906679764243614, 'f1-score': 0.990699532536304, 'support': 10180}
 
time = 4.91 secondes

Val loss 0.8718313767422445 accuracy 0.8983200788497925 macro_avg {'precision': 0.9091984748318037, 'recall': 0.9001378818730345, 'f1-score': 0.9009247433880564, 'support': 1131} weighted_avg {'precision': 0.911648437796124, 'recall': 0.8983200707338639, 'f1-score': 0.9009764634535008, 'support': 1131}
 
----------
Epoch 27/40
time = 219.77 secondes

Train loss 0.04683483094658539 accuracy 0.9904715418815613 macro_avg {'precision': 0.9907126952630086, 'recall': 0.9904299653396533, 'f1-score': 0.9905488173846114, 'support': 10180} weighted_avg {'precision': 0.9905742971530102, 'recall': 0.9904715127701376, 'f1-score': 0.9904997879388057, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.7706838865484366 accuracy 0.9071618318557739 macro_avg {'precision': 0.9087197949291651, 'recall': 0.9086153168794034, 'f1-score': 0.9062095197734419, 'support': 1131} weighted_avg {'precision': 0.9131825622493647, 'recall': 0.9071618037135278, 'f1-score': 0.907430363854908, 'support': 1131}
 
----------
Epoch 28/40
time = 238.56 secondes

Train loss 0.03747292974340313 accuracy 0.9918467998504639 macro_avg {'precision': 0.991569028064854, 'recall': 0.9916000467265608, 'f1-score': 0.9915774177590031, 'support': 10180} weighted_avg {'precision': 0.9918813697160291, 'recall': 0.9918467583497053, 'f1-score': 0.991856719737464, 'support': 10180}
 
time = 7.53 secondes

Val loss 0.7398414855038347 accuracy 0.9080460071563721 macro_avg {'precision': 0.9069243953722591, 'recall': 0.9097181379380036, 'f1-score': 0.907413456300068, 'support': 1131} weighted_avg {'precision': 0.910722146862189, 'recall': 0.9080459770114943, 'f1-score': 0.9084153865189453, 'support': 1131}
 
----------
Epoch 29/40
time = 273.31 secondes

Train loss 0.04082086434592702 accuracy 0.9912574291229248 macro_avg {'precision': 0.9908840263386347, 'recall': 0.9904622177896103, 'f1-score': 0.9906428410553001, 'support': 10180} weighted_avg {'precision': 0.991333556937932, 'recall': 0.9912573673870334, 'f1-score': 0.9912639469301884, 'support': 10180}
 
time = 7.38 secondes

Val loss 0.7864075064969935 accuracy 0.9115827083587646 macro_avg {'precision': 0.9109939369694613, 'recall': 0.9107354035841976, 'f1-score': 0.9100794005864048, 'support': 1131} weighted_avg {'precision': 0.9138807059131406, 'recall': 0.9115826702033598, 'f1-score': 0.9119131571503732, 'support': 1131}
 
----------
Epoch 30/40
time = 268.51 secondes

Train loss 0.043603002998475415 accuracy 0.9914538860321045 macro_avg {'precision': 0.9914862324786119, 'recall': 0.9911208860181308, 'f1-score': 0.9912836678342121, 'support': 10180} weighted_avg {'precision': 0.9915106850527104, 'recall': 0.9914538310412574, 'f1-score': 0.9914625654576542, 'support': 10180}
 
time = 7.55 secondes

Val loss 0.873538985305087 accuracy 0.9018567800521851 macro_avg {'precision': 0.9069680122030827, 'recall': 0.9029615375478851, 'f1-score': 0.9022063481094607, 'support': 1131} weighted_avg {'precision': 0.9108884669188059, 'recall': 0.9018567639257294, 'f1-score': 0.903453447366854, 'support': 1131}
 
----------
Epoch 31/40
time = 271.73 secondes

Train loss 0.03251755231354804 accuracy 0.9931238293647766 macro_avg {'precision': 0.9934549970697603, 'recall': 0.9932387881448163, 'f1-score': 0.9933352431575699, 'support': 10180} weighted_avg {'precision': 0.9931774480813609, 'recall': 0.9931237721021611, 'f1-score': 0.9931386197619053, 'support': 10180}
 
time = 7.33 secondes

Val loss 0.7959042240392181 accuracy 0.9062776565551758 macro_avg {'precision': 0.9049985075192964, 'recall': 0.9058403820225622, 'f1-score': 0.904741852052006, 'support': 1131} weighted_avg {'precision': 0.9075861449403145, 'recall': 0.9062776304155614, 'f1-score': 0.9062024467019951, 'support': 1131}
 
----------
Epoch 32/40
time = 264.10 secondes

Train loss 0.03215694315108671 accuracy 0.9932220578193665 macro_avg {'precision': 0.9935482080237248, 'recall': 0.9932583237525188, 'f1-score': 0.9933733138429035, 'support': 10180} weighted_avg {'precision': 0.9933108910178264, 'recall': 0.9932220039292731, 'f1-score': 0.9932346338564448, 'support': 10180}
 
time = 7.42 secondes

Val loss 0.8279811651553527 accuracy 0.9053934812545776 macro_avg {'precision': 0.9053429663916536, 'recall': 0.9072972450571747, 'f1-score': 0.904885999481024, 'support': 1131} weighted_avg {'precision': 0.9091180783269026, 'recall': 0.905393457117595, 'f1-score': 0.9059413832399372, 'support': 1131}
 
----------
Epoch 33/40
time = 269.00 secondes

Train loss 0.022605627455932493 accuracy 0.9944007992744446 macro_avg {'precision': 0.9947063170464153, 'recall': 0.9945088804952447, 'f1-score': 0.9945786006180393, 'support': 10180} weighted_avg {'precision': 0.9944984083067377, 'recall': 0.9944007858546169, 'f1-score': 0.9944186599199637, 'support': 10180}
 
time = 7.27 secondes

Val loss 0.861799381664113 accuracy 0.9053934812545776 macro_avg {'precision': 0.9096562629868984, 'recall': 0.9053746175733179, 'f1-score': 0.905434716557866, 'support': 1131} weighted_avg {'precision': 0.9117097780427611, 'recall': 0.905393457117595, 'f1-score': 0.9064731142078696, 'support': 1131}
 
----------
Epoch 34/40
time = 265.45 secondes

Train loss 0.026123207489779723 accuracy 0.9944007992744446 macro_avg {'precision': 0.9946445835116627, 'recall': 0.9945317380900794, 'f1-score': 0.9945583369770479, 'support': 10180} weighted_avg {'precision': 0.994488155605169, 'recall': 0.9944007858546169, 'f1-score': 0.9944127878725011, 'support': 10180}
 
time = 7.38 secondes

Val loss 0.8158254677536988 accuracy 0.9098143577575684 macro_avg {'precision': 0.9103377385560047, 'recall': 0.9090084323689298, 'f1-score': 0.9091680251985454, 'support': 1131} weighted_avg {'precision': 0.9114969618415405, 'recall': 0.9098143236074271, 'f1-score': 0.9101751960592952, 'support': 1131}
 
----------
Epoch 35/40
time = 259.10 secondes

Train loss 0.023400445663112918 accuracy 0.9949902296066284 macro_avg {'precision': 0.9951466435571173, 'recall': 0.9948831502931883, 'f1-score': 0.9949945969963064, 'support': 10180} weighted_avg {'precision': 0.9950550568605767, 'recall': 0.9949901768172889, 'f1-score': 0.9950023213685603, 'support': 10180}
 
time = 7.48 secondes

Val loss 0.8855101435121827 accuracy 0.9080460071563721 macro_avg {'precision': 0.9054002361845888, 'recall': 0.9075016589616656, 'f1-score': 0.9055368151005633, 'support': 1131} weighted_avg {'precision': 0.9087993345379598, 'recall': 0.9080459770114943, 'f1-score': 0.9074579227576363, 'support': 1131}
 
----------
Epoch 36/40
time = 266.47 secondes

Train loss 0.020801392898910174 accuracy 0.9945972561836243 macro_avg {'precision': 0.9947935765868172, 'recall': 0.9946380468747462, 'f1-score': 0.9946966406339286, 'support': 10180} weighted_avg {'precision': 0.9946668394344048, 'recall': 0.9945972495088409, 'f1-score': 0.9946116199790289, 'support': 10180}
 
time = 7.35 secondes

Val loss 0.8287905697371302 accuracy 0.9106985330581665 macro_avg {'precision': 0.91080338715549, 'recall': 0.9122067667612219, 'f1-score': 0.9102753104430542, 'support': 1131} weighted_avg {'precision': 0.9138932516617063, 'recall': 0.9106984969053935, 'f1-score': 0.9109358310680689, 'support': 1131}
 
----------
Epoch 37/40
time = 272.83 secondes

Train loss 0.01761471263811047 accuracy 0.9959725141525269 macro_avg {'precision': 0.9962944888136255, 'recall': 0.9960447055285859, 'f1-score': 0.9961436024589364, 'support': 10180} weighted_avg {'precision': 0.9960668764823606, 'recall': 0.9959724950884087, 'f1-score': 0.9959919011333696, 'support': 10180}
 
time = 7.42 secondes

Val loss 0.8659169417040201 accuracy 0.9080460071563721 macro_avg {'precision': 0.9070358580006621, 'recall': 0.9085192939222202, 'f1-score': 0.9070149336959737, 'support': 1131} weighted_avg {'precision': 0.9100481944383088, 'recall': 0.9080459770114943, 'f1-score': 0.908270135369527, 'support': 1131}
 
----------
Epoch 38/40
time = 239.72 secondes

Train loss 0.016626731144381006 accuracy 0.995874285697937 macro_avg {'precision': 0.9961114117568609, 'recall': 0.9959046623455079, 'f1-score': 0.9959821525928971, 'support': 10180} weighted_avg {'precision': 0.9959684178198424, 'recall': 0.9958742632612967, 'f1-score': 0.9958936597765506, 'support': 10180}
 
time = 4.90 secondes

Val loss 0.8629623673872963 accuracy 0.9089301824569702 macro_avg {'precision': 0.910901710980801, 'recall': 0.9095121977409397, 'f1-score': 0.9092580710442382, 'support': 1131} weighted_avg {'precision': 0.9122859265560784, 'recall': 0.9089301503094607, 'f1-score': 0.9095944869372058, 'support': 1131}
 
----------
Epoch 39/40
time = 221.39 secondes

Train loss 0.009891796546209914 accuracy 0.9969548583030701 macro_avg {'precision': 0.9971727993459281, 'recall': 0.9969797237776529, 'f1-score': 0.9970515061631096, 'support': 10180} weighted_avg {'precision': 0.9970444980711491, 'recall': 0.9969548133595285, 'f1-score': 0.9969731917859667, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.9459587670333759 accuracy 0.9089301824569702 macro_avg {'precision': 0.9102311522122989, 'recall': 0.9105078815268277, 'f1-score': 0.9087555599315984, 'support': 1131} weighted_avg {'precision': 0.9120584359293354, 'recall': 0.9089301503094607, 'f1-score': 0.9088193433894571, 'support': 1131}
 
----------
Epoch 40/40
time = 221.62 secondes

Train loss 0.010042260334513534 accuracy 0.9965619444847107 macro_avg {'precision': 0.9968306700114397, 'recall': 0.9965498747446304, 'f1-score': 0.9966651361640674, 'support': 10180} weighted_avg {'precision': 0.9966507918322293, 'recall': 0.9965618860510805, 'f1-score': 0.9965796155456967, 'support': 10180}
 
time = 4.90 secondes

Val loss 0.9654523783901054 accuracy 0.9080460071563721 macro_avg {'precision': 0.9084019960206519, 'recall': 0.9085922630781903, 'f1-score': 0.9071073943779963, 'support': 1131} weighted_avg {'precision': 0.9109309255795497, 'recall': 0.9080459770114943, 'f1-score': 0.9080775568541543, 'support': 1131}
 
----------
best_accuracy 0.9115827083587646 best_epoch 29 macro_avg {'precision': 0.9109939369694613, 'recall': 0.9107354035841976, 'f1-score': 0.9100794005864048, 'support': 1131} weighted_avg {'precision': 0.9138807059131406, 'recall': 0.9115826702033598, 'f1-score': 0.9119131571503732, 'support': 1131}

average train time 239.51876841783525

average val time 5.787899845838547
 
time = 33.45 secondes

test_accuracy 0.8431607484817505 macro_avg {'precision': 0.8394473356558748, 'recall': 0.8346197003966953, 'f1-score': 0.8355777163379312, 'support': 7530} weighted_avg {'precision': 0.8458979217887587, 'recall': 0.8431606905710491, 'f1-score': 0.8431169369218939, 'support': 7530}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_head_tail_1
----------
Epoch 1/40
time = 228.39 secondes

Train loss 1.2830585191023407 accuracy 0.674361526966095 macro_avg {'precision': 0.7042590416835216, 'recall': 0.6590775138621932, 'f1-score': 0.6544036528757893, 'support': 10180} weighted_avg {'precision': 0.7068209071841186, 'recall': 0.6743614931237721, 'f1-score': 0.669243462486306, 'support': 10180}
 
time = 5.69 secondes

Val loss 0.6803297761460425 accuracy 0.802829384803772 macro_avg {'precision': 0.7829066236232617, 'recall': 0.7963952747543152, 'f1-score': 0.7825332420681937, 'support': 1131} weighted_avg {'precision': 0.7961894642612414, 'recall': 0.8028293545534925, 'f1-score': 0.7924820789714299, 'support': 1131}
 
----------
Epoch 2/40
time = 223.07 secondes

Train loss 0.4760333944691032 accuracy 0.8636542558670044 macro_avg {'precision': 0.8552114187577553, 'recall': 0.8531098400824131, 'f1-score': 0.8521712455956246, 'support': 10180} weighted_avg {'precision': 0.8622795231272339, 'recall': 0.8636542239685658, 'f1-score': 0.8614585343242591, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.5085004410693343 accuracy 0.8682581782341003 macro_avg {'precision': 0.8730237347390896, 'recall': 0.8677118827473749, 'f1-score': 0.8667095070377094, 'support': 1131} weighted_avg {'precision': 0.877731336027114, 'recall': 0.8682581786030061, 'f1-score': 0.8695298371943511, 'support': 1131}
 
----------
Epoch 3/40
time = 220.07 secondes

Train loss 0.3025209774676962 accuracy 0.9140471816062927 macro_avg {'precision': 0.9082012066775091, 'recall': 0.9073028929809321, 'f1-score': 0.9074763078174733, 'support': 10180} weighted_avg {'precision': 0.9137298295723221, 'recall': 0.9140471512770137, 'f1-score': 0.9136494292328738, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.517105889808334 accuracy 0.8735632300376892 macro_avg {'precision': 0.8730278007006314, 'recall': 0.8689261513603252, 'f1-score': 0.8686954785274986, 'support': 1131} weighted_avg {'precision': 0.8739423766903065, 'recall': 0.8735632183908046, 'f1-score': 0.8717047749984709, 'support': 1131}
 
----------
Epoch 4/40
time = 226.24 secondes

Train loss 0.23388779827202064 accuracy 0.9373281002044678 macro_avg {'precision': 0.9347337704323134, 'recall': 0.9332953590050452, 'f1-score': 0.9338069963566689, 'support': 10180} weighted_avg {'precision': 0.9373312563784226, 'recall': 0.937328094302554, 'f1-score': 0.9371480047208787, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.6222884856380889 accuracy 0.8709107041358948 macro_avg {'precision': 0.8701225204903705, 'recall': 0.8635248045782566, 'f1-score': 0.8619830578495751, 'support': 1131} weighted_avg {'precision': 0.8713772200058817, 'recall': 0.8709106984969054, 'f1-score': 0.866788524616402, 'support': 1131}
 
----------
Epoch 5/40
time = 225.24 secondes

Train loss 0.18871747343062628 accuracy 0.9521611332893372 macro_avg {'precision': 0.9503342929621489, 'recall': 0.9496045574033858, 'f1-score': 0.9499260222212437, 'support': 10180} weighted_avg {'precision': 0.952296480345862, 'recall': 0.9521611001964636, 'f1-score': 0.9521911427638041, 'support': 10180}
 
time = 5.39 secondes

Val loss 0.5795758194454067 accuracy 0.890362560749054 macro_avg {'precision': 0.897684537324418, 'recall': 0.8888184083303443, 'f1-score': 0.8877912779853091, 'support': 1131} weighted_avg {'precision': 0.8990947139198906, 'recall': 0.8903625110521662, 'f1-score': 0.8899815993390532, 'support': 1131}
 
----------
Epoch 6/40
time = 218.94 secondes

Train loss 0.16204462121635546 accuracy 0.9601179361343384 macro_avg {'precision': 0.9583310993324275, 'recall': 0.9581733986960348, 'f1-score': 0.9581780898136877, 'support': 10180} weighted_avg {'precision': 0.9601762479969125, 'recall': 0.9601178781925344, 'f1-score': 0.9600760217268638, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.6753562799782794 accuracy 0.8885942101478577 macro_avg {'precision': 0.8919521791672229, 'recall': 0.8900623761752587, 'f1-score': 0.8881208143193767, 'support': 1131} weighted_avg {'precision': 0.8961500521364948, 'recall': 0.8885941644562334, 'f1-score': 0.8895354499833277, 'support': 1131}
 
----------
Epoch 7/40
time = 225.05 secondes

Train loss 0.16323311493883358 accuracy 0.9629666209220886 macro_avg {'precision': 0.9616248810017298, 'recall': 0.9612335650687485, 'f1-score': 0.9613809945566338, 'support': 10180} weighted_avg {'precision': 0.9630540503718173, 'recall': 0.962966601178782, 'f1-score': 0.9629619260589494, 'support': 10180}
 
time = 4.98 secondes

Val loss 0.6721763610938223 accuracy 0.8868258595466614 macro_avg {'precision': 0.8884019780767936, 'recall': 0.8905161099500083, 'f1-score': 0.8873819065260198, 'support': 1131} weighted_avg {'precision': 0.890759614941706, 'recall': 0.8868258178603006, 'f1-score': 0.8866618966750515, 'support': 1131}
 
----------
Epoch 8/40
time = 225.68 secondes

Train loss 0.15330203396867262 accuracy 0.9667976498603821 macro_avg {'precision': 0.9660405169003596, 'recall': 0.9657636417444605, 'f1-score': 0.9658754885849022, 'support': 10180} weighted_avg {'precision': 0.9669265008402023, 'recall': 0.9667976424361493, 'f1-score': 0.9668351598862488, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.6529183594352552 accuracy 0.895667552947998 macro_avg {'precision': 0.9030228395397237, 'recall': 0.89772670021303, 'f1-score': 0.8972375579675027, 'support': 1131} weighted_avg {'precision': 0.9048165886347984, 'recall': 0.8956675508399646, 'f1-score': 0.897065411506179, 'support': 1131}
 
----------
Epoch 9/40
time = 226.13 secondes

Train loss 0.13662856976105794 accuracy 0.9730845093727112 macro_avg {'precision': 0.9727139129883786, 'recall': 0.9722014234562524, 'f1-score': 0.9723753798654101, 'support': 10180} weighted_avg {'precision': 0.9733212945359608, 'recall': 0.9730844793713163, 'f1-score': 0.9731206464600454, 'support': 10180}
 
time = 4.97 secondes

Val loss 0.6732724142895395 accuracy 0.9018567800521851 macro_avg {'precision': 0.9047545748686925, 'recall': 0.9017121580090525, 'f1-score': 0.9011853347821608, 'support': 1131} weighted_avg {'precision': 0.9055505815894599, 'recall': 0.9018567639257294, 'f1-score': 0.9018706313837621, 'support': 1131}
 
----------
Epoch 10/40
time = 224.05 secondes

Train loss 0.13843456778278374 accuracy 0.9704322814941406 macro_avg {'precision': 0.9698349841204432, 'recall': 0.9699565279248816, 'f1-score': 0.9698452334933906, 'support': 10180} weighted_avg {'precision': 0.9705204568024737, 'recall': 0.9704322200392927, 'f1-score': 0.9704262554351565, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.7197302790304554 accuracy 0.8859416842460632 macro_avg {'precision': 0.8900575696317728, 'recall': 0.8861258858937815, 'f1-score': 0.884914652772183, 'support': 1131} weighted_avg {'precision': 0.8914674862747624, 'recall': 0.8859416445623343, 'f1-score': 0.8854500164473632, 'support': 1131}
 
----------
Epoch 11/40
time = 219.44 secondes

Train loss 0.13139081810948475 accuracy 0.9738703966140747 macro_avg {'precision': 0.9736684849165931, 'recall': 0.973443169210449, 'f1-score': 0.9734917485877299, 'support': 10180} weighted_avg {'precision': 0.9740390802309602, 'recall': 0.9738703339882122, 'f1-score': 0.9738889475039889, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.8076934009080451 accuracy 0.8770999312400818 macro_avg {'precision': 0.8827841205681656, 'recall': 0.8825475808493584, 'f1-score': 0.8790945694528173, 'support': 1131} weighted_avg {'precision': 0.8883508247186942, 'recall': 0.8770999115826702, 'f1-score': 0.8790593401530211, 'support': 1131}
 
----------
Epoch 12/40
time = 223.79 secondes

Train loss 0.10938843878728427 accuracy 0.9778978824615479 macro_avg {'precision': 0.9774395518149864, 'recall': 0.9775138502804281, 'f1-score': 0.9774007344198911, 'support': 10180} weighted_avg {'precision': 0.978067530106983, 'recall': 0.9778978388998035, 'f1-score': 0.9779136623730593, 'support': 10180}
 
time = 5.13 secondes

Val loss 0.7774392747505515 accuracy 0.8912467360496521 macro_avg {'precision': 0.8946150483248185, 'recall': 0.8892649578245019, 'f1-score': 0.8900982843902188, 'support': 1131} weighted_avg {'precision': 0.8986783089433154, 'recall': 0.8912466843501327, 'f1-score': 0.8931143771141562, 'support': 1131}
 
----------
Epoch 13/40
time = 226.46 secondes

Train loss 0.11155108422688534 accuracy 0.9778978824615479 macro_avg {'precision': 0.9776704245064309, 'recall': 0.9775646881152541, 'f1-score': 0.9775582612885254, 'support': 10180} weighted_avg {'precision': 0.9780849699405078, 'recall': 0.9778978388998035, 'f1-score': 0.9779310017241668, 'support': 10180}
 
time = 5.04 secondes

Val loss 0.667198965942051 accuracy 0.9053934812545776 macro_avg {'precision': 0.9094459387771506, 'recall': 0.9082001027326541, 'f1-score': 0.9070601493723103, 'support': 1131} weighted_avg {'precision': 0.9110051288414909, 'recall': 0.905393457117595, 'f1-score': 0.9063979071049585, 'support': 1131}
 
----------
Epoch 14/40
time = 223.72 secondes

Train loss 0.09489547432345642 accuracy 0.9803536534309387 macro_avg {'precision': 0.9796051534259874, 'recall': 0.9794420547060902, 'f1-score': 0.9795049454138441, 'support': 10180} weighted_avg {'precision': 0.9803840715557735, 'recall': 0.9803536345776032, 'f1-score': 0.9803497244575784, 'support': 10180}
 
time = 4.97 secondes

Val loss 0.8236819178625961 accuracy 0.8885942101478577 macro_avg {'precision': 0.8924806633927205, 'recall': 0.891406145735823, 'f1-score': 0.8894266637439617, 'support': 1131} weighted_avg {'precision': 0.897012719702795, 'recall': 0.8885941644562334, 'f1-score': 0.8901763171461834, 'support': 1131}
 
----------
Epoch 15/40
time = 220.57 secondes

Train loss 0.09940407353899627 accuracy 0.9805501103401184 macro_avg {'precision': 0.9798024209897817, 'recall': 0.9798467931352688, 'f1-score': 0.9797707689675124, 'support': 10180} weighted_avg {'precision': 0.9807467881995214, 'recall': 0.9805500982318271, 'f1-score': 0.9805934896694872, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.8028926655808365 accuracy 0.8974359035491943 macro_avg {'precision': 0.9010533487605932, 'recall': 0.9014841393647052, 'f1-score': 0.8992854851126639, 'support': 1131} weighted_avg {'precision': 0.9038555075659606, 'recall': 0.8974358974358975, 'f1-score': 0.8987085594220998, 'support': 1131}
 
----------
Epoch 16/40
time = 225.67 secondes

Train loss 0.0912077322089437 accuracy 0.9808448553085327 macro_avg {'precision': 0.9803215789343982, 'recall': 0.9799263363448478, 'f1-score': 0.9800924856272228, 'support': 10180} weighted_avg {'precision': 0.9808893508652115, 'recall': 0.980844793713163, 'f1-score': 0.9808398575471531, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.7025544699945513 accuracy 0.9062776565551758 macro_avg {'precision': 0.9088509189218129, 'recall': 0.90860636062189, 'f1-score': 0.9075842467231221, 'support': 1131} weighted_avg {'precision': 0.9105005612810371, 'recall': 0.9062776304155614, 'f1-score': 0.9072373697116672, 'support': 1131}
 
----------
Epoch 17/40
time = 225.33 secondes

Train loss 0.08630306620627202 accuracy 0.9828094840049744 macro_avg {'precision': 0.9823508356693033, 'recall': 0.9824075504358625, 'f1-score': 0.9823259079640847, 'support': 10180} weighted_avg {'precision': 0.9829960924748676, 'recall': 0.9828094302554028, 'f1-score': 0.9828508482979531, 'support': 10180}
 
time = 4.99 secondes

Val loss 0.8942839723999831 accuracy 0.8930150270462036 macro_avg {'precision': 0.9007151679336574, 'recall': 0.8952769932440068, 'f1-score': 0.8948096792620515, 'support': 1131} weighted_avg {'precision': 0.904122434072862, 'recall': 0.8930150309460654, 'f1-score': 0.8954762901176235, 'support': 1131}
 
----------
Epoch 18/40
time = 223.02 secondes

Train loss 0.08818634122925711 accuracy 0.9833988547325134 macro_avg {'precision': 0.982777147363395, 'recall': 0.9830179235251956, 'f1-score': 0.982869021714319, 'support': 10180} weighted_avg {'precision': 0.983492867799917, 'recall': 0.9833988212180746, 'f1-score': 0.983419697984918, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.8542920814804129 accuracy 0.8850575089454651 macro_avg {'precision': 0.8967613702687279, 'recall': 0.8787807723708673, 'f1-score': 0.8826495444774116, 'support': 1131} weighted_avg {'precision': 0.8937408205432551, 'recall': 0.8850574712643678, 'f1-score': 0.8851589664638703, 'support': 1131}
 
----------
Epoch 19/40
time = 220.07 secondes

Train loss 0.07127160499498292 accuracy 0.9854617118835449 macro_avg {'precision': 0.9854239651197053, 'recall': 0.9853421158014983, 'f1-score': 0.9853434517289734, 'support': 10180} weighted_avg {'precision': 0.9855478739108663, 'recall': 0.9854616895874263, 'f1-score': 0.9854634743599178, 'support': 10180}
 
time = 5.04 secondes

Val loss 0.8503135254920803 accuracy 0.8930150270462036 macro_avg {'precision': 0.9010060471162811, 'recall': 0.8926668268211125, 'f1-score': 0.8943171214606396, 'support': 1131} weighted_avg {'precision': 0.8982501643476957, 'recall': 0.8930150309460654, 'f1-score': 0.8931534592656328, 'support': 1131}
 
----------
Epoch 20/40
time = 223.63 secondes

Train loss 0.09931909155223122 accuracy 0.9826130270957947 macro_avg {'precision': 0.9823445431365923, 'recall': 0.9819972351735679, 'f1-score': 0.9821437700956475, 'support': 10180} weighted_avg {'precision': 0.9826888762062062, 'recall': 0.9826129666011788, 'f1-score': 0.9826240857427412, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.7391820351917437 accuracy 0.9000884294509888 macro_avg {'precision': 0.9060432140246375, 'recall': 0.8990008936378062, 'f1-score': 0.9003013733227536, 'support': 1131} weighted_avg {'precision': 0.9045216992502138, 'recall': 0.9000884173297966, 'f1-score': 0.9001302055354593, 'support': 1131}
 
----------
Epoch 21/40
time = 224.77 secondes

Train loss 0.0897147113443981 accuracy 0.9846758842468262 macro_avg {'precision': 0.9846110814473107, 'recall': 0.9834878813718435, 'f1-score': 0.9839657088858423, 'support': 10180} weighted_avg {'precision': 0.9847917811216021, 'recall': 0.9846758349705305, 'f1-score': 0.9846580057183008, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.8042383269752412 accuracy 0.8894783854484558 macro_avg {'precision': 0.895127049799442, 'recall': 0.8923985210025984, 'f1-score': 0.8902442519640319, 'support': 1131} weighted_avg {'precision': 0.8958600848880639, 'recall': 0.8894783377541998, 'f1-score': 0.8891899581834081, 'support': 1131}
 
----------
Epoch 22/40
time = 225.84 secondes

Train loss 0.07822807987250147 accuracy 0.9854617118835449 macro_avg {'precision': 0.9857727529825773, 'recall': 0.9853139418547737, 'f1-score': 0.9854954109342259, 'support': 10180} weighted_avg {'precision': 0.9855292056190923, 'recall': 0.9854616895874263, 'f1-score': 0.9854466993324194, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.7859687734334598 accuracy 0.8983200788497925 macro_avg {'precision': 0.9019515089778773, 'recall': 0.8996435899348313, 'f1-score': 0.8985948734199326, 'support': 1131} weighted_avg {'precision': 0.9044969228780887, 'recall': 0.8983200707338639, 'f1-score': 0.8992254021018883, 'support': 1131}
 
----------
Epoch 23/40
time = 241.50 secondes

Train loss 0.0651263971062697 accuracy 0.9878193140029907 macro_avg {'precision': 0.9879145125975753, 'recall': 0.9874858839952975, 'f1-score': 0.987674821933077, 'support': 10180} weighted_avg {'precision': 0.9878761265083196, 'recall': 0.9878192534381139, 'f1-score': 0.9878225188853894, 'support': 10180}
 
time = 5.88 secondes

Val loss 0.7304968578942017 accuracy 0.9098143577575684 macro_avg {'precision': 0.9101097806461986, 'recall': 0.9120402751219381, 'f1-score': 0.9096550260447234, 'support': 1131} weighted_avg {'precision': 0.9133610376667175, 'recall': 0.9098143236074271, 'f1-score': 0.9103307712507201, 'support': 1131}
 
----------
Epoch 24/40
time = 274.74 secondes

Train loss 0.05365153830771905 accuracy 0.9894892573356628 macro_avg {'precision': 0.9893995562683207, 'recall': 0.9891529803691327, 'f1-score': 0.9892616496631697, 'support': 10180} weighted_avg {'precision': 0.9895355757176935, 'recall': 0.9894891944990177, 'f1-score': 0.9894970053639878, 'support': 10180}
 
time = 5.65 secondes

Val loss 0.8997549361294762 accuracy 0.8877100348472595 macro_avg {'precision': 0.8961223675132548, 'recall': 0.8898452396494816, 'f1-score': 0.8886728912675205, 'support': 1131} weighted_avg {'precision': 0.9002516888420047, 'recall': 0.887709991158267, 'f1-score': 0.8895546103737937, 'support': 1131}
 
----------
Epoch 25/40
time = 259.11 secondes

Train loss 0.06884385467567208 accuracy 0.9879175424575806 macro_avg {'precision': 0.9881007249012329, 'recall': 0.9878045755330607, 'f1-score': 0.9878976316469028, 'support': 10180} weighted_avg {'precision': 0.9880140330387167, 'recall': 0.9879174852652259, 'f1-score': 0.9879093544605301, 'support': 10180}
 
time = 5.88 secondes

Val loss 0.7622452639785274 accuracy 0.9053934812545776 macro_avg {'precision': 0.9109506100923348, 'recall': 0.9058808934392519, 'f1-score': 0.906222597785673, 'support': 1131} weighted_avg {'precision': 0.9101332914490035, 'recall': 0.905393457117595, 'f1-score': 0.905636935019289, 'support': 1131}
 
----------
Epoch 26/40
time = 264.92 secondes

Train loss 0.04669598382689676 accuracy 0.9904715418815613 macro_avg {'precision': 0.9903606855965357, 'recall': 0.9902953065448491, 'f1-score': 0.9903074043285445, 'support': 10180} weighted_avg {'precision': 0.9905219908011307, 'recall': 0.9904715127701376, 'f1-score': 0.9904757264623179, 'support': 10180}
 
time = 5.71 secondes

Val loss 0.838899769730502 accuracy 0.9009726047515869 macro_avg {'precision': 0.9027889577509572, 'recall': 0.9028489171424686, 'f1-score': 0.9019102911825364, 'support': 1131} weighted_avg {'precision': 0.9035816511704642, 'recall': 0.900972590627763, 'f1-score': 0.9012977390872786, 'support': 1131}
 
----------
Epoch 27/40
time = 272.83 secondes

Train loss 0.048253639850815176 accuracy 0.9902750849723816 macro_avg {'precision': 0.9902211509506772, 'recall': 0.9900475391253266, 'f1-score': 0.9901176830602261, 'support': 10180} weighted_avg {'precision': 0.9903245104191528, 'recall': 0.9902750491159136, 'f1-score': 0.9902824583912908, 'support': 10180}
 
time = 5.83 secondes

Val loss 0.8188495534418059 accuracy 0.8974359035491943 macro_avg {'precision': 0.8996167370438538, 'recall': 0.8988796802695515, 'f1-score': 0.8973164966260102, 'support': 1131} weighted_avg {'precision': 0.9016978949735063, 'recall': 0.8974358974358975, 'f1-score': 0.8976279268177039, 'support': 1131}
 
----------
Epoch 28/40
time = 259.51 secondes

Train loss 0.04910119991081256 accuracy 0.9907662272453308 macro_avg {'precision': 0.9910642110167578, 'recall': 0.9903740859527878, 'f1-score': 0.9906845273192022, 'support': 10180} weighted_avg {'precision': 0.990842298550745, 'recall': 0.9907662082514734, 'f1-score': 0.9907742664398819, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.7500910023962103 accuracy 0.9098143577575684 macro_avg {'precision': 0.9113203372109595, 'recall': 0.9107852622534358, 'f1-score': 0.9101329277309456, 'support': 1131} weighted_avg {'precision': 0.912815214249484, 'recall': 0.9098143236074271, 'f1-score': 0.9103385518353394, 'support': 1131}
 
----------
Epoch 29/40
time = 272.89 secondes

Train loss 0.04040519720335712 accuracy 0.9912574291229248 macro_avg {'precision': 0.9913642692202187, 'recall': 0.9912058417485937, 'f1-score': 0.9912695481930717, 'support': 10180} weighted_avg {'precision': 0.9913131003787603, 'recall': 0.9912573673870334, 'f1-score': 0.9912696984381495, 'support': 10180}
 
time = 5.71 secondes

Val loss 0.8330000724029009 accuracy 0.8992042541503906 macro_avg {'precision': 0.905430585822752, 'recall': 0.9003626740644843, 'f1-score': 0.9005021599755644, 'support': 1131} weighted_avg {'precision': 0.9079238415305372, 'recall': 0.8992042440318302, 'f1-score': 0.9011278410616644, 'support': 1131}
 
----------
Epoch 30/40
time = 270.02 secondes

Train loss 0.04569226762631581 accuracy 0.9903733134269714 macro_avg {'precision': 0.990108543940225, 'recall': 0.9901225401566196, 'f1-score': 0.9900947883886037, 'support': 10180} weighted_avg {'precision': 0.9904593268686983, 'recall': 0.9903732809430256, 'f1-score': 0.9903952546066227, 'support': 10180}
 
time = 5.77 secondes

Val loss 0.8477973503333845 accuracy 0.9036251306533813 macro_avg {'precision': 0.9047870878840808, 'recall': 0.9066408901404245, 'f1-score': 0.9035884451897065, 'support': 1131} weighted_avg {'precision': 0.9089181026891537, 'recall': 0.9036251105216623, 'f1-score': 0.904236218416003, 'support': 1131}
 
----------
Epoch 31/40
time = 256.06 secondes

Train loss 0.03334398646325991 accuracy 0.9929273724555969 macro_avg {'precision': 0.9929487490478703, 'recall': 0.9929615002299419, 'f1-score': 0.9929364254261343, 'support': 10180} weighted_avg {'precision': 0.9929861983202067, 'recall': 0.9929273084479371, 'f1-score': 0.9929375948500194, 'support': 10180}
 
time = 5.65 secondes

Val loss 0.9186351692723561 accuracy 0.8983200788497925 macro_avg {'precision': 0.9063450962680604, 'recall': 0.9018444497598278, 'f1-score': 0.9011079699872366, 'support': 1131} weighted_avg {'precision': 0.9065729024089156, 'recall': 0.8983200707338639, 'f1-score': 0.8993492933011918, 'support': 1131}
 
----------
Epoch 32/40
time = 273.66 secondes

Train loss 0.03458768821879712 accuracy 0.9933202862739563 macro_avg {'precision': 0.9932799816985195, 'recall': 0.993297238708751, 'f1-score': 0.9932778319690531, 'support': 10180} weighted_avg {'precision': 0.9933726437594124, 'recall': 0.9933202357563851, 'f1-score': 0.9933353766791319, 'support': 10180}
 
time = 6.33 secondes

Val loss 0.8209230712272471 accuracy 0.9000884294509888 macro_avg {'precision': 0.9089025711191251, 'recall': 0.9026393712431442, 'f1-score': 0.9027905386655046, 'support': 1131} weighted_avg {'precision': 0.9089723588301702, 'recall': 0.9000884173297966, 'f1-score': 0.901483429862932, 'support': 1131}
 
----------
Epoch 33/40
time = 262.68 secondes

Train loss 0.02862371812961596 accuracy 0.9942043423652649 macro_avg {'precision': 0.9944816104608714, 'recall': 0.9941365309684848, 'f1-score': 0.9942761594640492, 'support': 10180} weighted_avg {'precision': 0.9943133566710758, 'recall': 0.9942043222003929, 'f1-score': 0.994225352916464, 'support': 10180}
 
time = 5.51 secondes

Val loss 0.8116327364196608 accuracy 0.9080460071563721 macro_avg {'precision': 0.9142588168705018, 'recall': 0.9096550472959442, 'f1-score': 0.9097076622140007, 'support': 1131} weighted_avg {'precision': 0.9148225489906766, 'recall': 0.9080459770114943, 'f1-score': 0.9090973226349927, 'support': 1131}
 
----------
Epoch 34/40
time = 255.19 secondes

Train loss 0.02521852284292463 accuracy 0.9942043423652649 macro_avg {'precision': 0.9943079446592116, 'recall': 0.9941753379652468, 'f1-score': 0.9942305908738488, 'support': 10180} weighted_avg {'precision': 0.9942500748773712, 'recall': 0.9942043222003929, 'f1-score': 0.9942159465315707, 'support': 10180}
 
time = 5.66 secondes

Val loss 0.7971520368156862 accuracy 0.9080460071563721 macro_avg {'precision': 0.9113842177442665, 'recall': 0.9105245797907674, 'f1-score': 0.90960286907002, 'support': 1131} weighted_avg {'precision': 0.9119610047778623, 'recall': 0.9080459770114943, 'f1-score': 0.9086009590226772, 'support': 1131}
 
----------
Epoch 35/40
time = 276.28 secondes

Train loss 0.018866839215725708 accuracy 0.9951866865158081 macro_avg {'precision': 0.9951232633621314, 'recall': 0.9950397810292939, 'f1-score': 0.9950685264673617, 'support': 10180} weighted_avg {'precision': 0.9952266717737993, 'recall': 0.9951866404715127, 'f1-score': 0.9951928399244035, 'support': 10180}
 
time = 5.93 secondes

Val loss 0.7989277140160327 accuracy 0.9142352342605591 macro_avg {'precision': 0.9173509888765119, 'recall': 0.9160262924155391, 'f1-score': 0.9158541140914297, 'support': 1131} weighted_avg {'precision': 0.9168808775656581, 'recall': 0.9142351900972591, 'f1-score': 0.9146711219526091, 'support': 1131}
 
----------
Epoch 36/40
time = 263.36 secondes

Train loss 0.02088680397727426 accuracy 0.9951866865158081 macro_avg {'precision': 0.9954519481130799, 'recall': 0.9952458143698883, 'f1-score': 0.9953321567888818, 'support': 10180} weighted_avg {'precision': 0.9952480568007868, 'recall': 0.9951866404715127, 'f1-score': 0.9952000124525677, 'support': 10180}
 
time = 5.43 secondes

Val loss 0.8447996072806656 accuracy 0.9027409553527832 macro_avg {'precision': 0.9062595226531425, 'recall': 0.9058737963831083, 'f1-score': 0.9050719168720965, 'support': 1131} weighted_avg {'precision': 0.9062865891098957, 'recall': 0.9027409372236959, 'f1-score': 0.9034694407363894, 'support': 1131}
 
----------
Epoch 37/40
time = 265.57 secondes

Train loss 0.019834817239222976 accuracy 0.9956778287887573 macro_avg {'precision': 0.9959121865827083, 'recall': 0.9956057905005278, 'f1-score': 0.9957418564129732, 'support': 10180} weighted_avg {'precision': 0.9957291621589945, 'recall': 0.9956777996070727, 'f1-score': 0.9956858700314872, 'support': 10180}
 
time = 6.11 secondes

Val loss 0.8075849248731354 accuracy 0.9045093059539795 macro_avg {'precision': 0.907721606761308, 'recall': 0.907030334788311, 'f1-score': 0.9062837744646124, 'support': 1131} weighted_avg {'precision': 0.9084663001140592, 'recall': 0.9045092838196287, 'f1-score': 0.9053500296492316, 'support': 1131}
 
----------
Epoch 38/40
time = 272.95 secondes

Train loss 0.015428862647397804 accuracy 0.9960707426071167 macro_avg {'precision': 0.9963027291292768, 'recall': 0.9961585284586338, 'f1-score': 0.9962075505356163, 'support': 10180} weighted_avg {'precision': 0.9961577953783604, 'recall': 0.9960707269155207, 'f1-score': 0.996089595254898, 'support': 10180}
 
time = 5.70 secondes

Val loss 0.7738343446110865 accuracy 0.9098143577575684 macro_avg {'precision': 0.910970303896218, 'recall': 0.9094090055389298, 'f1-score': 0.90954912726053, 'support': 1131} weighted_avg {'precision': 0.911138252714741, 'recall': 0.9098143236074271, 'f1-score': 0.9098512444765454, 'support': 1131}
 
----------
Epoch 39/40
time = 257.69 secondes

Train loss 0.011648876173695304 accuracy 0.9961689710617065 macro_avg {'precision': 0.996380501976392, 'recall': 0.9961833137639962, 'f1-score': 0.9962734888983846, 'support': 10180} weighted_avg {'precision': 0.9962084242911655, 'recall': 0.9961689587426326, 'f1-score': 0.996180001730347, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.7964345079902377 accuracy 0.9160035848617554 macro_avg {'precision': 0.9186972026975244, 'recall': 0.9173179568329168, 'f1-score': 0.9168545876906293, 'support': 1131} weighted_avg {'precision': 0.9186686632276271, 'recall': 0.9160035366931919, 'f1-score': 0.9161764901430488, 'support': 1131}
 
----------
Epoch 40/40
time = 273.64 secondes

Train loss 0.007985838912719949 accuracy 0.9972495436668396 macro_avg {'precision': 0.9974308593084042, 'recall': 0.9973055308223422, 'f1-score': 0.9973463125709001, 'support': 10180} weighted_avg {'precision': 0.9973269233538239, 'recall': 0.9972495088408644, 'f1-score': 0.9972655025716816, 'support': 10180}
 
time = 5.90 secondes

Val loss 0.7818349600403681 accuracy 0.9160035848617554 macro_avg {'precision': 0.9185465000697303, 'recall': 0.9164383274432799, 'f1-score': 0.9164584660744604, 'support': 1131} weighted_avg {'precision': 0.9187100585206788, 'recall': 0.9160035366931919, 'f1-score': 0.9163272445950416, 'support': 1131}
 
----------
best_accuracy 0.9160035848617554 best_epoch 39 macro_avg {'precision': 0.9186972026975244, 'recall': 0.9173179568329168, 'f1-score': 0.9168545876906293, 'support': 1131} weighted_avg {'precision': 0.9186686632276271, 'recall': 0.9160035366931919, 'f1-score': 0.9161764901430488, 'support': 1131}

average train time 242.44419813752174

average val time 5.325443249940872
 
time = 40.70 secondes

test_accuracy 0.8568393588066101 macro_avg {'precision': 0.8518582980958938, 'recall': 0.8482212885774771, 'f1-score': 0.848456066680767, 'support': 7530} weighted_avg {'precision': 0.8584324309199101, 'recall': 0.8568393094289508, 'f1-score': 0.8561709381631089, 'support': 7530}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_tail_1
----------
Epoch 1/40
time = 260.89 secondes

Train loss 1.3489243150038097 accuracy 0.6430255770683289 macro_avg {'precision': 0.6421288794942742, 'recall': 0.627617718201417, 'f1-score': 0.6178057284980474, 'support': 10180} weighted_avg {'precision': 0.6509498122980395, 'recall': 0.6430255402750491, 'f1-score': 0.6319110890174966, 'support': 10180}
 
time = 5.71 secondes

Val loss 0.7758146591589484 accuracy 0.7745358347892761 macro_avg {'precision': 0.75000800233174, 'recall': 0.7602371843579887, 'f1-score': 0.7471252833912481, 'support': 1131} weighted_avg {'precision': 0.7606594079774615, 'recall': 0.7745358090185677, 'f1-score': 0.7608244911943329, 'support': 1131}
 
----------
Epoch 2/40
time = 252.42 secondes

Train loss 0.521368660874887 accuracy 0.8458743095397949 macro_avg {'precision': 0.8385298594102263, 'recall': 0.833408622153309, 'f1-score': 0.8303423101433663, 'support': 10180} weighted_avg {'precision': 0.8452379922203637, 'recall': 0.8458742632612967, 'f1-score': 0.8414598171832356, 'support': 10180}
 
time = 5.90 secondes

Val loss 0.5787412159039941 accuracy 0.8470380306243896 macro_avg {'precision': 0.8483921330110376, 'recall': 0.8412065545377521, 'f1-score': 0.8398355723705461, 'support': 1131} weighted_avg {'precision': 0.8554539937089262, 'recall': 0.8470380194518126, 'f1-score': 0.8462296612554556, 'support': 1131}
 
----------
Epoch 3/40
time = 267.30 secondes

Train loss 0.33317013077870644 accuracy 0.9026522636413574 macro_avg {'precision': 0.8952105299670295, 'recall': 0.8942552386056724, 'f1-score': 0.894223643688594, 'support': 10180} weighted_avg {'precision': 0.9017756803812275, 'recall': 0.9026522593320235, 'f1-score': 0.9017819106640604, 'support': 10180}
 
time = 5.96 secondes

Val loss 0.6776986181945868 accuracy 0.8213970065116882 macro_avg {'precision': 0.8365735688714768, 'recall': 0.8265020282609102, 'f1-score': 0.8186348264921259, 'support': 1131} weighted_avg {'precision': 0.8477642113309525, 'recall': 0.8213969938107869, 'f1-score': 0.8200390696817704, 'support': 1131}
 
----------
Epoch 4/40
time = 255.47 secondes

Train loss 0.255845244715116 accuracy 0.9268172979354858 macro_avg {'precision': 0.9208046528829978, 'recall': 0.9205095520887074, 'f1-score': 0.9205667624318961, 'support': 10180} weighted_avg {'precision': 0.9271619772355103, 'recall': 0.9268172888015717, 'f1-score': 0.9268979039378227, 'support': 10180}
 
time = 5.49 secondes

Val loss 0.6254286199388369 accuracy 0.8673740029335022 macro_avg {'precision': 0.8792591765729364, 'recall': 0.8637533153584229, 'f1-score': 0.8658131216191155, 'support': 1131} weighted_avg {'precision': 0.8825757846965222, 'recall': 0.8673740053050398, 'f1-score': 0.8695200148646032, 'support': 1131}
 
----------
Epoch 5/40
time = 256.80 secondes

Train loss 0.20657903508369457 accuracy 0.9433202743530273 macro_avg {'precision': 0.9392027986868456, 'recall': 0.9385458103494498, 'f1-score': 0.9387896667919573, 'support': 10180} weighted_avg {'precision': 0.9430749110385411, 'recall': 0.943320235756385, 'f1-score': 0.9431283476541924, 'support': 10180}
 
time = 5.62 secondes

Val loss 0.672461210638547 accuracy 0.8541114330291748 macro_avg {'precision': 0.8716026144745486, 'recall': 0.8444999840672773, 'f1-score': 0.8490088933331771, 'support': 1131} weighted_avg {'precision': 0.8753179829366801, 'recall': 0.8541114058355438, 'f1-score': 0.8568509750752331, 'support': 1131}
 
----------
Epoch 6/40
time = 262.11 secondes

Train loss 0.17028248679803704 accuracy 0.9548134207725525 macro_avg {'precision': 0.9522288088086311, 'recall': 0.9517152591918678, 'f1-score': 0.9519229210603992, 'support': 10180} weighted_avg {'precision': 0.9548098169774213, 'recall': 0.9548133595284872, 'f1-score': 0.9547642817073733, 'support': 10180}
 
time = 5.70 secondes

Val loss 0.6883938733560466 accuracy 0.8717948794364929 macro_avg {'precision': 0.878424179663788, 'recall': 0.8647061282609595, 'f1-score': 0.8641684931536682, 'support': 1131} weighted_avg {'precision': 0.8804767603934021, 'recall': 0.8717948717948718, 'f1-score': 0.870273454863472, 'support': 1131}
 
----------
Epoch 7/40
time = 256.77 secondes

Train loss 0.16186950531295827 accuracy 0.9609037637710571 macro_avg {'precision': 0.9589101317928843, 'recall': 0.9592509368761217, 'f1-score': 0.9590157995436174, 'support': 10180} weighted_avg {'precision': 0.9611629442939975, 'recall': 0.9609037328094302, 'f1-score': 0.9609778402334871, 'support': 10180}
 
time = 5.47 secondes

Val loss 0.7452749495892982 accuracy 0.8744474053382874 macro_avg {'precision': 0.8870712567436998, 'recall': 0.8708877699045511, 'f1-score': 0.870058914382289, 'support': 1131} weighted_avg {'precision': 0.887131825930881, 'recall': 0.874447391688771, 'f1-score': 0.8732031753903244, 'support': 1131}
 
----------
Epoch 8/40
time = 257.31 secondes

Train loss 0.14609922794159136 accuracy 0.9669941663742065 macro_avg {'precision': 0.965498855336565, 'recall': 0.9651873170664246, 'f1-score': 0.9653090036195202, 'support': 10180} weighted_avg {'precision': 0.9670849877664451, 'recall': 0.9669941060903733, 'f1-score': 0.9670052567446684, 'support': 10180}
 
time = 5.78 secondes

Val loss 0.7225840872868529 accuracy 0.8850575089454651 macro_avg {'precision': 0.8889468285730352, 'recall': 0.883290609237523, 'f1-score': 0.8841883676728163, 'support': 1131} weighted_avg {'precision': 0.8896215248192892, 'recall': 0.8850574712643678, 'f1-score': 0.8854260937751661, 'support': 1131}
 
----------
Epoch 9/40
time = 264.11 secondes

Train loss 0.127357701207343 accuracy 0.9706287384033203 macro_avg {'precision': 0.9693280663840769, 'recall': 0.9689698939772136, 'f1-score': 0.9691219976184067, 'support': 10180} weighted_avg {'precision': 0.9707056290739128, 'recall': 0.9706286836935167, 'f1-score': 0.9706409126181181, 'support': 10180}
 
time = 5.49 secondes

Val loss 0.8895643739346218 accuracy 0.8709107041358948 macro_avg {'precision': 0.875048930939213, 'recall': 0.8701097851238876, 'f1-score': 0.8689064108419371, 'support': 1131} weighted_avg {'precision': 0.8768840070323547, 'recall': 0.8709106984969054, 'f1-score': 0.8705144074429776, 'support': 1131}
 
----------
Epoch 10/40
time = 248.89 secondes

Train loss 0.13171681130604287 accuracy 0.9712181091308594 macro_avg {'precision': 0.9695308821620265, 'recall': 0.9695057654292751, 'f1-score': 0.9694699342234332, 'support': 10180} weighted_avg {'precision': 0.9712888962004141, 'recall': 0.9712180746561886, 'f1-score': 0.9712047308802267, 'support': 10180}
 
time = 5.59 secondes

Val loss 0.9008327582080415 accuracy 0.8682581782341003 macro_avg {'precision': 0.8777686010529651, 'recall': 0.8675006510766492, 'f1-score': 0.8684263234655301, 'support': 1131} weighted_avg {'precision': 0.8796988120918926, 'recall': 0.8682581786030061, 'f1-score': 0.8697964434491892, 'support': 1131}
 
----------
Epoch 11/40
time = 269.85 secondes

Train loss 0.14095627630899407 accuracy 0.9704322814941406 macro_avg {'precision': 0.9697194637314457, 'recall': 0.9691610341797071, 'f1-score': 0.9693789172296354, 'support': 10180} weighted_avg {'precision': 0.9705989528786433, 'recall': 0.9704322200392927, 'f1-score': 0.9704540010398871, 'support': 10180}
 
time = 5.73 secondes

Val loss 1.0943122183007161 accuracy 0.8479222059249878 macro_avg {'precision': 0.8609070938752913, 'recall': 0.8445379342824794, 'f1-score': 0.8439568283937138, 'support': 1131} weighted_avg {'precision': 0.8673195651383739, 'recall': 0.847922192749779, 'f1-score': 0.8496813027368092, 'support': 1131}
 
----------
Epoch 12/40
time = 261.62 secondes

Train loss 0.10980969944565296 accuracy 0.9769155383110046 macro_avg {'precision': 0.9758003116921978, 'recall': 0.9750787929950515, 'f1-score': 0.9753798646741132, 'support': 10180} weighted_avg {'precision': 0.9769894327766383, 'recall': 0.9769155206286837, 'f1-score': 0.9768972719478504, 'support': 10180}
 
time = 5.53 secondes

Val loss 1.0196869289686963 accuracy 0.8488063812255859 macro_avg {'precision': 0.883451388958591, 'recall': 0.8319673666350363, 'f1-score': 0.8267229579123818, 'support': 1131} weighted_avg {'precision': 0.8820107856920495, 'recall': 0.8488063660477454, 'f1-score': 0.8427118006217226, 'support': 1131}
 
----------
Epoch 13/40
time = 249.94 secondes

Train loss 0.1128355105254204 accuracy 0.9773085117340088 macro_avg {'precision': 0.9767954944489647, 'recall': 0.9756822156304313, 'f1-score': 0.9761394215462909, 'support': 10180} weighted_avg {'precision': 0.9773298020469027, 'recall': 0.9773084479371317, 'f1-score': 0.9772386564950272, 'support': 10180}
 
time = 5.80 secondes

Val loss 0.8960464345549503 accuracy 0.8770999312400818 macro_avg {'precision': 0.8894786579019118, 'recall': 0.8763326275085029, 'f1-score': 0.8784585679051634, 'support': 1131} weighted_avg {'precision': 0.8898884062470683, 'recall': 0.8770999115826702, 'f1-score': 0.8789525042824008, 'support': 1131}
 
----------
Epoch 14/40
time = 266.61 secondes

Train loss 0.10030724507144939 accuracy 0.9798625111579895 macro_avg {'precision': 0.9791817066232756, 'recall': 0.9793281664785841, 'f1-score': 0.979218995032598, 'support': 10180} weighted_avg {'precision': 0.9799695932203086, 'recall': 0.9798624754420432, 'f1-score': 0.9798797325624639, 'support': 10180}
 
time = 5.97 secondes

Val loss 1.125699181934617 accuracy 0.8629531860351562 macro_avg {'precision': 0.8780962420504835, 'recall': 0.8642917652792729, 'f1-score': 0.8641117295572002, 'support': 1131} weighted_avg {'precision': 0.8799734849164909, 'recall': 0.8629531388152077, 'f1-score': 0.8648294550014092, 'support': 1131}
 
----------
Epoch 15/40
time = 258.12 secondes

Train loss 0.11156589062200944 accuracy 0.97956782579422 macro_avg {'precision': 0.9795319670838589, 'recall': 0.9790922798101986, 'f1-score': 0.9792713301731559, 'support': 10180} weighted_avg {'precision': 0.9796431421932243, 'recall': 0.9795677799607073, 'f1-score': 0.9795672968623881, 'support': 10180}
 
time = 5.58 secondes

Val loss 0.81810059694247 accuracy 0.8885942101478577 macro_avg {'precision': 0.8908429711693246, 'recall': 0.8900717827476585, 'f1-score': 0.8893721345769612, 'support': 1131} weighted_avg {'precision': 0.8930165866631322, 'recall': 0.8885941644562334, 'f1-score': 0.8896367475421361, 'support': 1131}
 
----------
Epoch 16/40
time = 249.88 secondes

Train loss 0.09246844603550135 accuracy 0.9805501103401184 macro_avg {'precision': 0.979494382709808, 'recall': 0.9796676349250607, 'f1-score': 0.9795413756838259, 'support': 10180} weighted_avg {'precision': 0.980649905375814, 'recall': 0.9805500982318271, 'f1-score': 0.9805641489460691, 'support': 10180}
 
time = 5.76 secondes

Val loss 0.9968213571076081 accuracy 0.8735632300376892 macro_avg {'precision': 0.8825128757582948, 'recall': 0.8682393284729182, 'f1-score': 0.872330319722634, 'support': 1131} weighted_avg {'precision': 0.8782025926929167, 'recall': 0.8735632183908046, 'f1-score': 0.873183065405861, 'support': 1131}
 
----------
Epoch 17/40
time = 268.46 secondes

Train loss 0.09139926127953824 accuracy 0.9825147986412048 macro_avg {'precision': 0.9825634708924117, 'recall': 0.9819122690739563, 'f1-score': 0.9821866367454017, 'support': 10180} weighted_avg {'precision': 0.9825751952251324, 'recall': 0.9825147347740668, 'f1-score': 0.9824962575304943, 'support': 10180}
 
time = 6.01 secondes

Val loss 0.9749284598342648 accuracy 0.8824049830436707 macro_avg {'precision': 0.8894411560592751, 'recall': 0.8823937404856419, 'f1-score': 0.8836107777059322, 'support': 1131} weighted_avg {'precision': 0.888099896738047, 'recall': 0.8824049513704686, 'f1-score': 0.8829504098207234, 'support': 1131}
 
----------
Epoch 18/40
time = 256.23 secondes

Train loss 0.08686089571830842 accuracy 0.9827112555503845 macro_avg {'precision': 0.9819152627215549, 'recall': 0.9821416728448987, 'f1-score': 0.9820004040409083, 'support': 10180} weighted_avg {'precision': 0.9827981628575785, 'recall': 0.9827111984282908, 'f1-score': 0.9827271615041061, 'support': 10180}
 
time = 5.40 secondes

Val loss 1.0246888230895808 accuracy 0.8682581782341003 macro_avg {'precision': 0.8796054314920718, 'recall': 0.8615121442793752, 'f1-score': 0.8656352130807099, 'support': 1131} weighted_avg {'precision': 0.8754061378068387, 'recall': 0.8682581786030061, 'f1-score': 0.867751566797979, 'support': 1131}
 
----------
Epoch 19/40
time = 257.73 secondes

Train loss 0.07095039073642204 accuracy 0.9850687980651855 macro_avg {'precision': 0.9849483549875346, 'recall': 0.9844255471296979, 'f1-score': 0.9846673895830357, 'support': 10180} weighted_avg {'precision': 0.9851177200917575, 'recall': 0.9850687622789784, 'f1-score': 0.9850752768426996, 'support': 10180}
 
time = 5.76 secondes

Val loss 0.932970997787853 accuracy 0.8850575089454651 macro_avg {'precision': 0.8900006524080639, 'recall': 0.8856032437631555, 'f1-score': 0.8860990150664115, 'support': 1131} weighted_avg {'precision': 0.889187897909242, 'recall': 0.8850574712643678, 'f1-score': 0.8855394251248927, 'support': 1131}
 
----------
Epoch 20/40
time = 266.31 secondes

Train loss 0.08086720255064742 accuracy 0.9840864539146423 macro_avg {'precision': 0.984225440315796, 'recall': 0.98412945120714, 'f1-score': 0.9841606624702999, 'support': 10180} weighted_avg {'precision': 0.9841535944108427, 'recall': 0.9840864440078585, 'f1-score': 0.9841027851524513, 'support': 10180}
 
time = 5.68 secondes

Val loss 0.8382557904065757 accuracy 0.8868258595466614 macro_avg {'precision': 0.8937517136811677, 'recall': 0.8864329898759735, 'f1-score': 0.8889319860648108, 'support': 1131} weighted_avg {'precision': 0.8909117475779937, 'recall': 0.8868258178603006, 'f1-score': 0.8876983903234494, 'support': 1131}
 
----------
Epoch 21/40
time = 254.63 secondes

Train loss 0.06781369714104125 accuracy 0.9856581687927246 macro_avg {'precision': 0.984882660550306, 'recall': 0.9849018074520508, 'f1-score': 0.9848754009762463, 'support': 10180} weighted_avg {'precision': 0.9856840769055824, 'recall': 0.9856581532416503, 'f1-score': 0.9856552798096522, 'support': 10180}
 
time = 5.85 secondes

Val loss 0.9620458739951112 accuracy 0.8859416842460632 macro_avg {'precision': 0.8867773504875327, 'recall': 0.8876109730846732, 'f1-score': 0.8855784410607942, 'support': 1131} weighted_avg {'precision': 0.8912661499655908, 'recall': 0.8859416445623343, 'f1-score': 0.8871171983485193, 'support': 1131}
 
----------
Epoch 22/40
time = 257.35 secondes

Train loss 0.06566729069553055 accuracy 0.9868369698524475 macro_avg {'precision': 0.9864934286855072, 'recall': 0.9866543366610081, 'f1-score': 0.986556726314355, 'support': 10180} weighted_avg {'precision': 0.9869121948662557, 'recall': 0.9868369351669941, 'f1-score': 0.986857794928184, 'support': 10180}
 
time = 6.37 secondes

Val loss 0.9912984138118959 accuracy 0.8806366324424744 macro_avg {'precision': 0.8854272215162717, 'recall': 0.8801815650527247, 'f1-score': 0.8807829856767464, 'support': 1131} weighted_avg {'precision': 0.8847451266936038, 'recall': 0.8806366047745358, 'f1-score': 0.8807622955990803, 'support': 1131}
 
----------
Epoch 23/40
time = 263.53 secondes

Train loss 0.07242751247508143 accuracy 0.986542284488678 macro_avg {'precision': 0.9863646840677897, 'recall': 0.9865145720307883, 'f1-score': 0.9864053564130575, 'support': 10180} weighted_avg {'precision': 0.9866622110660136, 'recall': 0.9865422396856581, 'f1-score': 0.9865687222719681, 'support': 10180}
 
time = 5.47 secondes

Val loss 0.9652780971344738 accuracy 0.8859416842460632 macro_avg {'precision': 0.8917685696217206, 'recall': 0.8872248046355897, 'f1-score': 0.8866282462366802, 'support': 1131} weighted_avg {'precision': 0.8914695411922474, 'recall': 0.8859416445623343, 'f1-score': 0.8860249671710755, 'support': 1131}
 
----------
Epoch 24/40
time = 246.16 secondes

Train loss 0.054639035644905395 accuracy 0.9888015985488892 macro_avg {'precision': 0.9887712437095244, 'recall': 0.9882845994856163, 'f1-score': 0.9884972629714017, 'support': 10180} weighted_avg {'precision': 0.9888922767354094, 'recall': 0.9888015717092338, 'f1-score': 0.9888173555742619, 'support': 10180}
 
time = 5.30 secondes

Val loss 0.8393578696829959 accuracy 0.8974359035491943 macro_avg {'precision': 0.9023467855831852, 'recall': 0.8967685802892129, 'f1-score': 0.8982957864187675, 'support': 1131} weighted_avg {'precision': 0.9010553049962307, 'recall': 0.8974358974358975, 'f1-score': 0.8980198609313952, 'support': 1131}
 
----------
Epoch 25/40
time = 265.32 secondes

Train loss 0.051449816830027376 accuracy 0.9894892573356628 macro_avg {'precision': 0.989058708389335, 'recall': 0.9892457895371984, 'f1-score': 0.989129213326469, 'support': 10180} weighted_avg {'precision': 0.9895576767130893, 'recall': 0.9894891944990177, 'f1-score': 0.9895003300976426, 'support': 10180}
 
time = 5.77 secondes

Val loss 0.8805852823965145 accuracy 0.8894783854484558 macro_avg {'precision': 0.8912707669471679, 'recall': 0.8901857226041727, 'f1-score': 0.8900789217994978, 'support': 1131} weighted_avg {'precision': 0.8918943106089748, 'recall': 0.8894783377541998, 'f1-score': 0.8900207211252017, 'support': 1131}
 
----------
Epoch 26/40
time = 259.52 secondes

Train loss 0.05084770732452626 accuracy 0.9892927408218384 macro_avg {'precision': 0.9890248571982931, 'recall': 0.9890405044818149, 'f1-score': 0.9890206076678458, 'support': 10180} weighted_avg {'precision': 0.9893460783752409, 'recall': 0.9892927308447937, 'f1-score': 0.9893070732703951, 'support': 10180}
 
time = 5.55 secondes

Val loss 0.9315305006036307 accuracy 0.8965517282485962 macro_avg {'precision': 0.9015298198202115, 'recall': 0.896769279738235, 'f1-score': 0.8973888811980236, 'support': 1131} weighted_avg {'precision': 0.902968738639918, 'recall': 0.896551724137931, 'f1-score': 0.898132012489229, 'support': 1131}
 
----------
Epoch 27/40
time = 250.34 secondes

Train loss 0.04783191157995181 accuracy 0.9904715418815613 macro_avg {'precision': 0.9905982652115032, 'recall': 0.9902803853682383, 'f1-score': 0.9904275346971444, 'support': 10180} weighted_avg {'precision': 0.9905062993100263, 'recall': 0.9904715127701376, 'f1-score': 0.9904784120618537, 'support': 10180}
 
time = 5.55 secondes

Val loss 0.9155300832203401 accuracy 0.8974359035491943 macro_avg {'precision': 0.9015185003674647, 'recall': 0.8992477884753848, 'f1-score': 0.8987314937505471, 'support': 1131} weighted_avg {'precision': 0.9030542815295869, 'recall': 0.8974358974358975, 'f1-score': 0.8985162753516055, 'support': 1131}
 
----------
Epoch 28/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 9.24 GiB already allocated; 23.31 MiB free; 9.32 GiB reserved in total by PyTorch)
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
ECtHR_BERT_head_bert_summarizer_1
----------
Epoch 1/40
time = 211.84 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Train loss 0.27693274344007174 micro_f1_score 0.5792753288714517 
 
time = 6.87 secondes

Val loss 0.23137662352108565 micro_f1_score 0.649164677804296
 
----------
Epoch 2/40
time = 208.98 secondes

Train loss 0.1789005107405755 micro_f1_score 0.7570035789213871 
 
time = 6.54 secondes

Val loss 0.20647304798247385 micro_f1_score 0.6981647793830535
 
----------
Epoch 3/40
time = 205.65 secondes

Train loss 0.1530407963605883 micro_f1_score 0.7998862759433004 
 
time = 6.76 secondes

Val loss 0.20594409914290318 micro_f1_score 0.7159090909090909
 
----------
Epoch 4/40
time = 209.24 secondes

Train loss 0.13718582137009583 micro_f1_score 0.8249052342930882 
 
time = 6.73 secondes

Val loss 0.18917556620035014 micro_f1_score 0.7497142857142857
 
----------
Epoch 5/40
time = 210.84 secondes

Train loss 0.1240305308621746 micro_f1_score 0.8464440520147697 
 
time = 6.58 secondes

Val loss 0.2062300065013229 micro_f1_score 0.7449664429530202
 
----------
Epoch 6/40
time = 209.16 secondes

Train loss 0.1109794232222411 micro_f1_score 0.8673971732571066 
 
time = 6.56 secondes

Val loss 0.208324346874581 micro_f1_score 0.744868035190616
 
----------
Epoch 7/40
time = 206.30 secondes

Train loss 0.10151084632285544 micro_f1_score 0.8807863031071654 
 
time = 6.61 secondes

Val loss 0.22045229705142194 micro_f1_score 0.737913486005089
 
----------
Epoch 8/40
time = 207.75 secondes

Train loss 0.09226954154636677 micro_f1_score 0.8946021496909325 
 
time = 6.53 secondes

Val loss 0.2159393614799273 micro_f1_score 0.7477114610032956
 
----------
Epoch 9/40
time = 206.28 secondes

Train loss 0.08420579892893633 micro_f1_score 0.9066050301653217 
 
time = 6.54 secondes

Val loss 0.21897992058122745 micro_f1_score 0.7594287806664226
 
----------
Epoch 10/40
time = 206.20 secondes

Train loss 0.07610385683742729 micro_f1_score 0.9183186705767351 
 
time = 6.56 secondes

Val loss 0.23639539989535927 micro_f1_score 0.7526881720430108
 
----------
Epoch 11/40
time = 207.84 secondes

Train loss 0.06809303061617118 micro_f1_score 0.9285325029194239 
 
time = 7.50 secondes

Val loss 0.2501993592767442 micro_f1_score 0.7491833030852995
 
----------
Epoch 12/40
time = 207.87 secondes

Train loss 0.0622563411552996 micro_f1_score 0.9340050377833753 
 
time = 6.53 secondes

Val loss 0.2650585666787429 micro_f1_score 0.7539190667152752
 
----------
Epoch 13/40
time = 208.15 secondes

Train loss 0.055928544890605385 micro_f1_score 0.9423998762950363 
 
time = 6.58 secondes

Val loss 0.2735705347579034 micro_f1_score 0.7547440028643037
 
----------
Epoch 14/40
time = 209.19 secondes

Train loss 0.04946158227326164 micro_f1_score 0.9494247548451856 
 
time = 6.52 secondes

Val loss 0.307858055121586 micro_f1_score 0.7366926898509581
 
----------
Epoch 15/40
time = 204.78 secondes

Train loss 0.04358832592916515 micro_f1_score 0.9558517605362508 
 
time = 6.53 secondes

Val loss 0.3144773919990317 micro_f1_score 0.7447272727272727
 
----------
Epoch 16/40
time = 208.18 secondes

Train loss 0.0402016495908233 micro_f1_score 0.9600030737311255 
 
time = 6.52 secondes

Val loss 0.317219933036898 micro_f1_score 0.747991234477721
 
----------
Epoch 17/40
time = 208.75 secondes

Train loss 0.03410195787332382 micro_f1_score 0.9644529750479846 
 
time = 6.55 secondes

Val loss 0.3422920639153387 micro_f1_score 0.7431616341030195
 
----------
Epoch 18/40
time = 205.00 secondes

Train loss 0.03146390440949314 micro_f1_score 0.968165585038977 
 
time = 7.52 secondes

Val loss 0.3389551463368975 micro_f1_score 0.7524752475247525
 
----------
Epoch 19/40
time = 209.03 secondes

Train loss 0.027396350205133393 micro_f1_score 0.9726247987117552 
 
time = 6.54 secondes

Val loss 0.36098746491260214 micro_f1_score 0.7571225071225072
 
----------
Epoch 20/40
time = 208.83 secondes

Train loss 0.025207039473372772 micro_f1_score 0.9745808131077253 
 
time = 6.54 secondes

Val loss 0.3622348331769959 micro_f1_score 0.7553930530164532
 
----------
Epoch 21/40
time = 206.02 secondes

Train loss 0.023653651017859278 micro_f1_score 0.974766783911913 
 
time = 6.54 secondes

Val loss 0.39210509713433805 micro_f1_score 0.7499999999999999
 
----------
Epoch 22/40
time = 207.28 secondes

Train loss 0.021151003654236562 micro_f1_score 0.9785883612449339 
 
time = 6.58 secondes

Val loss 0.3826869484342513 micro_f1_score 0.7569692637598283
 
----------
Epoch 23/40
time = 209.11 secondes

Train loss 0.01809977278546514 micro_f1_score 0.9816068219188558 
 
time = 6.54 secondes

Val loss 0.40683677218488007 micro_f1_score 0.7516826071555084
 
----------
Epoch 24/40
time = 209.45 secondes

Train loss 0.016157582725780002 micro_f1_score 0.9843279313632031 
 
time = 6.59 secondes

Val loss 0.41627289305944914 micro_f1_score 0.7524254401724758
 
----------
Epoch 25/40
time = 206.40 secondes

Train loss 0.016185353779672385 micro_f1_score 0.982902068544386 
 
time = 6.57 secondes

Val loss 0.40656769092454287 micro_f1_score 0.7592796765894891
 
----------
Epoch 26/40
time = 207.21 secondes

Train loss 0.014553340998407965 micro_f1_score 0.9853350093322667 
 
time = 6.50 secondes

Val loss 0.43382633391950953 micro_f1_score 0.751701898960946
 
----------
Epoch 27/40
time = 205.50 secondes

Train loss 0.014453501110111976 micro_f1_score 0.9853933869799015 
 
time = 6.52 secondes

Val loss 0.4303549219106064 micro_f1_score 0.7639796659404502
 
----------
Epoch 28/40
time = 206.06 secondes

Train loss 0.012688410585666986 micro_f1_score 0.987719298245614 
 
time = 6.51 secondes

Val loss 0.42558903094442163 micro_f1_score 0.7584208620065194
 
----------
Epoch 29/40
time = 207.07 secondes

Train loss 0.010975452359471583 micro_f1_score 0.9895579268292682 
 
time = 6.51 secondes

Val loss 0.4676600120595244 micro_f1_score 0.7584973166368515
 
----------
Epoch 30/40
time = 208.91 secondes

Train loss 0.010652740920635846 micro_f1_score 0.9888812733226714 
 
time = 6.55 secondes

Val loss 0.45024423105794875 micro_f1_score 0.7567949170490647
 
----------
Epoch 31/40
time = 206.11 secondes

Train loss 0.00842146866697648 micro_f1_score 0.9917720554624408 
 
time = 6.65 secondes

Val loss 0.4630919892768391 micro_f1_score 0.7570162481536189
 
----------
Epoch 32/40
time = 207.84 secondes

Train loss 0.008526823751175848 micro_f1_score 0.9913599512807825 
 
time = 6.61 secondes

Val loss 0.4685921686106041 micro_f1_score 0.7656934306569344
 
----------
Epoch 33/40
time = 205.84 secondes

Train loss 0.007384206037589523 micro_f1_score 0.9928813430279037 
 
time = 6.58 secondes

Val loss 0.4709668418423074 micro_f1_score 0.7609403254972875
 
----------
Epoch 34/40
time = 208.02 secondes

Train loss 0.006744798179551999 micro_f1_score 0.9929596224835406 
 
time = 6.56 secondes

Val loss 0.49013624083800394 micro_f1_score 0.7567567567567568
 
----------
Epoch 35/40
time = 206.00 secondes

Train loss 0.006055769168820047 micro_f1_score 0.9937647327199451 
 
time = 6.55 secondes

Val loss 0.48418414763739853 micro_f1_score 0.7581412367361874
 
----------
Epoch 36/40
time = 207.92 secondes

Train loss 0.005235421222050308 micro_f1_score 0.9949041679342867 
 
time = 6.53 secondes

Val loss 0.5131294947178637 micro_f1_score 0.7514164305949008
 
----------
Epoch 37/40
time = 208.22 secondes

Train loss 0.004721062755069815 micro_f1_score 0.9954396898989131 
 
time = 6.59 secondes

Val loss 0.5046022375587558 micro_f1_score 0.7555233611010503
 
----------
Epoch 38/40
time = 208.99 secondes

Train loss 0.003901778888515158 micro_f1_score 0.9963119273031443 
 
time = 6.55 secondes

Val loss 0.5154605444337501 micro_f1_score 0.7562225475841874
 
----------
Epoch 39/40
time = 206.07 secondes

Train loss 0.0037507598148508107 micro_f1_score 0.9967300380228137 
 
time = 6.54 secondes

Val loss 0.505447371450604 micro_f1_score 0.7574221578566257
 
----------
Epoch 40/40
time = 207.97 secondes

Train loss 0.0028116819971064863 micro_f1_score 0.9973396169048343 
 
time = 6.52 secondes

Val loss 0.5147865253393767 micro_f1_score 0.7588447653429603
 
----------
best_f1_socre 0.7656934306569344 best_epoch 32

average train time 207.64586367607117

average val time 6.61764897108078
 
time = 7.01 secondes

test_f1_score 0.7548093739069606

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_head_text_rank_1
----------
Epoch 1/40
time = 197.38 secondes

Train loss 0.2762619513179268 micro_f1_score 0.58467464575595 
 
time = 6.56 secondes

Val loss 0.24799804609329973 micro_f1_score 0.6003992015968065
 
----------
Epoch 2/40
time = 202.39 secondes

Train loss 0.17873171690750767 micro_f1_score 0.756486943607937 
 
time = 6.55 secondes

Val loss 0.2086623263407926 micro_f1_score 0.7200938232994528
 
----------
Epoch 3/40
time = 202.47 secondes

Train loss 0.15353152432420233 micro_f1_score 0.8001302348296773 
 
time = 6.57 secondes

Val loss 0.19893138611414393 micro_f1_score 0.7283326855810338
 
----------
Epoch 4/40
time = 200.45 secondes

Train loss 0.13534142524003984 micro_f1_score 0.8288331920732939 
 
time = 6.59 secondes

Val loss 0.2063323496306529 micro_f1_score 0.7345099511828764
 
----------
Epoch 5/40
time = 202.09 secondes

Train loss 0.12125847867043975 micro_f1_score 0.8532597833473239 
 
time = 6.55 secondes

Val loss 0.2115980130238611 micro_f1_score 0.7331821617535903
 
----------
Epoch 6/40
time = 203.20 secondes

Train loss 0.1093585743090591 micro_f1_score 0.8701888596700932 
 
time = 6.57 secondes

Val loss 0.21503107616158781 micro_f1_score 0.743289224952741
 
----------
Epoch 7/40
time = 203.91 secondes

Train loss 0.0982157685156274 micro_f1_score 0.8834782608695653 
 
time = 6.56 secondes

Val loss 0.226040066998513 micro_f1_score 0.7478711588300629
 
----------
Epoch 8/40
time = 200.16 secondes

Train loss 0.0880708124312396 micro_f1_score 0.9001732010706975 
 
time = 6.65 secondes

Val loss 0.22891921586677677 micro_f1_score 0.753731343283582
 
----------
Epoch 9/40
time = 202.96 secondes

Train loss 0.07827907671547822 micro_f1_score 0.9139726027397261 
 
time = 6.54 secondes

Val loss 0.23792911308710693 micro_f1_score 0.7515642252484358
 
----------
Epoch 10/40
time = 201.72 secondes

Train loss 0.07083282879897737 micro_f1_score 0.9224582227081056 
 
time = 6.61 secondes

Val loss 0.2684030641542106 micro_f1_score 0.7414238288454444
 
----------
Epoch 11/40
time = 200.88 secondes

Train loss 0.06202050073844221 micro_f1_score 0.9321756735710947 
 
time = 6.59 secondes

Val loss 0.2722122564178998 micro_f1_score 0.742184626700993
 
----------
Epoch 12/40
time = 202.00 secondes

Train loss 0.05628689066087408 micro_f1_score 0.9404859086491739 
 
time = 6.56 secondes

Val loss 0.2822285612098506 micro_f1_score 0.745314222712238
 
----------
Epoch 13/40
time = 202.03 secondes

Train loss 0.0484698953039877 micro_f1_score 0.9505624057825365 
 
time = 6.58 secondes

Val loss 0.30962306468701756 micro_f1_score 0.7343029443064917
 
----------
Epoch 14/40
time = 202.87 secondes

Train loss 0.04337510795600981 micro_f1_score 0.9546823129776886 
 
time = 6.57 secondes

Val loss 0.3160516336315968 micro_f1_score 0.7497275699237196
 
----------
Epoch 15/40
time = 194.45 secondes

Train loss 0.038931360288506356 micro_f1_score 0.959322033898305 
 
time = 6.54 secondes

Val loss 0.32559551652826246 micro_f1_score 0.748159057437408
 
----------
Epoch 16/40
time = 202.76 secondes

Train loss 0.03378992780124315 micro_f1_score 0.9654377880184332 
 
time = 6.81 secondes

Val loss 0.3471229385401382 micro_f1_score 0.7389903329752954
 
----------
Epoch 17/40
time = 200.96 secondes

Train loss 0.029752392571646014 micro_f1_score 0.9693670206648008 
 
time = 6.53 secondes

Val loss 0.36394073582086406 micro_f1_score 0.7474023647438195
 
----------
Epoch 18/40
time = 199.32 secondes

Train loss 0.027346421010804842 micro_f1_score 0.971266569611524 
 
time = 6.54 secondes

Val loss 0.3921560127227033 micro_f1_score 0.7441197434069851
 
----------
Epoch 19/40
time = 202.32 secondes

Train loss 0.024445906515770197 micro_f1_score 0.9750316419284317 
 
time = 6.57 secondes

Val loss 0.36841138354578956 micro_f1_score 0.7534297367445311
 
----------
Epoch 20/40
time = 200.27 secondes

Train loss 0.022721878004098847 micro_f1_score 0.9767691071223544 
 
time = 6.55 secondes

Val loss 0.4091228642424599 micro_f1_score 0.7293693693693695
 
----------
Epoch 21/40
time = 203.47 secondes

Train loss 0.020619115041834217 micro_f1_score 0.9798852772466541 
 
time = 6.53 secondes

Val loss 0.4105759327773188 micro_f1_score 0.748014440433213
 
----------
Epoch 22/40
time = 202.37 secondes

Train loss 0.017723473922982978 micro_f1_score 0.9821182943603851 
 
time = 6.56 secondes

Val loss 0.4152697178184009 micro_f1_score 0.7470398277717977
 
----------
Epoch 23/40
time = 202.56 secondes

Train loss 0.015711729552152123 micro_f1_score 0.9833689350015259 
 
time = 6.61 secondes

Val loss 0.4227143502870544 micro_f1_score 0.7467046669041681
 
----------
Epoch 24/40
time = 200.69 secondes

Train loss 0.015044160028025487 micro_f1_score 0.9847971042103257 
 
time = 6.58 secondes

Val loss 0.43745805445264596 micro_f1_score 0.7470398277717977
 
----------
Epoch 25/40
time = 203.49 secondes

Train loss 0.015254453497651444 micro_f1_score 0.9844804575786463 
 
time = 6.57 secondes

Val loss 0.426048250960522 micro_f1_score 0.761521580102414
 
----------
Epoch 26/40
time = 202.36 secondes

Train loss 0.011950225714243569 micro_f1_score 0.98648700087549 
 
time = 6.54 secondes

Val loss 0.4575942862229269 micro_f1_score 0.7588677893228234
 
----------
Epoch 27/40
time = 203.23 secondes

Train loss 0.011258864102876999 micro_f1_score 0.9888257503527708 
 
time = 6.53 secondes

Val loss 0.46017516465460667 micro_f1_score 0.7547723935389133
 
----------
Epoch 28/40
time = 199.57 secondes

Train loss 0.010630674846391698 micro_f1_score 0.9882702414502246 
 
time = 6.53 secondes

Val loss 0.4719524486143081 micro_f1_score 0.7515505290040132
 
----------
Epoch 29/40
time = 203.24 secondes

Train loss 0.010125607135436898 micro_f1_score 0.9903754707650169 
 
time = 6.59 secondes

Val loss 0.4863837567753479 micro_f1_score 0.7549336203803374
 
----------
Epoch 30/40
time = 203.79 secondes

Train loss 0.008650975276117327 micro_f1_score 0.9915896030749325 
 
time = 6.56 secondes

Val loss 0.5035125583288123 micro_f1_score 0.7503639010189229
 
----------
Epoch 31/40
time = 200.80 secondes

Train loss 0.008281836482710131 micro_f1_score 0.9918569254185693 
 
time = 6.57 secondes

Val loss 0.5082694406880707 micro_f1_score 0.7526652452025586
 
----------
Epoch 32/40
time = 202.63 secondes

Train loss 0.007466070559284826 micro_f1_score 0.9924320212968245 
 
time = 6.55 secondes

Val loss 0.509487438885892 micro_f1_score 0.7565836298932385
 
----------
Epoch 33/40
time = 202.43 secondes

Train loss 0.006405078329453727 micro_f1_score 0.9931548524490417 
 
time = 6.53 secondes

Val loss 0.5064237078682321 micro_f1_score 0.7521367521367521
 
----------
Epoch 34/40
time = 200.91 secondes

Train loss 0.005120639858760442 micro_f1_score 0.9944881590451211 
 
time = 6.56 secondes

Val loss 0.5518830396601411 micro_f1_score 0.7398431931575197
 
----------
Epoch 35/40
time = 202.18 secondes

Train loss 0.005100896539500412 micro_f1_score 0.9947488584474885 
 
time = 6.71 secondes

Val loss 0.5625226475664826 micro_f1_score 0.7474120082815735
 
----------
Epoch 36/40
time = 202.82 secondes

Train loss 0.0039048502935337317 micro_f1_score 0.9959711136450019 
 
time = 6.58 secondes

Val loss 0.5286277034243599 micro_f1_score 0.7577030812324931
 
----------
Epoch 37/40
time = 204.62 secondes

Train loss 0.0036016808403383856 micro_f1_score 0.996275746750779 
 
time = 6.57 secondes

Val loss 0.5400807312766059 micro_f1_score 0.7570661896243293
 
----------
Epoch 38/40
time = 245.16 secondes

Train loss 0.0026728696157701675 micro_f1_score 0.9976815780472047 
 
time = 11.10 secondes

Val loss 0.550419141034611 micro_f1_score 0.7586948727142345
 
----------
Epoch 39/40
time = 252.68 secondes

Train loss 0.002323460545163913 micro_f1_score 0.9977203647416414 
 
time = 9.39 secondes

Val loss 0.5426670620675946 micro_f1_score 0.7575539568345323
 
----------
Epoch 40/40
time = 247.76 secondes

Train loss 0.0015882211804850372 micro_f1_score 0.9983289023927079 
 
time = 7.48 secondes

Val loss 0.5718885298635139 micro_f1_score 0.751592356687898
 
----------
best_f1_socre 0.761521580102414 best_epoch 25

average train time 205.28354244828225

average val time 6.780291563272476
 
time = 7.51 secondes

test_f1_score 0.7569273938968784

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_head_tail_1
----------
Epoch 1/40
time = 258.40 secondes

Train loss 0.26506395068120314 micro_f1_score 0.6276652452025586 
 
time = 14.09 secondes

Val loss 0.21140336343010918 micro_f1_score 0.6797642436149312
 
----------
Epoch 2/40
time = 234.97 secondes

Train loss 0.16545089507760766 micro_f1_score 0.7776915853509166 
 
time = 11.31 secondes

Val loss 0.20225933072019797 micro_f1_score 0.7175452399685286
 
----------
Epoch 3/40
time = 249.21 secondes

Train loss 0.1394676879350398 micro_f1_score 0.8207027005272265 
 
time = 9.18 secondes

Val loss 0.1816113859903617 micro_f1_score 0.7624521072796935
 
----------
Epoch 4/40
time = 242.00 secondes

Train loss 0.1218947393769348 micro_f1_score 0.8493380794304682 
 
time = 9.63 secondes

Val loss 0.18719989049141525 micro_f1_score 0.7572668931672328
 
----------
Epoch 5/40
time = 201.56 secondes

Train loss 0.10786856358015054 micro_f1_score 0.8711510191108108 
 
time = 7.78 secondes

Val loss 0.188481085857407 micro_f1_score 0.7612328258447828
 
----------
Epoch 6/40
time = 199.19 secondes

Train loss 0.0943109035424821 micro_f1_score 0.8892045454545454 
 
time = 6.69 secondes

Val loss 0.1850788020452515 micro_f1_score 0.7802359882005899
 
----------
Epoch 7/40
time = 196.17 secondes

Train loss 0.08311659424809051 micro_f1_score 0.9047936869380865 
 
time = 6.85 secondes

Val loss 0.2036734546427844 micro_f1_score 0.775018261504748
 
----------
Epoch 8/40
time = 199.63 secondes

Train loss 0.07218587837173596 micro_f1_score 0.9190478051831296 
 
time = 6.77 secondes

Val loss 0.21424735313067672 micro_f1_score 0.7762920130104806
 
----------
Epoch 9/40
time = 208.56 secondes

Train loss 0.06263443069104609 micro_f1_score 0.9319330999611046 
 
time = 10.54 secondes

Val loss 0.22246146202087402 micro_f1_score 0.7713235294117646
 
----------
Epoch 10/40
time = 233.99 secondes

Train loss 0.05516004669060511 micro_f1_score 0.9392389366813919 
 
time = 6.71 secondes

Val loss 0.24290081247931622 micro_f1_score 0.7667870036101083
 
----------
Epoch 11/40
time = 199.89 secondes

Train loss 0.04817599237619622 micro_f1_score 0.948922030754965 
 
time = 6.72 secondes

Val loss 0.2649866088491971 micro_f1_score 0.7640857869865504
 
----------
Epoch 12/40
time = 194.48 secondes

Train loss 0.042617004050512435 micro_f1_score 0.9544753086419753 
 
time = 6.69 secondes

Val loss 0.25727441106907656 micro_f1_score 0.7795992714025503
 
----------
Epoch 13/40
time = 198.58 secondes

Train loss 0.03886744587027745 micro_f1_score 0.9588566145311658 
 
time = 6.80 secondes

Val loss 0.2843052229431809 micro_f1_score 0.7741472172351885
 
----------
Epoch 14/40
time = 196.66 secondes

Train loss 0.03377579419678581 micro_f1_score 0.9637823097899144 
 
time = 6.70 secondes

Val loss 0.3027286561908292 micro_f1_score 0.7700573065902578
 
----------
Epoch 15/40
time = 195.30 secondes

Train loss 0.029793643232262988 micro_f1_score 0.9676577786303473 
 
time = 7.73 secondes

Val loss 0.3224544561788684 micro_f1_score 0.7707896575821105
 
----------
Epoch 16/40
time = 238.95 secondes

Train loss 0.02878019668215357 micro_f1_score 0.9694115845488304 
 
time = 10.55 secondes

Val loss 0.30748933828512176 micro_f1_score 0.7744525547445256
 
----------
Epoch 17/40
time = 254.89 secondes

Train loss 0.024635049009796333 micro_f1_score 0.9734262521059888 
 
time = 11.01 secondes

Val loss 0.33118595353892594 micro_f1_score 0.7744058500914078
 
----------
Epoch 18/40
time = 255.29 secondes

Train loss 0.021429578485864062 micro_f1_score 0.9776623773339951 
 
time = 10.63 secondes

Val loss 0.3370852301843831 micro_f1_score 0.7785379906373785
 
----------
Epoch 19/40
time = 257.37 secondes

Train loss 0.019158005171380978 micro_f1_score 0.9803756872327428 
 
time = 10.60 secondes

Val loss 0.3475137478748306 micro_f1_score 0.7742170429715951
 
----------
Epoch 20/40
time = 253.46 secondes

Train loss 0.0171225300248684 micro_f1_score 0.9821748921714568 
 
time = 10.95 secondes

Val loss 0.36936965719109677 micro_f1_score 0.767053701015965
 
----------
Epoch 21/40
time = 257.27 secondes

Train loss 0.015094594449879338 micro_f1_score 0.9849276910749036 
 
time = 10.93 secondes

Val loss 0.36171261123457893 micro_f1_score 0.7794759825327512
 
----------
Epoch 22/40
time = 259.68 secondes

Train loss 0.013880568435680444 micro_f1_score 0.984951807687912 
 
time = 10.84 secondes

Val loss 0.37907839334401927 micro_f1_score 0.7763347763347764
 
----------
Epoch 23/40
time = 257.85 secondes

Train loss 0.012333657198145342 micro_f1_score 0.9872048743335872 
 
time = 10.99 secondes

Val loss 0.40075229511397786 micro_f1_score 0.7771387491013659
 
----------
Epoch 24/40
time = 255.08 secondes

Train loss 0.010848536409609658 micro_f1_score 0.9880472021317092 
 
time = 10.68 secondes

Val loss 0.41464181000091993 micro_f1_score 0.7765567765567766
 
----------
Epoch 25/40
time = 256.94 secondes

Train loss 0.010562504838811505 micro_f1_score 0.9887246685966784 
 
time = 11.24 secondes

Val loss 0.442025219197156 micro_f1_score 0.7640282422891119
 
----------
Epoch 26/40
time = 257.41 secondes

Train loss 0.01015883562769788 micro_f1_score 0.98950091296409 
 
time = 6.93 secondes

Val loss 0.4261170072389431 micro_f1_score 0.77878022374594
 
----------
Epoch 27/40
time = 196.48 secondes

Train loss 0.009356776847552454 micro_f1_score 0.9900304414003044 
 
time = 6.70 secondes

Val loss 0.4413390708629225 micro_f1_score 0.7710314557425019
 
----------
Epoch 28/40
time = 195.18 secondes

Train loss 0.008508976735364748 micro_f1_score 0.9913599512807825 
 
time = 6.67 secondes

Val loss 0.4389407198082228 micro_f1_score 0.7796733212341198
 
----------
Epoch 29/40
time = 197.24 secondes

Train loss 0.007909708936409795 micro_f1_score 0.9921943418497505 
 
time = 6.69 secondes

Val loss 0.44929391753355985 micro_f1_score 0.7720694645441389
 
----------
Epoch 30/40
time = 198.08 secondes

Train loss 0.006510077664653009 micro_f1_score 0.9930378542895187 
 
time = 6.69 secondes

Val loss 0.45678213983774185 micro_f1_score 0.7718918918918919
 
----------
Epoch 31/40
time = 197.53 secondes

Train loss 0.006072079605991684 micro_f1_score 0.9933513164393449 
 
time = 6.81 secondes

Val loss 0.48146248706540123 micro_f1_score 0.7698300283286119
 
----------
Epoch 32/40
time = 195.00 secondes

Train loss 0.005353197201232737 micro_f1_score 0.9943404109849204 
 
time = 6.67 secondes

Val loss 0.4685279651254904 micro_f1_score 0.7806267806267806
 
----------
Epoch 33/40
time = 197.74 secondes

Train loss 0.004957904537286185 micro_f1_score 0.9946369480050208 
 
time = 6.67 secondes

Val loss 0.5015883207565448 micro_f1_score 0.7720430107526881
 
----------
Epoch 34/40
time = 194.50 secondes

Train loss 0.0044874191574070885 micro_f1_score 0.9955156950672646 
 
time = 6.69 secondes

Val loss 0.5061992496007779 micro_f1_score 0.7757404795486601
 
----------
Epoch 35/40
time = 194.10 secondes

Train loss 0.0032651005767465823 micro_f1_score 0.9965045592705167 
 
time = 6.66 secondes

Val loss 0.4970396624908584 micro_f1_score 0.7835125448028674
 
----------
Epoch 36/40
time = 198.08 secondes

Train loss 0.0031498606877616736 micro_f1_score 0.9966181555648439 
 
time = 6.68 secondes

Val loss 0.5041484122821054 micro_f1_score 0.7790613718411553
 
----------
Epoch 37/40
time = 196.60 secondes

Train loss 0.00278146753323879 micro_f1_score 0.9974148418491484 
 
time = 6.72 secondes

Val loss 0.5087308057996093 micro_f1_score 0.7805227354099535
 
----------
Epoch 38/40
time = 198.18 secondes

Train loss 0.0017760788076966573 micro_f1_score 0.9983663234679534 
 
time = 6.69 secondes

Val loss 0.5035036534803813 micro_f1_score 0.7789017341040462
 
----------
Epoch 39/40
time = 198.08 secondes

Train loss 0.0012910034858209286 micro_f1_score 0.998480358635362 
 
time = 6.71 secondes

Val loss 0.5110335666380945 micro_f1_score 0.78059377262853
 
----------
Epoch 40/40
time = 198.39 secondes

Train loss 0.0011780092159365718 micro_f1_score 0.9988981344275999 
 
time = 6.72 secondes

Val loss 0.5248996476169492 micro_f1_score 0.7780959198282034
 
----------
best_f1_socre 0.7835125448028674 best_epoch 35

average train time 219.1993641912937

average val time 8.333159780502319
 
time = 7.21 secondes

test_f1_score 0.7613398464759246

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_tail_1
----------
Epoch 1/40
time = 198.46 secondes

Train loss 0.2628511105504659 micro_f1_score 0.6281912980942108 
 
time = 7.68 secondes

Val loss 0.20952062833993162 micro_f1_score 0.7151051625239006
 
----------
Epoch 2/40
time = 199.82 secondes

Train loss 0.1662156180923318 micro_f1_score 0.7817225509533202 
 
time = 6.66 secondes

Val loss 0.19195793850011514 micro_f1_score 0.7412587412587414
 
----------
Epoch 3/40
time = 197.20 secondes

Train loss 0.13943671903296098 micro_f1_score 0.8260447821518068 
 
time = 6.70 secondes

Val loss 0.18816395794026186 micro_f1_score 0.7559171597633136
 
----------
Epoch 4/40
time = 196.17 secondes

Train loss 0.12221817206255756 micro_f1_score 0.8507933970182662 
 
time = 6.66 secondes

Val loss 0.19015411831072118 micro_f1_score 0.7601038190582128
 
----------
Epoch 5/40
time = 198.80 secondes

Train loss 0.10999974966686857 micro_f1_score 0.8696826468602296 
 
time = 6.65 secondes

Val loss 0.184335253827396 micro_f1_score 0.770856102003643
 
----------
Epoch 6/40
time = 199.85 secondes

Train loss 0.09641549726696433 micro_f1_score 0.8894851055434998 
 
time = 6.67 secondes

Val loss 0.19668321268724615 micro_f1_score 0.7655677655677655
 
----------
Epoch 7/40
time = 197.65 secondes

Train loss 0.08540909958013275 micro_f1_score 0.9038687907086244 
 
time = 6.68 secondes

Val loss 0.21481771339647104 micro_f1_score 0.7620777333817654
 
----------
Epoch 8/40
time = 199.35 secondes

Train loss 0.07536614481695332 micro_f1_score 0.916965818635867 
 
time = 6.65 secondes

Val loss 0.2158096312988 micro_f1_score 0.7653874004344677
 
----------
Epoch 9/40
time = 198.61 secondes

Train loss 0.06605652882004374 micro_f1_score 0.9302379750766722 
 
time = 6.66 secondes

Val loss 0.2336798699908569 micro_f1_score 0.7631483496554226
 
----------
Epoch 10/40
time = 197.38 secondes

Train loss 0.05813211839193025 micro_f1_score 0.9381471283980302 
 
time = 6.67 secondes

Val loss 0.25734698937320316 micro_f1_score 0.7575544624033731
 
----------
Epoch 11/40
time = 199.52 secondes

Train loss 0.0510023315521041 micro_f1_score 0.9474376329530072 
 
time = 6.71 secondes

Val loss 0.2693676400135775 micro_f1_score 0.7607758620689655
 
----------
Epoch 12/40
time = 199.84 secondes

Train loss 0.04446540192453409 micro_f1_score 0.9544384861695151 
 
time = 6.70 secondes

Val loss 0.2972809228985036 micro_f1_score 0.7499097798628652
 
----------
Epoch 13/40
time = 197.80 secondes

Train loss 0.039580944663979246 micro_f1_score 0.9589915062070026 
 
time = 6.78 secondes

Val loss 0.29875146487697224 micro_f1_score 0.756043956043956
 
----------
Epoch 14/40
time = 198.33 secondes

Train loss 0.03635510147742189 micro_f1_score 0.9619621540705484 
 
time = 6.73 secondes

Val loss 0.31960069962212295 micro_f1_score 0.7539118065433855
 
----------
Epoch 15/40
time = 198.99 secondes

Train loss 0.031942714403932995 micro_f1_score 0.9668544277119976 
 
time = 6.70 secondes

Val loss 0.32129709569157144 micro_f1_score 0.7527272727272728
 
----------
Epoch 16/40
time = 199.15 secondes

Train loss 0.02925693497737148 micro_f1_score 0.9691542288557213 
 
time = 6.73 secondes

Val loss 0.3614735063470778 micro_f1_score 0.753050969131371
 
----------
Epoch 17/40
time = 197.73 secondes

Train loss 0.025638201566064847 micro_f1_score 0.9725325172149961 
 
time = 6.65 secondes

Val loss 0.35177981291638044 micro_f1_score 0.7558600793364588
 
----------
Epoch 18/40
time = 199.14 secondes

Train loss 0.02271375619965765 micro_f1_score 0.9762870037481833 
 
time = 6.73 secondes

Val loss 0.36221165625286883 micro_f1_score 0.7593659942363112
 
----------
Epoch 19/40
time = 197.09 secondes

Train loss 0.01997708287100551 micro_f1_score 0.9783073632752827 
 
time = 6.67 secondes

Val loss 0.3626237069485617 micro_f1_score 0.7626262626262627
 
----------
Epoch 20/40
time = 201.06 secondes

Train loss 0.018172496683194997 micro_f1_score 0.9811651669971023 
 
time = 6.76 secondes

Val loss 0.3838384982015266 micro_f1_score 0.7574437182280319
 
----------
Epoch 21/40
time = 196.42 secondes

Train loss 0.015495114301846295 micro_f1_score 0.9838518801297957 
 
time = 6.63 secondes

Val loss 0.38869660677479917 micro_f1_score 0.7637824023366192
 
----------
Epoch 22/40
time = 198.16 secondes

Train loss 0.01473191286336792 micro_f1_score 0.9835003620012954 
 
time = 6.63 secondes

Val loss 0.4055550994931674 micro_f1_score 0.7553306830502349
 
----------
Epoch 23/40
time = 196.25 secondes

Train loss 0.013353249963807143 micro_f1_score 0.9853725430443394 
 
time = 6.71 secondes

Val loss 0.41399343800349314 micro_f1_score 0.7641509433962264
 
----------
Epoch 24/40
time = 199.33 secondes

Train loss 0.01317891816418253 micro_f1_score 0.9860163840731567 
 
time = 6.66 secondes

Val loss 0.42497471097062844 micro_f1_score 0.7631578947368421
 
----------
Epoch 25/40
time = 199.39 secondes

Train loss 0.011064312318515616 micro_f1_score 0.9886870071991771 
 
time = 6.70 secondes

Val loss 0.44613154403498917 micro_f1_score 0.7583697234352256
 
----------
Epoch 26/40
time = 200.22 secondes

Train loss 0.010380679497101737 micro_f1_score 0.9887050770108385 
 
time = 6.68 secondes

Val loss 0.4411448986559618 micro_f1_score 0.7608856088560885
 
----------
Epoch 27/40
time = 196.38 secondes

Train loss 0.008499299879094742 micro_f1_score 0.9910979228486648 
 
time = 6.71 secondes

Val loss 0.47218190157999756 micro_f1_score 0.7517216382747371
 
----------
Epoch 28/40
time = 198.99 secondes

Train loss 0.009334606990736788 micro_f1_score 0.9899965767753224 
 
time = 6.68 secondes

Val loss 0.4738851810087923 micro_f1_score 0.7583697234352256
 
----------
Epoch 29/40
time = 200.06 secondes

Train loss 0.007077925474653524 micro_f1_score 0.9929245283018868 
 
time = 6.68 secondes

Val loss 0.4737075024940928 micro_f1_score 0.7648559970834852
 
----------
Epoch 30/40
time = 193.63 secondes

Train loss 0.00712923859887146 micro_f1_score 0.9927401269527538 
 
time = 6.64 secondes

Val loss 0.4746933501274859 micro_f1_score 0.7688855646970829
 
----------
Epoch 31/40
time = 198.96 secondes

Train loss 0.0069322890039574805 micro_f1_score 0.9929655119966538 
 
time = 6.65 secondes

Val loss 0.4803002217753989 micro_f1_score 0.7656423546834507
 
----------
Epoch 32/40
time = 200.55 secondes

Train loss 0.005505494387109941 micro_f1_score 0.9947170385010072 
 
time = 6.69 secondes

Val loss 0.5262199271409238 micro_f1_score 0.7549090909090909
 
----------
Epoch 33/40
time = 195.87 secondes

Train loss 0.005545834171900899 micro_f1_score 0.9944482470149821 
 
time = 6.67 secondes

Val loss 0.49092059154979517 micro_f1_score 0.7649836541954231
 
----------
Epoch 34/40
time = 199.78 secondes

Train loss 0.004895046868610685 micro_f1_score 0.9953265701584407 
 
time = 6.80 secondes

Val loss 0.5068288091264788 micro_f1_score 0.7647706422018349
 
----------
Epoch 35/40
time = 197.53 secondes

Train loss 0.003942068313291521 micro_f1_score 0.9958575609014556 
 
time = 6.79 secondes

Val loss 0.5247386014852368 micro_f1_score 0.7568555758683729
 
----------
Epoch 36/40
time = 195.65 secondes

Train loss 0.0033808121697720627 micro_f1_score 0.9964680414720292 
 
time = 6.68 secondes

Val loss 0.5046459548786039 micro_f1_score 0.7679231337767923
 
----------
Epoch 37/40
time = 199.13 secondes

Train loss 0.002702257947500609 micro_f1_score 0.9976449137734559 
 
time = 6.67 secondes

Val loss 0.5224729935165311 micro_f1_score 0.7653323540213
 
----------
Epoch 38/40
time = 199.36 secondes

Train loss 0.0024308712024526564 micro_f1_score 0.9976059281778453 
 
time = 6.65 secondes

Val loss 0.5140844822907057 micro_f1_score 0.7656934306569344
 
----------
Epoch 39/40
time = 199.41 secondes

Train loss 0.0015608735576795485 micro_f1_score 0.9982518811279166 
 
time = 6.74 secondes

Val loss 0.5236923157191667 micro_f1_score 0.7669227954628614
 
----------
Epoch 40/40
time = 196.04 secondes

Train loss 0.0015525739516445494 micro_f1_score 0.9982142178654204 
 
time = 6.65 secondes

Val loss 0.5347291641548032 micro_f1_score 0.7614213197969543
 
----------
best_f1_socre 0.770856102003643 best_epoch 5

average train time 198.32670292258263

average val time 6.712906986474991
 
time = 7.15 secondes

test_f1_score 0.7626876340243031

----------
516 516
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
Hyperpartisan_BERT_head_bert_summarizer_1
----------
Epoch 1/40
time = 11.81 secondes

Train loss 0.6564284472754507 accuracy 0.604651153087616 macro_avg {'precision': 0.493521385162252, 'recall': 0.4972449327893633, 'f1-score': 0.45252558874927185, 'support': 516} weighted_avg {'precision': 0.5327761822349866, 'recall': 0.6046511627906976, 'f1-score': 0.5319440869620751, 'support': 516}
 
time = 0.41 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss 0.6781933009624481 accuracy 0.59375 macro_avg {'precision': 0.296875, 'recall': 0.5, 'f1-score': 0.37254901960784315, 'support': 64} weighted_avg {'precision': 0.3525390625, 'recall': 0.59375, 'f1-score': 0.4424019607843137, 'support': 64}
 
----------
Epoch 2/40
time = 10.96 secondes

Train loss 0.5317654058788762 accuracy 0.7538759708404541 macro_avg {'precision': 0.7884451996601529, 'recall': 0.676584366822164, 'f1-score': 0.6852071574396541, 'support': 516} weighted_avg {'precision': 0.7740074621459102, 'recall': 0.7538759689922481, 'f1-score': 0.7256676190183609, 'support': 516}
 
time = 0.35 secondes

Val loss 0.5335587412118912 accuracy 0.71875 macro_avg {'precision': 0.7136363636363636, 'recall': 0.6902834008097165, 'f1-score': 0.6945917285259808, 'support': 64} weighted_avg {'precision': 0.7161931818181818, 'recall': 0.71875, 'f1-score': 0.7106972428419936, 'support': 64}
 
----------
Epoch 3/40
time = 10.93 secondes

Train loss 0.4269268973307176 accuracy 0.817829430103302 macro_avg {'precision': 0.802165401138183, 'recall': 0.8098272190887961, 'f1-score': 0.8054829240122556, 'support': 516} weighted_avg {'precision': 0.8210153671052546, 'recall': 0.8178294573643411, 'f1-score': 0.8189691373660722, 'support': 516}
 
time = 0.35 secondes

Val loss 0.5928634405136108 accuracy 0.75 macro_avg {'precision': 0.7584541062801933, 'recall': 0.7165991902834008, 'f1-score': 0.722943722943723, 'support': 64} weighted_avg {'precision': 0.7548309178743962, 'recall': 0.75, 'f1-score': 0.7391774891774892, 'support': 64}
 
----------
Epoch 4/40
time = 10.94 secondes

Train loss 0.3294560726393353 accuracy 0.8701550364494324 macro_avg {'precision': 0.8572784810126582, 'recall': 0.8670172130747851, 'f1-score': 0.861498708010336, 'support': 516} weighted_avg {'precision': 0.8730411637719556, 'recall': 0.8701550387596899, 'f1-score': 0.8710273821685397, 'support': 516}
 
time = 0.35 secondes

Val loss 0.8251591473817825 accuracy 0.75 macro_avg {'precision': 0.7885714285714285, 'recall': 0.7044534412955465, 'f1-score': 0.709090909090909, 'support': 64} weighted_avg {'precision': 0.7757142857142857, 'recall': 0.75, 'f1-score': 0.7295454545454545, 'support': 64}
 
----------
Epoch 5/40
time = 11.99 secondes

Train loss 0.2844695797133626 accuracy 0.9166666865348816 macro_avg {'precision': 0.912797619047619, 'recall': 0.9057994571135999, 'f1-score': 0.9090860666653009, 'support': 516} weighted_avg {'precision': 0.9163194444444446, 'recall': 0.9166666666666666, 'f1-score': 0.9163105310961327, 'support': 516}
 
time = 0.34 secondes

Val loss 0.5116091156378388 accuracy 0.828125 macro_avg {'precision': 0.822167487684729, 'recall': 0.8309716599190283, 'f1-score': 0.8246575342465753, 'support': 64} weighted_avg {'precision': 0.8340825123152709, 'recall': 0.828125, 'f1-score': 0.829280821917808, 'support': 64}
 
----------
Epoch 6/40
time = 10.88 secondes

Train loss 0.24489098126915368 accuracy 0.9089147448539734 macro_avg {'precision': 0.8981012658227848, 'recall': 0.9089527493782813, 'f1-score': 0.9028423772609819, 'support': 516} weighted_avg {'precision': 0.9113384358747915, 'recall': 0.9089147286821705, 'f1-score': 0.9095266710734532, 'support': 516}
 
time = 0.35 secondes

Val loss 0.793603777885437 accuracy 0.78125 macro_avg {'precision': 0.7863636363636364, 'recall': 0.7550607287449393, 'f1-score': 0.7624602332979852, 'support': 64} weighted_avg {'precision': 0.7838068181818182, 'recall': 0.78125, 'f1-score': 0.7749867444326617, 'support': 64}
 
----------
Epoch 7/40
time = 10.65 secondes

Train loss 0.2702000670586572 accuracy 0.9069767594337463 macro_avg {'precision': 0.8962319483854604, 'recall': 0.9062789525868374, 'f1-score': 0.9006721314105135, 'support': 516} weighted_avg {'precision': 0.9091621263827758, 'recall': 0.9069767441860465, 'f1-score': 0.9075587084422496, 'support': 516}
 
time = 0.35 secondes

Val loss 1.1345112174749374 accuracy 0.765625 macro_avg {'precision': 0.8006802721088435, 'recall': 0.7236842105263157, 'f1-score': 0.7308662741799832, 'support': 64} weighted_avg {'precision': 0.7883078231292517, 'recall': 0.765625, 'f1-score': 0.7490012615643398, 'support': 64}
 
----------
Epoch 8/40
time = 10.94 secondes

Train loss 0.24346368864291545 accuracy 0.9379844665527344 macro_avg {'precision': 0.932903141914406, 'recall': 0.932903141914406, 'f1-score': 0.932903141914406, 'support': 516} weighted_avg {'precision': 0.937984496124031, 'recall': 0.937984496124031, 'f1-score': 0.937984496124031, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0402442067861557 accuracy 0.78125 macro_avg {'precision': 0.8125, 'recall': 0.742914979757085, 'f1-score': 0.751937984496124, 'support': 64} weighted_avg {'precision': 0.80078125, 'recall': 0.78125, 'f1-score': 0.7679263565891474, 'support': 64}
 
----------
Epoch 9/40
time = 10.89 secondes

Train loss 0.16928489145004388 accuracy 0.9573643207550049 macro_avg {'precision': 0.9538709100661541, 'recall': 0.9538709100661541, 'f1-score': 0.9538709100661541, 'support': 516} weighted_avg {'precision': 0.9573643410852714, 'recall': 0.9573643410852714, 'f1-score': 0.9573643410852714, 'support': 516}
 
time = 0.35 secondes

Val loss 0.8552372977137566 accuracy 0.8125 macro_avg {'precision': 0.8227272727272728, 'recall': 0.7874493927125505, 'f1-score': 0.7963944856839873, 'support': 64} weighted_avg {'precision': 0.8176136363636364, 'recall': 0.8125, 'f1-score': 0.8071314952279958, 'support': 64}
 
----------
Epoch 10/40
time = 11.79 secondes

Train loss 0.2184644823287134 accuracy 0.9341084957122803 macro_avg {'precision': 0.9229235880398672, 'recall': 0.9448661476195894, 'f1-score': 0.9307273158019427, 'support': 516} weighted_avg {'precision': 0.9413917432846585, 'recall': 0.9341085271317829, 'f1-score': 0.9349390000899893, 'support': 516}
 
time = 0.35 secondes

Val loss 0.525822214782238 accuracy 0.859375 macro_avg {'precision': 0.8533533533533533, 'recall': 0.8572874493927125, 'f1-score': 0.8550943396226415, 'support': 64} weighted_avg {'precision': 0.8605793293293293, 'recall': 0.859375, 'f1-score': 0.8597641509433962, 'support': 64}
 
----------
Epoch 11/40
time = 12.56 secondes

Train loss 0.21448321677179952 accuracy 0.9515503644943237 macro_avg {'precision': 0.9523801608935576, 'recall': 0.9423873998342083, 'f1-score': 0.9470127949723769, 'support': 516} weighted_avg {'precision': 0.9516437370927733, 'recall': 0.9515503875968992, 'f1-score': 0.951279935056365, 'support': 516}
 
time = 0.35 secondes

Val loss 0.6052107587456703 accuracy 0.828125 macro_avg {'precision': 0.822167487684729, 'recall': 0.8309716599190283, 'f1-score': 0.8246575342465753, 'support': 64} weighted_avg {'precision': 0.8340825123152709, 'recall': 0.828125, 'f1-score': 0.829280821917808, 'support': 64}
 
----------
Epoch 12/40
time = 10.89 secondes

Train loss 0.2194247061781811 accuracy 0.9302325248718262 macro_avg {'precision': 0.9194221659396838, 'recall': 0.9360564341790876, 'f1-score': 0.926101554667982, 'support': 516} weighted_avg {'precision': 0.9344719276296725, 'recall': 0.9302325581395349, 'f1-score': 0.930909771823396, 'support': 516}
 
time = 0.35 secondes

Val loss 0.983617901802063 accuracy 0.796875 macro_avg {'precision': 0.8099415204678362, 'recall': 0.7682186234817814, 'f1-score': 0.7772423025435075, 'support': 64} weighted_avg {'precision': 0.8039108187134503, 'recall': 0.796875, 'f1-score': 0.7896419009370819, 'support': 64}
 
----------
Epoch 13/40
time = 10.91 secondes

Train loss 0.18718289348974146 accuracy 0.9593023061752319 macro_avg {'precision': 0.9622660427807486, 'recall': 0.9496204671423696, 'f1-score': 0.9553793129007152, 'support': 516} weighted_avg {'precision': 0.9596998974008207, 'recall': 0.9593023255813954, 'f1-score': 0.9590202789180785, 'support': 516}
 
time = 0.35 secondes

Val loss 1.006586641073227 accuracy 0.8125 macro_avg {'precision': 0.8357487922705313, 'recall': 0.7813765182186234, 'f1-score': 0.7922077922077922, 'support': 64} weighted_avg {'precision': 0.8257850241545894, 'recall': 0.8125, 'f1-score': 0.8043831168831169, 'support': 64}
 
----------
Epoch 14/40
time = 10.96 secondes

Train loss 0.17747956688275957 accuracy 0.9593023061752319 macro_avg {'precision': 0.9555007784120395, 'recall': 0.956544706857598, 'f1-score': 0.9560182648401826, 'support': 516} weighted_avg {'precision': 0.9593566333981004, 'recall': 0.9593023255813954, 'f1-score': 0.959325616792326, 'support': 516}
 
time = 0.35 secondes

Val loss 1.1108031123876572 accuracy 0.8125 macro_avg {'precision': 0.8227272727272728, 'recall': 0.7874493927125505, 'f1-score': 0.7963944856839873, 'support': 64} weighted_avg {'precision': 0.8176136363636364, 'recall': 0.8125, 'f1-score': 0.8071314952279958, 'support': 64}
 
----------
Epoch 15/40
time = 10.89 secondes

Train loss 0.25546512649761455 accuracy 0.9554263353347778 macro_avg {'precision': 0.9554563492063493, 'recall': 0.947734993417096, 'f1-score': 0.9513716170535329, 'support': 516} weighted_avg {'precision': 0.9554290482342808, 'recall': 0.9554263565891473, 'f1-score': 0.9552358654700244, 'support': 516}
 
time = 0.35 secondes

Val loss 0.6987213715910912 accuracy 0.875 macro_avg {'precision': 0.8690476190476191, 'recall': 0.8765182186234818, 'f1-score': 0.8718718718718719, 'support': 64} weighted_avg {'precision': 0.8779761904761905, 'recall': 0.875, 'f1-score': 0.8756256256256256, 'support': 64}
 
----------
Epoch 16/40
time = 10.97 secondes

Train loss 0.10328832256367826 accuracy 0.9786821603775024 macro_avg {'precision': 0.974537037037037, 'recall': 0.9798205549144223, 'f1-score': 0.9770654620242679, 'support': 516} weighted_avg {'precision': 0.9789961958082114, 'recall': 0.9786821705426356, 'f1-score': 0.9787411745031601, 'support': 516}
 
time = 0.35 secondes

Val loss 0.9933269396424294 accuracy 0.828125 macro_avg {'precision': 0.8473684210526315, 'recall': 0.8006072874493927, 'f1-score': 0.811512717536814, 'support': 64} weighted_avg {'precision': 0.8384868421052631, 'recall': 0.828125, 'f1-score': 0.8220046854082999, 'support': 64}
 
----------
Epoch 17/40
time = 10.92 secondes

Train loss 0.12676535194358704 accuracy 0.9748061895370483 macro_avg {'precision': 0.9755379351187734, 'recall': 0.9698568015213822, 'f1-score': 0.972580902279611, 'support': 516} weighted_avg {'precision': 0.9748543419167288, 'recall': 0.9748062015503876, 'f1-score': 0.9747305110990007, 'support': 516}
 
time = 0.35 secondes

Val loss 0.7951362002640963 accuracy 0.84375 macro_avg {'precision': 0.8380566801619433, 'recall': 0.8380566801619433, 'f1-score': 0.8380566801619433, 'support': 64} weighted_avg {'precision': 0.84375, 'recall': 0.84375, 'f1-score': 0.84375, 'support': 64}
 
----------
Epoch 18/40
time = 10.94 secondes

Train loss 0.06015514926349914 accuracy 0.9844961166381836 macro_avg {'precision': 0.9803172973579941, 'recall': 0.9866879053362156, 'f1-score': 0.9833387148853729, 'support': 516} weighted_avg {'precision': 0.9848818618777474, 'recall': 0.9844961240310077, 'f1-score': 0.9845471861991977, 'support': 516}
 
time = 0.35 secondes

Val loss 1.6498669609427452 accuracy 0.75 macro_avg {'precision': 0.7773279352226721, 'recall': 0.7773279352226721, 'f1-score': 0.7499999999999999, 'support': 64} weighted_avg {'precision': 0.8046558704453441, 'recall': 0.75, 'f1-score': 0.7499999999999999, 'support': 64}
 
----------
Epoch 19/40
time = 10.94 secondes

Train loss 0.14942409731024367 accuracy 0.9689922332763672 macro_avg {'precision': 0.9610992655768775, 'recall': 0.9745298506249696, 'f1-score': 0.9669590830505218, 'support': 516} weighted_avg {'precision': 0.9709308753390671, 'recall': 0.9689922480620154, 'f1-score': 0.9692146254851476, 'support': 516}
 
time = 0.35 secondes

Val loss 1.1979424841701984 accuracy 0.765625 macro_avg {'precision': 0.776847290640394, 'recall': 0.784412955465587, 'f1-score': 0.7651088818204062, 'support': 64} weighted_avg {'precision': 0.7992918719211823, 'recall': 0.765625, 'f1-score': 0.7671733545387815, 'support': 64}
 
----------
Epoch 20/40
time = 10.90 secondes

Train loss 0.12477985177555996 accuracy 0.9748061895370483 macro_avg {'precision': 0.9722366372599895, 'recall': 0.9733189213789964, 'f1-score': 0.9727732115677321, 'support': 516} weighted_avg {'precision': 0.9748429096116791, 'recall': 0.9748062015503876, 'f1-score': 0.9748206199190588, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0309614166617393 accuracy 0.78125 macro_avg {'precision': 0.7882352941176471, 'recall': 0.7975708502024291, 'f1-score': 0.780392156862745, 'support': 64} weighted_avg {'precision': 0.8091911764705884, 'recall': 0.78125, 'f1-score': 0.7829656862745098, 'support': 64}
 
----------
Epoch 21/40
time = 10.91 secondes

Train loss 0.39107131817956653 accuracy 0.9360464811325073 macro_avg {'precision': 0.9418103448275863, 'recall': 0.9198429855501195, 'f1-score': 0.929148896332203, 'support': 516} weighted_avg {'precision': 0.9372633208589504, 'recall': 0.936046511627907, 'f1-score': 0.9352325073383518, 'support': 516}
 
time = 0.35 secondes

Val loss 1.5540831983089447 accuracy 0.796875 macro_avg {'precision': 0.8725490196078431, 'recall': 0.75, 'f1-score': 0.7602996254681648, 'support': 64} weighted_avg {'precision': 0.8486519607843137, 'recall': 0.796875, 'f1-score': 0.7778558052434457, 'support': 64}
 
----------
Epoch 22/40
time = 11.64 secondes

Train loss 0.05016793652127186 accuracy 0.9903100728988647 macro_avg {'precision': 0.9889724961079398, 'recall': 0.9900931359003949, 'f1-score': 0.9895281582952815, 'support': 516} weighted_avg {'precision': 0.9903291858252575, 'recall': 0.9903100775193798, 'f1-score': 0.9903156230457919, 'support': 516}
 
time = 0.36 secondes

Val loss 1.2130243480205536 accuracy 0.828125 macro_avg {'precision': 0.8877551020408163, 'recall': 0.7884615384615384, 'f1-score': 0.8026352677319877, 'support': 64} weighted_avg {'precision': 0.8667091836734694, 'recall': 0.828125, 'f1-score': 0.8159342584805158, 'support': 64}
 
----------
Epoch 23/40
time = 10.84 secondes

Train loss 0.06955826049670577 accuracy 0.9844961166381836 macro_avg {'precision': 0.9832257854786015, 'recall': 0.9832257854786015, 'f1-score': 0.9832257854786015, 'support': 516} weighted_avg {'precision': 0.9844961240310077, 'recall': 0.9844961240310077, 'f1-score': 0.9844961240310077, 'support': 516}
 
time = 0.35 secondes

Val loss 1.4040207266807556 accuracy 0.8125 macro_avg {'precision': 0.8541666666666667, 'recall': 0.7753036437246963, 'f1-score': 0.787375415282392, 'support': 64} weighted_avg {'precision': 0.8385416666666667, 'recall': 0.8125, 'f1-score': 0.801079734219269, 'support': 64}
 
----------
Epoch 24/40
time = 10.88 secondes

Train loss 0.22597418022765356 accuracy 0.961240291595459 macro_avg {'precision': 0.9665481386609144, 'recall': 0.9499861840287371, 'f1-score': 0.9573412698412698, 'support': 516} weighted_avg {'precision': 0.9621145406677256, 'recall': 0.9612403100775194, 'f1-score': 0.9608903962101637, 'support': 516}
 
time = 0.35 secondes

Val loss 1.2908625602722168 accuracy 0.75 macro_avg {'precision': 0.7420634920634921, 'recall': 0.7469635627530364, 'f1-score': 0.7437437437437437, 'support': 64} weighted_avg {'precision': 0.753968253968254, 'recall': 0.75, 'f1-score': 0.7512512512512513, 'support': 64}
 
----------
Epoch 25/40
time = 10.84 secondes

Train loss 0.02542632190637629 accuracy 0.9941860437393188 macro_avg {'precision': 0.9954819277108433, 'recall': 0.9919786096256684, 'f1-score': 0.9936875843592369, 'support': 516} weighted_avg {'precision': 0.9942385822359204, 'recall': 0.9941860465116279, 'f1-score': 0.9941757335015786, 'support': 516}
 
time = 0.35 secondes

Val loss 1.1900890246033669 accuracy 0.8125 macro_avg {'precision': 0.807843137254902, 'recall': 0.8178137651821862, 'f1-score': 0.8095238095238094, 'support': 64} weighted_avg {'precision': 0.821813725490196, 'recall': 0.8125, 'f1-score': 0.8139880952380951, 'support': 64}
 
----------
Epoch 26/40
time = 10.91 secondes

Train loss 0.1445942476786899 accuracy 0.9689922332763672 macro_avg {'precision': 0.9768115942028985, 'recall': 0.9572192513368984, 'f1-score': 0.9657841950831357, 'support': 516} weighted_avg {'precision': 0.9704302887316032, 'recall': 0.9689922480620154, 'f1-score': 0.968667381937572, 'support': 516}
 
time = 0.35 secondes

Val loss 1.5419094413518906 accuracy 0.78125 macro_avg {'precision': 0.7971014492753623, 'recall': 0.7489878542510121, 'f1-score': 0.7575757575757576, 'support': 64} weighted_avg {'precision': 0.7903079710144928, 'recall': 0.78125, 'f1-score': 0.771780303030303, 'support': 64}
 
----------
Epoch 27/40
time = 10.87 secondes

Train loss 0.06955266067366624 accuracy 0.9883720874786377 macro_avg {'precision': 0.9844559585492227, 'recall': 0.9908814589665653, 'f1-score': 0.9875040361640297, 'support': 516} weighted_avg {'precision': 0.9887335823593204, 'recall': 0.9883720930232558, 'f1-score': 0.9884103896493981, 'support': 516}
 
time = 0.35 secondes

Val loss 1.24226388707757 accuracy 0.796875 macro_avg {'precision': 0.8442176870748299, 'recall': 0.7560728744939271, 'f1-score': 0.7667507709559854, 'support': 64} weighted_avg {'precision': 0.8275085034013605, 'recall': 0.796875, 'f1-score': 0.7824677600224279, 'support': 64}
 
----------
Epoch 28/40
time = 10.95 secondes

Train loss 0.09959345586266312 accuracy 0.9825581312179565 macro_avg {'precision': 0.9866863905325444, 'recall': 0.9759358288770054, 'f1-score': 0.9809246061900556, 'support': 516} weighted_avg {'precision': 0.9830225677721205, 'recall': 0.9825581395348837, 'f1-score': 0.9824607766202913, 'support': 516}
 
time = 0.35 secondes

Val loss 1.009985402226448 accuracy 0.8125 macro_avg {'precision': 0.8055555555555556, 'recall': 0.811740890688259, 'f1-score': 0.8078078078078078, 'support': 64} weighted_avg {'precision': 0.8159722222222222, 'recall': 0.8125, 'f1-score': 0.8134384384384383, 'support': 64}
 
----------
Epoch 29/40
time = 10.91 secondes

Train loss 0.049174451562011556 accuracy 0.9883720874786377 macro_avg {'precision': 0.9844559585492227, 'recall': 0.9908814589665653, 'f1-score': 0.9875040361640297, 'support': 516} weighted_avg {'precision': 0.9887335823593204, 'recall': 0.9883720930232558, 'f1-score': 0.9884103896493981, 'support': 516}
 
time = 0.35 secondes

Val loss 1.6082258224487305 accuracy 0.796875 macro_avg {'precision': 0.8725490196078431, 'recall': 0.75, 'f1-score': 0.7602996254681648, 'support': 64} weighted_avg {'precision': 0.8486519607843137, 'recall': 0.796875, 'f1-score': 0.7778558052434457, 'support': 64}
 
----------
Epoch 30/40
time = 10.91 secondes

Train loss 0.13300470047573926 accuracy 0.9786821603775024 macro_avg {'precision': 0.9838235294117648, 'recall': 0.9705882352941176, 'f1-score': 0.9766272591384699, 'support': 516} weighted_avg {'precision': 0.9793718650250798, 'recall': 0.9786821705426356, 'f1-score': 0.9785344318142316, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0951700508594513 accuracy 0.828125 macro_avg {'precision': 0.8355481727574751, 'recall': 0.8066801619433198, 'f1-score': 0.8150774888363541, 'support': 64} weighted_avg {'precision': 0.831499169435216, 'recall': 0.828125, 'f1-score': 0.8242874967165746, 'support': 64}
 
----------
Epoch 31/40
time = 11.75 secondes

Train loss 0.13965962841184315 accuracy 0.9786821603775024 macro_avg {'precision': 0.9722222222222222, 'recall': 0.9832826747720365, 'f1-score': 0.9772135129167587, 'support': 516} weighted_avg {'precision': 0.9798664944013781, 'recall': 0.9786821705426356, 'f1-score': 0.9788054929387017, 'support': 516}
 
time = 0.35 secondes

Val loss 1.2952719554305077 accuracy 0.828125 macro_avg {'precision': 0.822167487684729, 'recall': 0.8309716599190283, 'f1-score': 0.8246575342465753, 'support': 64} weighted_avg {'precision': 0.8340825123152709, 'recall': 0.828125, 'f1-score': 0.829280821917808, 'support': 64}
 
----------
Epoch 32/40
time = 10.40 secondes

Train loss 0.1393795316054655 accuracy 0.9767441749572754 macro_avg {'precision': 0.9809509524523774, 'recall': 0.9690684784552119, 'f1-score': 0.9745344475883398, 'support': 516} weighted_avg {'precision': 0.9772635399237789, 'recall': 0.9767441860465116, 'f1-score': 0.9765988085163687, 'support': 516}
 
time = 0.35 secondes

Val loss 1.9937331676483154 accuracy 0.765625 macro_avg {'precision': 0.8584905660377358, 'recall': 0.7115384615384616, 'f1-score': 0.7148797148797148, 'support': 64} weighted_avg {'precision': 0.8319575471698113, 'recall': 0.765625, 'f1-score': 0.737433174933175, 'support': 64}
 
----------
Epoch 33/40
time = 10.92 secondes

Train loss 0.04628913190081772 accuracy 0.9922480583190918 macro_avg {'precision': 0.9927655752429166, 'recall': 0.9904588527867627, 'f1-score': 0.9915933528836756, 'support': 516} weighted_avg {'precision': 0.9922622404600905, 'recall': 0.9922480620155039, 'f1-score': 0.9922389688331174, 'support': 516}
 
time = 0.35 secondes

Val loss 1.2929169610142708 accuracy 0.796875 macro_avg {'precision': 0.8000977517106549, 'recall': 0.8107287449392713, 'f1-score': 0.7956276099238516, 'support': 64} weighted_avg {'precision': 0.8194342619745845, 'recall': 0.796875, 'f1-score': 0.7986213461066076, 'support': 64}
 
----------
Epoch 34/40
time = 10.92 secondes

Train loss 0.01272820499655085 accuracy 0.998062014579773 macro_avg {'precision': 0.9973404255319149, 'recall': 0.9984802431610942, 'f1-score': 0.9979056316590563, 'support': 516} weighted_avg {'precision': 0.9980723239320469, 'recall': 0.998062015503876, 'f1-score': 0.9980631246091585, 'support': 516}
 
time = 0.35 secondes

Val loss 1.8801231384277344 accuracy 0.796875 macro_avg {'precision': 0.8725490196078431, 'recall': 0.75, 'f1-score': 0.7602996254681648, 'support': 64} weighted_avg {'precision': 0.8486519607843137, 'recall': 0.796875, 'f1-score': 0.7778558052434457, 'support': 64}
 
----------
Epoch 35/40
time = 10.94 secondes

Train loss 0.0791956859616083 accuracy 0.9864341020584106 macro_avg {'precision': 0.9895833333333333, 'recall': 0.981283422459893, 'f1-score': 0.9852000573641189, 'support': 516} weighted_avg {'precision': 0.9867167312661498, 'recall': 0.9864341085271318, 'f1-score': 0.986376132969138, 'support': 516}
 
time = 0.35 secondes

Val loss 1.5179309844970703 accuracy 0.8125 macro_avg {'precision': 0.8357487922705313, 'recall': 0.7813765182186234, 'f1-score': 0.7922077922077922, 'support': 64} weighted_avg {'precision': 0.8257850241545894, 'recall': 0.8125, 'f1-score': 0.8043831168831169, 'support': 64}
 
----------
Epoch 36/40
time = 10.69 secondes

Train loss 0.02754994116218189 accuracy 0.9961240291595459 macro_avg {'precision': 0.9947089947089947, 'recall': 0.9969604863221885, 'f1-score': 0.9958160352880125, 'support': 516} weighted_avg {'precision': 0.9961650465526435, 'recall': 0.9961240310077519, 'f1-score': 0.9961284309466054, 'support': 516}
 
time = 0.35 secondes

Val loss 1.2280444502830505 accuracy 0.828125 macro_avg {'precision': 0.822167487684729, 'recall': 0.8309716599190283, 'f1-score': 0.8246575342465753, 'support': 64} weighted_avg {'precision': 0.8340825123152709, 'recall': 0.828125, 'f1-score': 0.829280821917808, 'support': 64}
 
----------
Epoch 37/40
time = 10.29 secondes

Train loss 0.014171845321305393 accuracy 0.998062014579773 macro_avg {'precision': 0.9973404255319149, 'recall': 0.9984802431610942, 'f1-score': 0.9979056316590563, 'support': 516} weighted_avg {'precision': 0.9980723239320469, 'recall': 0.998062015503876, 'f1-score': 0.9980631246091585, 'support': 516}
 
time = 0.35 secondes

Val loss 1.8152450025081635 accuracy 0.765625 macro_avg {'precision': 0.7725146198830409, 'recall': 0.73582995951417, 'f1-score': 0.7429718875502007, 'support': 64} weighted_avg {'precision': 0.7693347953216374, 'recall': 0.765625, 'f1-score': 0.7572791164658634, 'support': 64}
 
----------
Epoch 38/40
time = 10.36 secondes

Train loss 0.0003322362203052211 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 1.9734496772289276 accuracy 0.78125 macro_avg {'precision': 0.8342857142857143, 'recall': 0.736842105263158, 'f1-score': 0.7454545454545455, 'support': 64} weighted_avg {'precision': 0.8166071428571429, 'recall': 0.78125, 'f1-score': 0.7633522727272728, 'support': 64}
 
----------
Epoch 39/40
time = 11.50 secondes

Train loss 0.05564295875184612 accuracy 0.9922480583190918 macro_avg {'precision': 0.993993993993994, 'recall': 0.9893048128342246, 'f1-score': 0.9915734465583409, 'support': 516} weighted_avg {'precision': 0.99234117838769, 'recall': 0.9922480620155039, 'f1-score': 0.9922295794002391, 'support': 516}
 
time = 0.38 secondes

Val loss 1.8968857526779175 accuracy 0.78125 macro_avg {'precision': 0.8125, 'recall': 0.742914979757085, 'f1-score': 0.751937984496124, 'support': 64} weighted_avg {'precision': 0.80078125, 'recall': 0.78125, 'f1-score': 0.7679263565891474, 'support': 64}
 
----------
Epoch 40/40
time = 12.34 secondes

Train loss 0.006754130474953193 accuracy 0.998062014579773 macro_avg {'precision': 0.9984848484848485, 'recall': 0.9973262032085561, 'f1-score': 0.997900792084847, 'support': 516} weighted_avg {'precision': 0.9980678881841673, 'recall': 0.998062015503876, 'f1-score': 0.9980608880673791, 'support': 516}
 
time = 0.36 secondes

Val loss 1.8082922101020813 accuracy 0.78125 macro_avg {'precision': 0.7971014492753623, 'recall': 0.7489878542510121, 'f1-score': 0.7575757575757576, 'support': 64} weighted_avg {'precision': 0.7903079710144928, 'recall': 0.78125, 'f1-score': 0.771780303030303, 'support': 64}
 
----------
best_accuracy 0.875 best_epoch 15 macro_avg {'precision': 0.8690476190476191, 'recall': 0.8765182186234818, 'f1-score': 0.8718718718718719, 'support': 64} weighted_avg {'precision': 0.8779761904761905, 'recall': 0.875, 'f1-score': 0.8756256256256256, 'support': 64}

average train time 11.060766321420669

average val time 0.35243916511535645
 
time = 0.38 secondes

test_accuracy 0.9384615421295166 macro_avg {'precision': 0.9366471734892787, 'recall': 0.9366471734892787, 'f1-score': 0.9366471734892787, 'support': 65} weighted_avg {'precision': 0.9384615384615385, 'recall': 0.9384615384615385, 'f1-score': 0.9384615384615385, 'support': 65}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_head_text_rank_1
----------
Epoch 1/40
time = 10.73 secondes

Train loss 0.6605424591989228 accuracy 0.6027131676673889 macro_avg {'precision': 0.48703416149068324, 'recall': 0.49457113599791946, 'f1-score': 0.44827802031012853, 'support': 516} weighted_avg {'precision': 0.5276936178920506, 'recall': 0.6027131782945736, 'f1-score': 0.5286069303240157, 'support': 516}
 
time = 0.35 secondes

Val loss 0.7096070349216461 accuracy 0.59375 macro_avg {'precision': 0.296875, 'recall': 0.5, 'f1-score': 0.37254901960784315, 'support': 64} weighted_avg {'precision': 0.3525390625, 'recall': 0.59375, 'f1-score': 0.4424019607843137, 'support': 64}
 
----------
Epoch 2/40
time = 10.68 secondes

Train loss 0.5625291674426107 accuracy 0.7170542478561401 macro_avg {'precision': 0.7548781703952399, 'recall': 0.6246281878321929, 'f1-score': 0.618768091006619, 'support': 516} weighted_avg {'precision': 0.7408179746628644, 'recall': 0.7170542635658915, 'f1-score': 0.6720376959814919, 'support': 516}
 
time = 0.36 secondes

Val loss 0.6052070260047913 accuracy 0.734375 macro_avg {'precision': 0.7760180995475113, 'recall': 0.6852226720647773, 'f1-score': 0.686545664073754, 'support': 64} weighted_avg {'precision': 0.7628676470588236, 'recall': 0.734375, 'f1-score': 0.7095037453183521, 'support': 64}
 
----------
Epoch 3/40
time = 10.65 secondes

Train loss 0.4443510680487662 accuracy 0.817829430103302 macro_avg {'precision': 0.8034756067542952, 'recall': 0.8005948994684915, 'f1-score': 0.8019759941210092, 'support': 516} weighted_avg {'precision': 0.8170639186651386, 'recall': 0.8178294573643411, 'f1-score': 0.8173951159056196, 'support': 516}
 
time = 0.35 secondes

Val loss 0.5995791852474213 accuracy 0.75 macro_avg {'precision': 0.7885714285714285, 'recall': 0.7044534412955465, 'f1-score': 0.709090909090909, 'support': 64} weighted_avg {'precision': 0.7757142857142857, 'recall': 0.75, 'f1-score': 0.7295454545454545, 'support': 64}
 
----------
Epoch 4/40
time = 10.66 secondes

Train loss 0.2858043744257002 accuracy 0.895348846912384 macro_avg {'precision': 0.8856222311719695, 'recall': 0.8890821318856363, 'f1-score': 0.8872870249017037, 'support': 516} weighted_avg {'precision': 0.8959295301070536, 'recall': 0.8953488372093024, 'f1-score': 0.8955825129283631, 'support': 516}
 
time = 0.35 secondes

Val loss 0.6013370901346207 accuracy 0.78125 macro_avg {'precision': 0.8125, 'recall': 0.742914979757085, 'f1-score': 0.751937984496124, 'support': 64} weighted_avg {'precision': 0.80078125, 'recall': 0.78125, 'f1-score': 0.7679263565891474, 'support': 64}
 
----------
Epoch 5/40
time = 10.69 secondes

Train loss 0.23052683912894942 accuracy 0.9263566136360168 macro_avg {'precision': 0.9182373794895651, 'recall': 0.9237846008809714, 'f1-score': 0.9208588957055215, 'support': 516} weighted_avg {'precision': 0.9271060546541534, 'recall': 0.9263565891472868, 'f1-score': 0.9265991344461882, 'support': 516}
 
time = 0.37 secondes

Val loss 0.7080079466104507 accuracy 0.828125 macro_avg {'precision': 0.827677624602333, 'recall': 0.812753036437247, 'f1-score': 0.8181348488762593, 'support': 64} weighted_avg {'precision': 0.8279758748674444, 'recall': 0.828125, 'f1-score': 0.8261269697752518, 'support': 64}
 
----------
Epoch 6/40
time = 10.46 secondes

Train loss 0.2600959141202497 accuracy 0.9166666865348816 macro_avg {'precision': 0.9159608792095181, 'recall': 0.9023373372559855, 'f1-score': 0.9084014845333587, 'support': 516} weighted_avg {'precision': 0.9165574376554412, 'recall': 0.9166666666666666, 'f1-score': 0.9159734578425828, 'support': 516}
 
time = 0.35 secondes

Val loss 0.7281335964798927 accuracy 0.78125 macro_avg {'precision': 0.775, 'recall': 0.7672064777327935, 'f1-score': 0.7702564102564102, 'support': 64} weighted_avg {'precision': 0.7796875000000001, 'recall': 0.78125, 'f1-score': 0.7796794871794871, 'support': 64}
 
----------
Epoch 7/40
time = 11.07 secondes

Train loss 0.25153084017449256 accuracy 0.9244186282157898 macro_avg {'precision': 0.9148561082523347, 'recall': 0.9245729239471417, 'f1-score': 0.9192115457957808, 'support': 516} weighted_avg {'precision': 0.926171728990948, 'recall': 0.9244186046511628, 'f1-score': 0.9248558386008514, 'support': 516}
 
time = 0.36 secondes

Val loss 0.9939314126968384 accuracy 0.765625 macro_avg {'precision': 0.8006802721088435, 'recall': 0.7236842105263157, 'f1-score': 0.7308662741799832, 'support': 64} weighted_avg {'precision': 0.7883078231292517, 'recall': 0.765625, 'f1-score': 0.7490012615643398, 'support': 64}
 
----------
Epoch 8/40
time = 10.66 secondes

Train loss 0.22304420317099852 accuracy 0.9418604373931885 macro_avg {'precision': 0.9399851570874906, 'recall': 0.9336345756871414, 'f1-score': 0.9366487689889995, 'support': 516} weighted_avg {'precision': 0.9417143372179319, 'recall': 0.9418604651162791, 'f1-score': 0.9416491801381461, 'support': 516}
 
time = 0.35 secondes

Val loss 0.8028735369443893 accuracy 0.828125 macro_avg {'precision': 0.8473684210526315, 'recall': 0.8006072874493927, 'f1-score': 0.811512717536814, 'support': 64} weighted_avg {'precision': 0.8384868421052631, 'recall': 0.828125, 'f1-score': 0.8220046854082999, 'support': 64}
 
----------
Epoch 9/40
time = 10.67 secondes

Train loss 0.2933635136084349 accuracy 0.9127907156944275 macro_avg {'precision': 0.9327996474735605, 'recall': 0.8831412642426409, 'f1-score': 0.9005546157305295, 'support': 516} weighted_avg {'precision': 0.9198847798759326, 'recall': 0.9127906976744186, 'f1-score': 0.9101541938301552, 'support': 516}
 
time = 0.35 secondes

Val loss 0.7286328300833702 accuracy 0.78125 macro_avg {'precision': 0.776470588235294, 'recall': 0.785425101214575, 'f1-score': 0.7777777777777777, 'support': 64} weighted_avg {'precision': 0.7908088235294117, 'recall': 0.78125, 'f1-score': 0.782986111111111, 'support': 64}
 
----------
Epoch 10/40
time = 10.67 secondes

Train loss 0.18073595213619145 accuracy 0.9554263353347778 macro_avg {'precision': 0.956668439598431, 'recall': 0.946580953464558, 'f1-score': 0.9512517713745866, 'support': 516} weighted_avg {'precision': 0.9555660909276917, 'recall': 0.9554263565891473, 'f1-score': 0.9551775402518555, 'support': 516}
 
time = 0.35 secondes

Val loss 0.51289277151227 accuracy 0.875 macro_avg {'precision': 0.8831168831168831, 'recall': 0.8582995951417004, 'f1-score': 0.8666666666666667, 'support': 64} weighted_avg {'precision': 0.8782467532467533, 'recall': 0.875, 'f1-score': 0.8729166666666667, 'support': 64}
 
----------
Epoch 11/40
time = 10.63 secondes

Train loss 0.3236457746419491 accuracy 0.9224806427955627 macro_avg {'precision': 0.9187103158241939, 'recall': 0.9126668075353932, 'f1-score': 0.9155316919853327, 'support': 516} weighted_avg {'precision': 0.9221868302071807, 'recall': 0.9224806201550387, 'f1-score': 0.9221989068508614, 'support': 516}
 
time = 0.35 secondes

Val loss 1.6352065801620483 accuracy 0.734375 macro_avg {'precision': 0.8036020583190394, 'recall': 0.6791497975708503, 'f1-score': 0.6768636768636769, 'support': 64} weighted_avg {'precision': 0.7838228987993139, 'recall': 0.734375, 'f1-score': 0.7024242649242649, 'support': 64}
 
----------
Epoch 12/40
time = 10.65 secondes

Train loss 0.3112282303557026 accuracy 0.9263566136360168 macro_avg {'precision': 0.9166811977870425, 'recall': 0.9272467207385856, 'f1-score': 0.9213654373666565, 'support': 516} weighted_avg {'precision': 0.92832446535615, 'recall': 0.9263565891472868, 'f1-score': 0.9268173108501143, 'support': 516}
 
time = 0.35 secondes

Val loss 1.32326939702034 accuracy 0.75 macro_avg {'precision': 0.7885714285714285, 'recall': 0.7044534412955465, 'f1-score': 0.709090909090909, 'support': 64} weighted_avg {'precision': 0.7757142857142857, 'recall': 0.75, 'f1-score': 0.7295454545454545, 'support': 64}
 
----------
Epoch 13/40
time = 10.66 secondes

Train loss 0.3267449806162128 accuracy 0.9302325248718262 macro_avg {'precision': 0.9245160346537067, 'recall': 0.9245160346537067, 'f1-score': 0.9245160346537067, 'support': 516} weighted_avg {'precision': 0.9302325581395349, 'recall': 0.9302325581395349, 'f1-score': 0.9302325581395349, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0308425575494766 accuracy 0.71875 macro_avg {'precision': 0.7428571428571429, 'recall': 0.6720647773279352, 'f1-score': 0.6727272727272726, 'support': 64} weighted_avg {'precision': 0.7348214285714285, 'recall': 0.71875, 'f1-score': 0.6957386363636363, 'support': 64}
 
----------
Epoch 14/40
time = 10.64 secondes

Train loss 0.19839362585634895 accuracy 0.9418604373931885 macro_avg {'precision': 0.9347920242544796, 'recall': 0.9405588154023699, 'f1-score': 0.9375201808201485, 'support': 516} weighted_avg {'precision': 0.9425129365804452, 'recall': 0.9418604651162791, 'f1-score': 0.9420519482469907, 'support': 516}
 
time = 0.35 secondes

Val loss 0.5362202543765306 accuracy 0.84375 macro_avg {'precision': 0.8484848484848485, 'recall': 0.8259109311740891, 'f1-score': 0.8333333333333333, 'support': 64} weighted_avg {'precision': 0.8456439393939394, 'recall': 0.84375, 'f1-score': 0.8411458333333333, 'support': 64}
 
----------
Epoch 15/40
time = 10.62 secondes

Train loss 0.12333103743466464 accuracy 0.9748061895370483 macro_avg {'precision': 0.9703896604938271, 'recall': 0.9756270012840726, 'f1-score': 0.9728955460286803, 'support': 516} weighted_avg {'precision': 0.9751407879940664, 'recall': 0.9748062015503876, 'f1-score': 0.9748759335037345, 'support': 516}
 
time = 0.35 secondes

Val loss 1.040875717997551 accuracy 0.75 macro_avg {'precision': 0.7885714285714285, 'recall': 0.7044534412955465, 'f1-score': 0.709090909090909, 'support': 64} weighted_avg {'precision': 0.7757142857142857, 'recall': 0.75, 'f1-score': 0.7295454545454545, 'support': 64}
 
----------
Epoch 16/40
time = 11.60 secondes

Train loss 0.17197537893485842 accuracy 0.961240291595459 macro_avg {'precision': 0.9665481386609144, 'recall': 0.9499861840287371, 'f1-score': 0.9573412698412698, 'support': 516} weighted_avg {'precision': 0.9621145406677256, 'recall': 0.9612403100775194, 'f1-score': 0.9608903962101637, 'support': 516}
 
time = 0.36 secondes

Val loss 0.655223973095417 accuracy 0.84375 macro_avg {'precision': 0.8380566801619433, 'recall': 0.8380566801619433, 'f1-score': 0.8380566801619433, 'support': 64} weighted_avg {'precision': 0.84375, 'recall': 0.84375, 'f1-score': 0.84375, 'support': 64}
 
----------
Epoch 17/40
time = 10.65 secondes

Train loss 0.19215096785178917 accuracy 0.9418604373931885 macro_avg {'precision': 0.9312843850817449, 'recall': 0.9497911350226744, 'f1-score': 0.9385363064608347, 'support': 516} weighted_avg {'precision': 0.9466089092134169, 'recall': 0.9418604651162791, 'f1-score': 0.9424698942031106, 'support': 516}
 
time = 0.35 secondes

Val loss 0.7975719557143748 accuracy 0.859375 macro_avg {'precision': 0.8558974358974358, 'recall': 0.8512145748987854, 'f1-score': 0.8533231474407945, 'support': 64} weighted_avg {'precision': 0.8588782051282051, 'recall': 0.859375, 'f1-score': 0.858909472880061, 'support': 64}
 
----------
Epoch 18/40
time = 10.65 secondes

Train loss 0.40742375708015804 accuracy 0.9166666865348816 macro_avg {'precision': 0.9312800459209299, 'recall': 0.8907969377306049, 'f1-score': 0.9058358195653373, 'support': 516} weighted_avg {'precision': 0.9213036043146541, 'recall': 0.9166666666666666, 'f1-score': 0.9146242783561304, 'support': 516}
 
time = 0.35 secondes

Val loss 1.2683569490909576 accuracy 0.78125 macro_avg {'precision': 0.78125, 'recall': 0.791497975708502, 'f1-score': 0.7793103448275862, 'support': 64} weighted_avg {'precision': 0.798828125, 'recall': 0.78125, 'f1-score': 0.7831896551724138, 'support': 64}
 
----------
Epoch 19/40
time = 10.64 secondes

Train loss 0.24012065043136704 accuracy 0.9496123790740967 macro_avg {'precision': 0.9615217504649021, 'recall': 0.931635323374998, 'f1-score': 0.9437955592794303, 'support': 516} weighted_avg {'precision': 0.9526210803296072, 'recall': 0.9496124031007752, 'f1-score': 0.9487714136326291, 'support': 516}
 
time = 0.35 secondes

Val loss 1.019637830555439 accuracy 0.8125 macro_avg {'precision': 0.8125, 'recall': 0.8238866396761133, 'f1-score': 0.8108374384236454, 'support': 64} weighted_avg {'precision': 0.830078125, 'recall': 0.8125, 'f1-score': 0.8141625615763547, 'support': 64}
 
----------
Epoch 20/40
time = 10.62 secondes

Train loss 0.10457050223185708 accuracy 0.9767441749572754 macro_avg {'precision': 0.972039974975537, 'recall': 0.9783007980755165, 'f1-score': 0.9750080723280594, 'support': 516} weighted_avg {'precision': 0.9771784209146016, 'recall': 0.9767441860465116, 'f1-score': 0.9768207792987963, 'support': 516}
 
time = 0.37 secondes

Val loss 1.2293119207024574 accuracy 0.796875 macro_avg {'precision': 0.8083743842364532, 'recall': 0.8168016194331984, 'f1-score': 0.7964276975776854, 'support': 64} weighted_avg {'precision': 0.8313731527093596, 'recall': 0.796875, 'f1-score': 0.7982169072669439, 'support': 64}
 
----------
Epoch 21/40
time = 10.92 secondes

Train loss 0.10795290177603337 accuracy 0.9767441749572754 macro_avg {'precision': 0.9748386782179023, 'recall': 0.9748386782179023, 'f1-score': 0.9748386782179023, 'support': 516} weighted_avg {'precision': 0.9767441860465116, 'recall': 0.9767441860465116, 'f1-score': 0.9767441860465116, 'support': 516}
 
time = 0.35 secondes

Val loss 0.8619604706764221 accuracy 0.84375 macro_avg {'precision': 0.8416666666666667, 'recall': 0.8319838056680162, 'f1-score': 0.8358974358974359, 'support': 64} weighted_avg {'precision': 0.8432291666666667, 'recall': 0.84375, 'f1-score': 0.8426282051282052, 'support': 64}
 
----------
Epoch 22/40
time = 12.45 secondes

Train loss 0.12227688830303536 accuracy 0.9689922332763672 macro_avg {'precision': 0.9674859149179391, 'recall': 0.965297531004665, 'f1-score': 0.9663734115347019, 'support': 516} weighted_avg {'precision': 0.9689509786608079, 'recall': 0.9689922480620154, 'f1-score': 0.9689558753324695, 'support': 516}
 
time = 0.35 secondes

Val loss 1.46234130859375 accuracy 0.765625 macro_avg {'precision': 0.8006802721088435, 'recall': 0.7236842105263157, 'f1-score': 0.7308662741799832, 'support': 64} weighted_avg {'precision': 0.7883078231292517, 'recall': 0.765625, 'f1-score': 0.7490012615643398, 'support': 64}
 
----------
Epoch 23/40
time = 11.02 secondes

Train loss 0.11042578250988189 accuracy 0.9786821603775024 macro_avg {'precision': 0.9729272959183674, 'recall': 0.9821286348194984, 'f1-score': 0.9771651104128867, 'support': 516} weighted_avg {'precision': 0.9795175555687392, 'recall': 0.9786821705426356, 'f1-score': 0.9787848287469044, 'support': 516}
 
time = 0.35 secondes

Val loss 1.938717544078827 accuracy 0.71875 macro_avg {'precision': 0.8392857142857143, 'recall': 0.6538461538461539, 'f1-score': 0.6395494367959951, 'support': 64} weighted_avg {'precision': 0.8091517857142858, 'recall': 0.71875, 'f1-score': 0.671229662077597, 'support': 64}
 
----------
Epoch 24/40
time = 10.63 secondes

Train loss 0.22593400507078817 accuracy 0.9593023061752319 macro_avg {'precision': 0.9564516129032259, 'recall': 0.9553906669050599, 'f1-score': 0.9559166337817881, 'support': 516} weighted_avg {'precision': 0.9592627323497541, 'recall': 0.9593023255813954, 'f1-score': 0.9592786494149644, 'support': 516}
 
time = 0.35 secondes

Val loss 1.5420648977160454 accuracy 0.75 macro_avg {'precision': 0.7658730158730158, 'recall': 0.771255060728745, 'f1-score': 0.7497556207233627, 'support': 64} weighted_avg {'precision': 0.7896825396825398, 'recall': 0.75, 'f1-score': 0.7512218963831867, 'support': 64}
 
----------
Epoch 25/40
time = 10.67 secondes

Train loss 0.09045697788524469 accuracy 0.9786821603775024 macro_avg {'precision': 0.9729272959183674, 'recall': 0.9821286348194984, 'f1-score': 0.9771651104128867, 'support': 516} weighted_avg {'precision': 0.9795175555687392, 'recall': 0.9786821705426356, 'f1-score': 0.9787848287469044, 'support': 516}
 
time = 0.35 secondes

Val loss 0.8360017957165837 accuracy 0.859375 macro_avg {'precision': 0.8616118769883351, 'recall': 0.8451417004048583, 'f1-score': 0.8512012399896668, 'support': 64} weighted_avg {'precision': 0.8601206256627784, 'recall': 0.859375, 'f1-score': 0.8577402479979334, 'support': 64}
 
----------
Epoch 26/40
time = 10.64 secondes

Train loss 0.04716549849024776 accuracy 0.9883720874786377 macro_avg {'precision': 0.9853725332259364, 'recall': 0.9897274190140273, 'f1-score': 0.9874763361001893, 'support': 516} weighted_avg {'precision': 0.9885511712201107, 'recall': 0.9883720930232558, 'f1-score': 0.9883980569920404, 'support': 516}
 
time = 0.35 secondes

Val loss 0.7768532559275627 accuracy 0.859375 macro_avg {'precision': 0.8616118769883351, 'recall': 0.8451417004048583, 'f1-score': 0.8512012399896668, 'support': 64} weighted_avg {'precision': 0.8601206256627784, 'recall': 0.859375, 'f1-score': 0.8577402479979334, 'support': 64}
 
----------
Epoch 27/40
time = 10.63 secondes

Train loss 0.12570070049894805 accuracy 0.9728682041168213 macro_avg {'precision': 0.9795918367346939, 'recall': 0.9625668449197862, 'f1-score': 0.9701388888888889, 'support': 516} weighted_avg {'precision': 0.9739756367663344, 'recall': 0.9728682170542635, 'f1-score': 0.9726232773471145, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0391270071268082 accuracy 0.828125 macro_avg {'precision': 0.822167487684729, 'recall': 0.8309716599190283, 'f1-score': 0.8246575342465753, 'support': 64} weighted_avg {'precision': 0.8340825123152709, 'recall': 0.828125, 'f1-score': 0.829280821917808, 'support': 64}
 
----------
Epoch 28/40
time = 10.60 secondes

Train loss 0.030955768020992928 accuracy 0.9903100728988647 macro_avg {'precision': 0.9889724961079398, 'recall': 0.9900931359003949, 'f1-score': 0.9895281582952815, 'support': 516} weighted_avg {'precision': 0.9903291858252575, 'recall': 0.9903100775193798, 'f1-score': 0.9903156230457919, 'support': 516}
 
time = 0.35 secondes

Val loss 1.2450924217700958 accuracy 0.828125 macro_avg {'precision': 0.8877551020408163, 'recall': 0.7884615384615384, 'f1-score': 0.8026352677319877, 'support': 64} weighted_avg {'precision': 0.8667091836734694, 'recall': 0.828125, 'f1-score': 0.8159342584805158, 'support': 64}
 
----------
Epoch 29/40
time = 10.61 secondes

Train loss 0.06264967258197651 accuracy 0.9903100728988647 macro_avg {'precision': 0.9869791666666667, 'recall': 0.9924012158054711, 'f1-score': 0.9895752100110309, 'support': 516} weighted_avg {'precision': 0.990562419250646, 'recall': 0.9903100775193798, 'f1-score': 0.9903368975014364, 'support': 516}
 
time = 0.35 secondes

Val loss 1.196727193892002 accuracy 0.828125 macro_avg {'precision': 0.822167487684729, 'recall': 0.8309716599190283, 'f1-score': 0.8246575342465753, 'support': 64} weighted_avg {'precision': 0.8340825123152709, 'recall': 0.828125, 'f1-score': 0.829280821917808, 'support': 64}
 
----------
Epoch 30/40
time = 10.61 secondes

Train loss 0.0006341303206599233 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 1.423650823533535 accuracy 0.8125 macro_avg {'precision': 0.8227272727272728, 'recall': 0.7874493927125505, 'f1-score': 0.7963944856839873, 'support': 64} weighted_avg {'precision': 0.8176136363636364, 'recall': 0.8125, 'f1-score': 0.8071314952279958, 'support': 64}
 
----------
Epoch 31/40
time = 10.59 secondes

Train loss 0.060525244215210536 accuracy 0.9864341020584106 macro_avg {'precision': 0.981958762886598, 'recall': 0.9893617021276595, 'f1-score': 0.9854373042079417, 'support': 516} weighted_avg {'precision': 0.9869235994565653, 'recall': 0.9864341085271318, 'f1-score': 0.9864857946770157, 'support': 516}
 
time = 0.35 secondes

Val loss 1.613580346107483 accuracy 0.8125 macro_avg {'precision': 0.8541666666666667, 'recall': 0.7753036437246963, 'f1-score': 0.787375415282392, 'support': 64} weighted_avg {'precision': 0.8385416666666667, 'recall': 0.8125, 'f1-score': 0.801079734219269, 'support': 64}
 
----------
Epoch 32/40
time = 10.63 secondes

Train loss 0.11006049698246925 accuracy 0.9825581312179565 macro_avg {'precision': 0.9866863905325444, 'recall': 0.9759358288770054, 'f1-score': 0.9809246061900556, 'support': 516} weighted_avg {'precision': 0.9830225677721205, 'recall': 0.9825581395348837, 'f1-score': 0.9824607766202913, 'support': 516}
 
time = 0.35 secondes

Val loss 1.3180142641067505 accuracy 0.828125 macro_avg {'precision': 0.8313782991202345, 'recall': 0.8431174089068827, 'f1-score': 0.827069516089413, 'support': 64} weighted_avg {'precision': 0.8508980938416422, 'recall': 0.828125, 'f1-score': 0.829602677474822, 'support': 64}
 
----------
Epoch 33/40
time = 10.63 secondes

Train loss 0.027684850373065496 accuracy 0.9941860437393188 macro_avg {'precision': 0.9931564608199274, 'recall': 0.9942866895307446, 'f1-score': 0.9937168949771689, 'support': 516} weighted_avg {'precision': 0.9942007548786522, 'recall': 0.9941860465116279, 'f1-score': 0.9941893738274752, 'support': 516}
 
time = 0.35 secondes

Val loss 1.4890284836292267 accuracy 0.84375 macro_avg {'precision': 0.8958333333333333, 'recall': 0.8076923076923077, 'f1-score': 0.8228128460686601, 'support': 64} weighted_avg {'precision': 0.8763020833333333, 'recall': 0.84375, 'f1-score': 0.834233111849391, 'support': 64}
 
----------
Epoch 34/40
time = 11.54 secondes

Train loss 0.041252435399766196 accuracy 0.9941860437393188 macro_avg {'precision': 0.9921052631578947, 'recall': 0.9954407294832827, 'f1-score': 0.9937311438232733, 'support': 516} weighted_avg {'precision': 0.9942778457772338, 'recall': 0.9941860465116279, 'f1-score': 0.9941958645552614, 'support': 516}
 
time = 0.35 secondes

Val loss 1.615382805466652 accuracy 0.78125 macro_avg {'precision': 0.7976190476190477, 'recall': 0.8036437246963564, 'f1-score': 0.7810361681329423, 'support': 64} weighted_avg {'precision': 0.8221726190476191, 'recall': 0.78125, 'f1-score': 0.7823191593352883, 'support': 64}
 
----------
Epoch 35/40
time = 10.62 secondes

Train loss 0.03317820132184407 accuracy 0.9941860437393188 macro_avg {'precision': 0.9921052631578947, 'recall': 0.9954407294832827, 'f1-score': 0.9937311438232733, 'support': 516} weighted_avg {'precision': 0.9942778457772338, 'recall': 0.9941860465116279, 'f1-score': 0.9941958645552614, 'support': 516}
 
time = 0.35 secondes

Val loss 1.7221191823482513 accuracy 0.796875 macro_avg {'precision': 0.8099415204678362, 'recall': 0.7682186234817814, 'f1-score': 0.7772423025435075, 'support': 64} weighted_avg {'precision': 0.8039108187134503, 'recall': 0.796875, 'f1-score': 0.7896419009370819, 'support': 64}
 
----------
Epoch 36/40
time = 10.64 secondes

Train loss 0.014369503070184059 accuracy 0.998062014579773 macro_avg {'precision': 0.9984848484848485, 'recall': 0.9973262032085561, 'f1-score': 0.997900792084847, 'support': 516} weighted_avg {'precision': 0.9980678881841673, 'recall': 0.998062015503876, 'f1-score': 0.9980608880673791, 'support': 516}
 
time = 0.35 secondes

Val loss 1.4072116911411285 accuracy 0.828125 macro_avg {'precision': 0.8355481727574751, 'recall': 0.8066801619433198, 'f1-score': 0.8150774888363541, 'support': 64} weighted_avg {'precision': 0.831499169435216, 'recall': 0.828125, 'f1-score': 0.8242874967165746, 'support': 64}
 
----------
Epoch 37/40
time = 10.63 secondes

Train loss 0.006484815638969215 accuracy 0.9961240291595459 macro_avg {'precision': 0.9947089947089947, 'recall': 0.9969604863221885, 'f1-score': 0.9958160352880125, 'support': 516} weighted_avg {'precision': 0.9961650465526435, 'recall': 0.9961240310077519, 'f1-score': 0.9961284309466054, 'support': 516}
 
time = 0.35 secondes

Val loss 1.5466705709695816 accuracy 0.796875 macro_avg {'precision': 0.8083743842364532, 'recall': 0.8168016194331984, 'f1-score': 0.7964276975776854, 'support': 64} weighted_avg {'precision': 0.8313731527093596, 'recall': 0.796875, 'f1-score': 0.7982169072669439, 'support': 64}
 
----------
Epoch 38/40
time = 10.62 secondes

Train loss 0.00014892297335653424 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 1.248579353094101 accuracy 0.84375 macro_avg {'precision': 0.8484848484848485, 'recall': 0.8259109311740891, 'f1-score': 0.8333333333333333, 'support': 64} weighted_avg {'precision': 0.8456439393939394, 'recall': 0.84375, 'f1-score': 0.8411458333333333, 'support': 64}
 
----------
Epoch 39/40
time = 10.62 secondes

Train loss 0.0001348562824811476 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 1.719920575618744 accuracy 0.8125 macro_avg {'precision': 0.8541666666666667, 'recall': 0.7753036437246963, 'f1-score': 0.787375415282392, 'support': 64} weighted_avg {'precision': 0.8385416666666667, 'recall': 0.8125, 'f1-score': 0.801079734219269, 'support': 64}
 
----------
Epoch 40/40
time = 10.56 secondes

Train loss 0.00013130963237596336 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 1.6792640686035156 accuracy 0.828125 macro_avg {'precision': 0.8877551020408163, 'recall': 0.7884615384615384, 'f1-score': 0.8026352677319877, 'support': 64} weighted_avg {'precision': 0.8667091836734694, 'recall': 0.828125, 'f1-score': 0.8159342584805158, 'support': 64}
 
----------
best_accuracy 0.875 best_epoch 10 macro_avg {'precision': 0.8831168831168831, 'recall': 0.8582995951417004, 'f1-score': 0.8666666666666667, 'support': 64} weighted_avg {'precision': 0.8782467532467533, 'recall': 0.875, 'f1-score': 0.8729166666666667, 'support': 64}

average train time 10.754409682750701

average val time 0.35053388476371766
 
time = 0.37 secondes

test_accuracy 0.9384615421295166 macro_avg {'precision': 0.9523809523809523, 'recall': 0.9259259259259259, 'f1-score': 0.935, 'support': 65} weighted_avg {'precision': 0.9443223443223443, 'recall': 0.9384615384615385, 'f1-score': 0.9375384615384615, 'support': 65}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_head_tail_1
----------
Epoch 1/40
time = 11.34 secondes

Train loss 0.6596690813700358 accuracy 0.6317829489707947 macro_avg {'precision': 0.5304458328920894, 'recall': 0.5046730491035872, 'f1-score': 0.4238228449187804, 'support': 516} weighted_avg {'precision': 0.560550163486016, 'recall': 0.6317829457364341, 'f1-score': 0.5190819878739638, 'support': 516}
 
time = 0.41 secondes

Val loss 0.6315710246562958 accuracy 0.59375 macro_avg {'precision': 0.296875, 'recall': 0.5, 'f1-score': 0.37254901960784315, 'support': 64} weighted_avg {'precision': 0.3525390625, 'recall': 0.59375, 'f1-score': 0.4424019607843137, 'support': 64}
 
----------
Epoch 2/40
time = 11.50 secondes

Train loss 0.48377058632446057 accuracy 0.7751938104629517 macro_avg {'precision': 0.7668872682277508, 'recall': 0.731385010483884, 'f1-score': 0.7416213416213415, 'support': 516} weighted_avg {'precision': 0.7720156477560305, 'recall': 0.7751937984496124, 'f1-score': 0.7672519269418493, 'support': 516}
 
time = 0.36 secondes

Val loss 0.39634618908166885 accuracy 0.84375 macro_avg {'precision': 0.8416666666666667, 'recall': 0.8319838056680162, 'f1-score': 0.8358974358974359, 'support': 64} weighted_avg {'precision': 0.8432291666666667, 'recall': 0.84375, 'f1-score': 0.8426282051282052, 'support': 64}
 
----------
Epoch 3/40
time = 11.90 secondes

Train loss 0.40346768001715344 accuracy 0.819767415523529 macro_avg {'precision': 0.80875552533152, 'recall': 0.7951904165921688, 'f1-score': 0.800859010270775, 'support': 516} weighted_avg {'precision': 0.8176401398037371, 'recall': 0.8197674418604651, 'f1-score': 0.8177457856527623, 'support': 516}
 
time = 0.36 secondes

Val loss 0.45120906084775925 accuracy 0.828125 macro_avg {'precision': 0.8642052565707135, 'recall': 0.7945344129554657, 'f1-score': 0.8073871409028728, 'support': 64} weighted_avg {'precision': 0.849773153942428, 'recall': 0.828125, 'f1-score': 0.8192373461012312, 'support': 64}
 
----------
Epoch 4/40
time = 11.81 secondes

Train loss 0.23361514147483942 accuracy 0.9089147448539734 macro_avg {'precision': 0.9018084066471164, 'recall': 0.9008744697105148, 'f1-score': 0.9013372279878116, 'support': 516} weighted_avg {'precision': 0.908816029765017, 'recall': 0.9089147286821705, 'f1-score': 0.9088617391668254, 'support': 516}
 
time = 0.36 secondes

Val loss 0.40084467828273773 accuracy 0.84375 macro_avg {'precision': 0.8380566801619433, 'recall': 0.8380566801619433, 'f1-score': 0.8380566801619433, 'support': 64} weighted_avg {'precision': 0.84375, 'recall': 0.84375, 'f1-score': 0.84375, 'support': 64}
 
----------
Epoch 5/40
time = 10.54 secondes

Train loss 0.1643663334914229 accuracy 0.9515503644943237 macro_avg {'precision': 0.9490243583027763, 'recall': 0.9458495196918226, 'f1-score': 0.9473965363269734, 'support': 516} weighted_avg {'precision': 0.9514479810038943, 'recall': 0.9515503875968992, 'f1-score': 0.9514644458464872, 'support': 516}
 
time = 0.35 secondes

Val loss 0.5240101963281631 accuracy 0.84375 macro_avg {'precision': 0.8590909090909091, 'recall': 0.819838056680162, 'f1-score': 0.8303287380699893, 'support': 64} weighted_avg {'precision': 0.8514204545454547, 'recall': 0.84375, 'f1-score': 0.8392762460233298, 'support': 64}
 
----------
Epoch 6/40
time = 10.49 secondes

Train loss 0.2643001386895776 accuracy 0.9263566136360168 macro_avg {'precision': 0.9195670113101306, 'recall': 0.9214765209758952, 'f1-score': 0.9205046704722366, 'support': 516} weighted_avg {'precision': 0.9265533885048856, 'recall': 0.9263565891472868, 'f1-score': 0.9264401879855018, 'support': 516}
 
time = 0.35 secondes

Val loss 0.9362668544054031 accuracy 0.78125 macro_avg {'precision': 0.8653846153846154, 'recall': 0.7307692307692308, 'f1-score': 0.7380116959064327, 'support': 64} weighted_avg {'precision': 0.8401442307692307, 'recall': 0.78125, 'f1-score': 0.7579678362573098, 'support': 64}
 
----------
Epoch 7/40
time = 10.43 secondes

Train loss 0.2164792153476314 accuracy 0.9302325248718262 macro_avg {'precision': 0.923028594442207, 'recall': 0.9268241145587829, 'f1-score': 0.924858016601136, 'support': 516} weighted_avg {'precision': 0.93066264552445, 'recall': 0.9302325581395349, 'f1-score': 0.9303883419522422, 'support': 516}
 
time = 0.35 secondes

Val loss 0.6410560756921768 accuracy 0.828125 macro_avg {'precision': 0.8473684210526315, 'recall': 0.8006072874493927, 'f1-score': 0.811512717536814, 'support': 64} weighted_avg {'precision': 0.8384868421052631, 'recall': 0.828125, 'f1-score': 0.8220046854082999, 'support': 64}
 
----------
Epoch 8/40
time = 10.47 secondes

Train loss 0.16884852570453376 accuracy 0.9573643207550049 macro_avg {'precision': 0.9570050300981281, 'recall': 0.9504087902085399, 'f1-score': 0.953542430591933, 'support': 516} weighted_avg {'precision': 0.9573363428265328, 'recall': 0.9573643410852714, 'f1-score': 0.9572093987679738, 'support': 516}
 
time = 0.35 secondes

Val loss 0.8910077214241028 accuracy 0.78125 macro_avg {'precision': 0.8653846153846154, 'recall': 0.7307692307692308, 'f1-score': 0.7380116959064327, 'support': 64} weighted_avg {'precision': 0.8401442307692307, 'recall': 0.78125, 'f1-score': 0.7579678362573098, 'support': 64}
 
----------
Epoch 9/40
time = 10.45 secondes

Train loss 0.09133738254648492 accuracy 0.9767441749572754 macro_avg {'precision': 0.9759124683595983, 'recall': 0.9736846382653641, 'f1-score': 0.9747800586510263, 'support': 516} weighted_avg {'precision': 0.9767213992605689, 'recall': 0.9767441860465116, 'f1-score': 0.9767169064993521, 'support': 516}
 
time = 0.36 secondes

Val loss 0.8879483863711357 accuracy 0.796875 macro_avg {'precision': 0.793743372216331, 'recall': 0.7803643724696356, 'f1-score': 0.785068457762852, 'support': 64} weighted_avg {'precision': 0.7958311240721103, 'recall': 0.796875, 'f1-score': 0.7945136915525703, 'support': 64}
 
----------
Epoch 10/40
time = 10.44 secondes

Train loss 0.1072948119165658 accuracy 0.9748061895370483 macro_avg {'precision': 0.9743648507071765, 'recall': 0.9710108414739204, 'f1-score': 0.9726461988900261, 'support': 516} weighted_avg {'precision': 0.9747883089486359, 'recall': 0.9748062015503876, 'f1-score': 0.9747615118401732, 'support': 516}
 
time = 0.36 secondes

Val loss 0.737784743309021 accuracy 0.796875 macro_avg {'precision': 0.8442176870748299, 'recall': 0.7560728744939271, 'f1-score': 0.7667507709559854, 'support': 64} weighted_avg {'precision': 0.8275085034013605, 'recall': 0.796875, 'f1-score': 0.7824677600224279, 'support': 64}
 
----------
Epoch 11/40
time = 10.48 secondes

Train loss 0.2665049189094906 accuracy 0.9437984228134155 macro_avg {'precision': 0.9594972067039106, 'recall': 0.9224598930481284, 'f1-score': 0.9368647553952281, 'support': 516} weighted_avg {'precision': 0.9483510891689403, 'recall': 0.9437984496124031, 'f1-score': 0.9426225599498413, 'support': 516}
 
time = 0.35 secondes

Val loss 0.5832408592104912 accuracy 0.859375 macro_avg {'precision': 0.8847953216374269, 'recall': 0.832995951417004, 'f1-score': 0.8457831325301204, 'support': 64} weighted_avg {'precision': 0.873062865497076, 'recall': 0.859375, 'f1-score': 0.8543674698795181, 'support': 64}
 
----------
Epoch 12/40
time = 10.47 secondes

Train loss 0.04179432731171167 accuracy 0.9922480583190918 macro_avg {'precision': 0.993993993993994, 'recall': 0.9893048128342246, 'f1-score': 0.9915734465583409, 'support': 516} weighted_avg {'precision': 0.99234117838769, 'recall': 0.9922480620155039, 'f1-score': 0.9922295794002391, 'support': 516}
 
time = 0.36 secondes

Val loss 0.6623151302337646 accuracy 0.828125 macro_avg {'precision': 0.8355481727574751, 'recall': 0.8066801619433198, 'f1-score': 0.8150774888363541, 'support': 64} weighted_avg {'precision': 0.831499169435216, 'recall': 0.828125, 'f1-score': 0.8242874967165746, 'support': 64}
 
----------
Epoch 13/40
time = 10.46 secondes

Train loss 0.24033935804442136 accuracy 0.9534883499145508 macro_avg {'precision': 0.9453101469492573, 'recall': 0.956601596151033, 'f1-score': 0.9503360657052567, 'support': 516} weighted_avg {'precision': 0.9551517399188739, 'recall': 0.9534883720930233, 'f1-score': 0.9537793542211248, 'support': 516}
 
time = 0.36 secondes

Val loss 1.2002610564231873 accuracy 0.78125 macro_avg {'precision': 0.8653846153846154, 'recall': 0.7307692307692308, 'f1-score': 0.7380116959064327, 'support': 64} weighted_avg {'precision': 0.8401442307692307, 'recall': 0.78125, 'f1-score': 0.7579678362573098, 'support': 64}
 
----------
Epoch 14/40
time = 10.47 secondes

Train loss 0.31955492023420945 accuracy 0.9166666865348816 macro_avg {'precision': 0.912797619047619, 'recall': 0.9057994571135999, 'f1-score': 0.9090860666653009, 'support': 516} weighted_avg {'precision': 0.9163194444444446, 'recall': 0.9166666666666666, 'f1-score': 0.9163105310961327, 'support': 516}
 
time = 0.35 secondes

Val loss 1.6759136319160461 accuracy 0.703125 macro_avg {'precision': 0.8333333333333333, 'recall': 0.6346153846153846, 'f1-score': 0.6121212121212122, 'support': 64} weighted_avg {'precision': 0.8020833333333333, 'recall': 0.703125, 'f1-score': 0.647348484848485, 'support': 64}
 
----------
Epoch 15/40
time = 10.47 secondes

Train loss 0.5836912018959551 accuracy 0.8779069781303406 macro_avg {'precision': 0.8681818181818182, 'recall': 0.8673260406677177, 'f1-score': 0.8677499013453646, 'support': 516} weighted_avg {'precision': 0.8777719050974866, 'recall': 0.877906976744186, 'f1-score': 0.8778359482448937, 'support': 516}
 
time = 0.35 secondes

Val loss 0.6547604948282242 accuracy 0.828125 macro_avg {'precision': 0.8213213213213213, 'recall': 0.8248987854251012, 'f1-score': 0.8228930817610063, 'support': 64} weighted_avg {'precision': 0.8294857357357358, 'recall': 0.828125, 'f1-score': 0.8286006289308177, 'support': 64}
 
----------
Epoch 16/40
time = 10.50 secondes

Train loss 0.3467303457966244 accuracy 0.9379844665527344 macro_avg {'precision': 0.9281071467917342, 'recall': 0.9421354615347106, 'f1-score': 0.9340522405942968, 'support': 516} weighted_avg {'precision': 0.9408579068388809, 'recall': 0.937984496124031, 'f1-score': 0.9384838301595528, 'support': 516}
 
time = 0.35 secondes

Val loss 1.7186913639307022 accuracy 0.75 macro_avg {'precision': 0.7658730158730158, 'recall': 0.771255060728745, 'f1-score': 0.7497556207233627, 'support': 64} weighted_avg {'precision': 0.7896825396825398, 'recall': 0.75, 'f1-score': 0.7512218963831867, 'support': 64}
 
----------
Epoch 17/40
time = 10.49 secondes

Train loss 0.169725679185693 accuracy 0.9709302186965942 macro_avg {'precision': 0.9701414353064431, 'recall': 0.9668172878435708, 'f1-score': 0.968437921796184, 'support': 516} weighted_avg {'precision': 0.9708982542911787, 'recall': 0.9709302325581395, 'f1-score': 0.9708786675078922, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0846027955412865 accuracy 0.8125 macro_avg {'precision': 0.8055555555555556, 'recall': 0.811740890688259, 'f1-score': 0.8078078078078078, 'support': 64} weighted_avg {'precision': 0.8159722222222222, 'recall': 0.8125, 'f1-score': 0.8134384384384383, 'support': 64}
 
----------
Epoch 18/40
time = 10.43 secondes

Train loss 0.14022537297216442 accuracy 0.9689922332763672 macro_avg {'precision': 0.9752439373767674, 'recall': 0.9583732912894365, 'f1-score': 0.9658730158730159, 'support': 516} weighted_avg {'precision': 0.9700219380667982, 'recall': 0.9689922480620154, 'f1-score': 0.968712316968131, 'support': 516}
 
time = 0.35 secondes

Val loss 1.3557828217744827 accuracy 0.765625 macro_avg {'precision': 0.7591133004926109, 'recall': 0.7661943319838057, 'f1-score': 0.7608966376089665, 'support': 64} weighted_avg {'precision': 0.7721366995073893, 'recall': 0.765625, 'f1-score': 0.7672011207970113, 'support': 64}
 
----------
Epoch 19/40
time = 10.46 secondes

Train loss 0.23763555962465366 accuracy 0.9496123790740967 macro_avg {'precision': 0.9438099073701167, 'recall': 0.9477918827105309, 'f1-score': 0.945730789767487, 'support': 516} weighted_avg {'precision': 0.9499588207563369, 'recall': 0.9496124031007752, 'f1-score': 0.9497249136321749, 'support': 516}
 
time = 0.35 secondes

Val loss 1.1842447817325592 accuracy 0.765625 macro_avg {'precision': 0.8242835595776772, 'recall': 0.7176113360323887, 'f1-score': 0.7234226447709595, 'support': 64} weighted_avg {'precision': 0.8057598039215685, 'recall': 0.765625, 'f1-score': 0.743679775280899, 'support': 64}
 
----------
Epoch 20/40
time = 11.17 secondes

Train loss 0.1668158994183283 accuracy 0.9670542478561401 macro_avg {'precision': 0.9593354430379747, 'recall': 0.9718560538335257, 'f1-score': 0.9648578811369508, 'support': 516} weighted_avg {'precision': 0.9687843440290451, 'recall': 0.9670542635658915, 'f1-score': 0.9672756044308234, 'support': 516}
 
time = 0.36 secondes

Val loss 1.0061602741479874 accuracy 0.828125 macro_avg {'precision': 0.827677624602333, 'recall': 0.812753036437247, 'f1-score': 0.8181348488762593, 'support': 64} weighted_avg {'precision': 0.8279758748674444, 'recall': 0.828125, 'f1-score': 0.8261269697752518, 'support': 64}
 
----------
Epoch 21/40
time = 10.47 secondes

Train loss 0.2946986602248878 accuracy 0.9476743936538696 macro_avg {'precision': 0.9620786516853932, 'recall': 0.927807486631016, 'f1-score': 0.941387071667473, 'support': 516} weighted_avg {'precision': 0.9516429318003659, 'recall': 0.9476744186046512, 'f1-score': 0.9466699312241554, 'support': 516}
 
time = 0.36 secondes

Val loss 1.5772504061460495 accuracy 0.78125 macro_avg {'precision': 0.78125, 'recall': 0.791497975708502, 'f1-score': 0.7793103448275862, 'support': 64} weighted_avg {'precision': 0.798828125, 'recall': 0.78125, 'f1-score': 0.7831896551724138, 'support': 64}
 
----------
Epoch 22/40
time = 10.46 secondes

Train loss 0.03525397430039999 accuracy 0.9844961166381836 macro_avg {'precision': 0.982185330809184, 'recall': 0.9843798254311396, 'f1-score': 0.9832641411520499, 'support': 516} weighted_avg {'precision': 0.9845631035446839, 'recall': 0.9844961240310077, 'f1-score': 0.9845137237864214, 'support': 516}
 
time = 0.35 secondes

Val loss 0.9561221450567245 accuracy 0.84375 macro_avg {'precision': 0.8743961352657005, 'recall': 0.8137651821862348, 'f1-score': 0.8268398268398268, 'support': 64} weighted_avg {'precision': 0.861262077294686, 'recall': 0.84375, 'f1-score': 0.8369859307359306, 'support': 64}
 
----------
Epoch 23/40
time = 10.49 secondes

Train loss 0.017910579944410445 accuracy 0.9961240291595459 macro_avg {'precision': 0.9969788519637462, 'recall': 0.9946524064171123, 'f1-score': 0.9957966764418378, 'support': 516} weighted_avg {'precision': 0.9961474507599709, 'recall': 0.9961240310077519, 'f1-score': 0.9961194844165587, 'support': 516}
 
time = 0.36 secondes

Val loss 1.368307113647461 accuracy 0.8125 macro_avg {'precision': 0.8357487922705313, 'recall': 0.7813765182186234, 'f1-score': 0.7922077922077922, 'support': 64} weighted_avg {'precision': 0.8257850241545894, 'recall': 0.8125, 'f1-score': 0.8043831168831169, 'support': 64}
 
----------
Epoch 24/40
time = 10.49 secondes

Train loss 0.03583230640100682 accuracy 0.9903100728988647 macro_avg {'precision': 0.990078201368524, 'recall': 0.988939095947857, 'f1-score': 0.9895039604242353, 'support': 516} weighted_avg {'precision': 0.9903068570172846, 'recall': 0.9903100775193798, 'f1-score': 0.9903044403368964, 'support': 516}
 
time = 0.37 secondes

Val loss 1.2778160125017166 accuracy 0.828125 macro_avg {'precision': 0.8642052565707135, 'recall': 0.7945344129554657, 'f1-score': 0.8073871409028728, 'support': 64} weighted_avg {'precision': 0.849773153942428, 'recall': 0.828125, 'f1-score': 0.8192373461012312, 'support': 64}
 
----------
Epoch 25/40
time = 10.48 secondes

Train loss 0.06816138484194226 accuracy 0.9864341020584106 macro_avg {'precision': 0.9828317901234568, 'recall': 0.9882076621751215, 'f1-score': 0.9854052940154432, 'support': 516} weighted_avg {'precision': 0.9867070114365011, 'recall': 0.9864341085271318, 'f1-score': 0.986471656502011, 'support': 516}
 
time = 0.36 secondes

Val loss 1.4257367104291916 accuracy 0.765625 macro_avg {'precision': 0.7598091198303287, 'recall': 0.7479757085020242, 'f1-score': 0.7520020666494445, 'support': 64} weighted_avg {'precision': 0.7636863732767762, 'recall': 0.765625, 'f1-score': 0.7629004133298889, 'support': 64}
 
----------
Epoch 26/40
time = 10.45 secondes

Train loss 0.05177163622945293 accuracy 0.9903100728988647 macro_avg {'precision': 0.9925149700598803, 'recall': 0.9866310160427807, 'f1-score': 0.9894541931844658, 'support': 516} weighted_avg {'precision': 0.9904551362391496, 'recall': 0.9903100775193798, 'f1-score': 0.990280965807308, 'support': 516}
 
time = 0.35 secondes

Val loss 1.422672763466835 accuracy 0.8125 macro_avg {'precision': 0.8125, 'recall': 0.8238866396761133, 'f1-score': 0.8108374384236454, 'support': 64} weighted_avg {'precision': 0.830078125, 'recall': 0.8125, 'f1-score': 0.8141625615763547, 'support': 64}
 
----------
Epoch 27/40
time = 10.50 secondes

Train loss 0.1287015793019567 accuracy 0.9806201457977295 macro_avg {'precision': 0.975365444524323, 'recall': 0.9836483916584042, 'f1-score': 0.9792186870720903, 'support': 516} weighted_avg {'precision': 0.9812874198659898, 'recall': 0.9806201550387597, 'f1-score': 0.9807038247681131, 'support': 516}
 
time = 0.35 secondes

Val loss 1.2483865022659302 accuracy 0.8125 macro_avg {'precision': 0.88, 'recall': 0.7692307692307692, 'f1-score': 0.7818181818181819, 'support': 64} weighted_avg {'precision': 0.8574999999999999, 'recall': 0.8125, 'f1-score': 0.797159090909091, 'support': 64}
 
----------
Epoch 28/40
time = 10.51 secondes

Train loss 0.10064396007201691 accuracy 0.9825581312179565 macro_avg {'precision': 0.9840264525893269, 'recall': 0.9782439087820816, 'f1-score': 0.9810175477320384, 'support': 516} weighted_avg {'precision': 0.9826547390779392, 'recall': 0.9825581395348837, 'f1-score': 0.9825057384531543, 'support': 516}
 
time = 0.35 secondes

Val loss 1.1951640993356705 accuracy 0.8125 macro_avg {'precision': 0.807843137254902, 'recall': 0.8178137651821862, 'f1-score': 0.8095238095238094, 'support': 64} weighted_avg {'precision': 0.821813725490196, 'recall': 0.8125, 'f1-score': 0.8139880952380951, 'support': 64}
 
----------
Epoch 29/40
time = 11.46 secondes

Train loss 0.05974671576404944 accuracy 0.9844961166381836 macro_avg {'precision': 0.9881305637982196, 'recall': 0.9786096256684492, 'f1-score': 0.98306503224536, 'support': 516} weighted_avg {'precision': 0.9848641685643963, 'recall': 0.9844961240310077, 'f1-score': 0.9844197991357732, 'support': 516}
 
time = 0.36 secondes

Val loss 1.0069537237286568 accuracy 0.84375 macro_avg {'precision': 0.8958333333333333, 'recall': 0.8076923076923077, 'f1-score': 0.8228128460686601, 'support': 64} weighted_avg {'precision': 0.8763020833333333, 'recall': 0.84375, 'f1-score': 0.834233111849391, 'support': 64}
 
----------
Epoch 30/40
time = 11.99 secondes

Train loss 0.013963271630045987 accuracy 0.998062014579773 macro_avg {'precision': 0.9984848484848485, 'recall': 0.9973262032085561, 'f1-score': 0.997900792084847, 'support': 516} weighted_avg {'precision': 0.9980678881841673, 'recall': 0.998062015503876, 'f1-score': 0.9980608880673791, 'support': 516}
 
time = 0.36 secondes

Val loss 1.2008270472288132 accuracy 0.828125 macro_avg {'precision': 0.822167487684729, 'recall': 0.8309716599190283, 'f1-score': 0.8246575342465753, 'support': 64} weighted_avg {'precision': 0.8340825123152709, 'recall': 0.828125, 'f1-score': 0.829280821917808, 'support': 64}
 
----------
Epoch 31/40
time = 11.77 secondes

Train loss 0.09304859910502494 accuracy 0.9806201457977295 macro_avg {'precision': 0.9825348396140843, 'recall': 0.9755701119906377, 'f1-score': 0.9788829229963332, 'support': 516} weighted_avg {'precision': 0.9807693512394343, 'recall': 0.9806201550387597, 'f1-score': 0.9805497267127153, 'support': 516}
 
time = 0.36 secondes

Val loss 1.294882819056511 accuracy 0.8125 macro_avg {'precision': 0.88, 'recall': 0.7692307692307692, 'f1-score': 0.7818181818181819, 'support': 64} weighted_avg {'precision': 0.8574999999999999, 'recall': 0.8125, 'f1-score': 0.797159090909091, 'support': 64}
 
----------
Epoch 32/40
time = 10.45 secondes

Train loss 0.05575115081437892 accuracy 0.9883720874786377 macro_avg {'precision': 0.991044776119403, 'recall': 0.9839572192513368, 'f1-score': 0.9873297537977999, 'support': 516} weighted_avg {'precision': 0.9885803540437349, 'recall': 0.9883720930232558, 'f1-score': 0.9883298360276292, 'support': 516}
 
time = 0.35 secondes

Val loss 1.6974951773881912 accuracy 0.796875 macro_avg {'precision': 0.7942326490713587, 'recall': 0.8046558704453441, 'f1-score': 0.7944156165060539, 'support': 64} weighted_avg {'precision': 0.8100867546432062, 'recall': 0.796875, 'f1-score': 0.7986317024956758, 'support': 64}
 
----------
Epoch 33/40
time = 10.43 secondes

Train loss 0.0362693121247558 accuracy 0.9941860437393188 macro_avg {'precision': 0.9931564608199274, 'recall': 0.9942866895307446, 'f1-score': 0.9937168949771689, 'support': 516} weighted_avg {'precision': 0.9942007548786522, 'recall': 0.9941860465116279, 'f1-score': 0.9941893738274752, 'support': 516}
 
time = 0.36 secondes

Val loss 1.061596155166626 accuracy 0.84375 macro_avg {'precision': 0.8590909090909091, 'recall': 0.819838056680162, 'f1-score': 0.8303287380699893, 'support': 64} weighted_avg {'precision': 0.8514204545454547, 'recall': 0.84375, 'f1-score': 0.8392762460233298, 'support': 64}
 
----------
Epoch 34/40
time = 10.24 secondes

Train loss 0.024572963094352886 accuracy 0.9961240291595459 macro_avg {'precision': 0.9969788519637462, 'recall': 0.9946524064171123, 'f1-score': 0.9957966764418378, 'support': 516} weighted_avg {'precision': 0.9961474507599709, 'recall': 0.9961240310077519, 'f1-score': 0.9961194844165587, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0496569573879242 accuracy 0.828125 macro_avg {'precision': 0.8877551020408163, 'recall': 0.7884615384615384, 'f1-score': 0.8026352677319877, 'support': 64} weighted_avg {'precision': 0.8667091836734694, 'recall': 0.828125, 'f1-score': 0.8159342584805158, 'support': 64}
 
----------
Epoch 35/40
time = 10.25 secondes

Train loss 0.004041581963203057 accuracy 0.998062014579773 macro_avg {'precision': 0.9984848484848485, 'recall': 0.9973262032085561, 'f1-score': 0.997900792084847, 'support': 516} weighted_avg {'precision': 0.9980678881841673, 'recall': 0.998062015503876, 'f1-score': 0.9980608880673791, 'support': 516}
 
time = 0.36 secondes

Val loss 0.8411343768239021 accuracy 0.84375 macro_avg {'precision': 0.8590909090909091, 'recall': 0.819838056680162, 'f1-score': 0.8303287380699893, 'support': 64} weighted_avg {'precision': 0.8514204545454547, 'recall': 0.84375, 'f1-score': 0.8392762460233298, 'support': 64}
 
----------
Epoch 36/40
time = 10.44 secondes

Train loss 0.001561227425664776 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0890958309173584 accuracy 0.84375 macro_avg {'precision': 0.8958333333333333, 'recall': 0.8076923076923077, 'f1-score': 0.8228128460686601, 'support': 64} weighted_avg {'precision': 0.8763020833333333, 'recall': 0.84375, 'f1-score': 0.834233111849391, 'support': 64}
 
----------
Epoch 37/40
time = 10.48 secondes

Train loss 0.00037296538137139356 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 0.9494830518960953 accuracy 0.875 macro_avg {'precision': 0.9130434782608696, 'recall': 0.8461538461538461, 'f1-score': 0.8614718614718614, 'support': 64} weighted_avg {'precision': 0.8967391304347826, 'recall': 0.875, 'f1-score': 0.8695887445887445, 'support': 64}
 
----------
Epoch 38/40
time = 10.48 secondes

Train loss 0.00023624253993726927 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 1.057065188884735 accuracy 0.875 macro_avg {'precision': 0.8954545454545455, 'recall': 0.8522267206477733, 'f1-score': 0.8642629904559915, 'support': 64} weighted_avg {'precision': 0.8852272727272728, 'recall': 0.875, 'f1-score': 0.8714209968186639, 'support': 64}
 
----------
Epoch 39/40
time = 10.48 secondes

Train loss 0.00015829471191285518 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.36 secondes

Val loss 1.154270812869072 accuracy 0.84375 macro_avg {'precision': 0.8484848484848485, 'recall': 0.8259109311740891, 'f1-score': 0.8333333333333333, 'support': 64} weighted_avg {'precision': 0.8456439393939394, 'recall': 0.84375, 'f1-score': 0.8411458333333333, 'support': 64}
 
----------
Epoch 40/40
time = 10.47 secondes

Train loss 0.01665398100047958 accuracy 0.998062014579773 macro_avg {'precision': 0.9973404255319149, 'recall': 0.9984802431610942, 'f1-score': 0.9979056316590563, 'support': 516} weighted_avg {'precision': 0.9980723239320469, 'recall': 0.998062015503876, 'f1-score': 0.9980631246091585, 'support': 516}
 
time = 0.35 secondes

Val loss 1.1452261954545975 accuracy 0.84375 macro_avg {'precision': 0.8484848484848485, 'recall': 0.8259109311740891, 'f1-score': 0.8333333333333333, 'support': 64} weighted_avg {'precision': 0.8456439393939394, 'recall': 0.84375, 'f1-score': 0.8411458333333333, 'support': 64}
 
----------
best_accuracy 0.875 best_epoch 37 macro_avg {'precision': 0.9130434782608696, 'recall': 0.8461538461538461, 'f1-score': 0.8614718614718614, 'support': 64} weighted_avg {'precision': 0.8967391304347826, 'recall': 0.875, 'f1-score': 0.8695887445887445, 'support': 64}

average train time 10.688299679756165

average val time 0.3572331190109253
 
time = 0.39 secondes

test_accuracy 0.9384615421295166 macro_avg {'precision': 0.9425, 'recall': 0.9312865497076024, 'f1-score': 0.9358974358974359, 'support': 65} weighted_avg {'precision': 0.9395384615384614, 'recall': 0.9384615384615385, 'f1-score': 0.9380670611439843, 'support': 65}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_tail_1
----------
Epoch 1/40
time = 11.07 secondes

Train loss 0.6331640346483751 accuracy 0.6259689927101135 macro_avg {'precision': 0.5592316513761468, 'recall': 0.5335809372104741, 'f1-score': 0.5124329881765439, 'support': 516} weighted_avg {'precision': 0.5858516019486523, 'recall': 0.625968992248062, 'f1-score': 0.5771804282494578, 'support': 516}
 
time = 0.41 secondes

Val loss 0.5212319567799568 accuracy 0.828125 macro_avg {'precision': 0.8473684210526315, 'recall': 0.8006072874493927, 'f1-score': 0.811512717536814, 'support': 64} weighted_avg {'precision': 0.8384868421052631, 'recall': 0.828125, 'f1-score': 0.8220046854082999, 'support': 64}
 
----------
Epoch 2/40
time = 9.77 secondes

Train loss 0.46200829563718854 accuracy 0.8062015771865845 macro_avg {'precision': 0.7910612682444134, 'recall': 0.7868601986249044, 'f1-score': 0.7888292299633315, 'support': 516} weighted_avg {'precision': 0.8050217881426736, 'recall': 0.8062015503875969, 'f1-score': 0.8054972671271536, 'support': 516}
 
time = 0.36 secondes

Val loss 0.4436412751674652 accuracy 0.8125 macro_avg {'precision': 0.88, 'recall': 0.7692307692307692, 'f1-score': 0.7818181818181819, 'support': 64} weighted_avg {'precision': 0.8574999999999999, 'recall': 0.8125, 'f1-score': 0.797159090909091, 'support': 64}
 
----------
Epoch 3/40
time = 10.31 secondes

Train loss 0.3470960146549976 accuracy 0.856589138507843 macro_avg {'precision': 0.8444250279112664, 'recall': 0.845992555629602, 'f1-score': 0.8451933056564608, 'support': 516} weighted_avg {'precision': 0.8569417304571277, 'recall': 0.8565891472868217, 'f1-score': 0.8567519450243982, 'support': 516}
 
time = 0.35 secondes

Val loss 0.44013281911611557 accuracy 0.859375 macro_avg {'precision': 0.8847953216374269, 'recall': 0.832995951417004, 'f1-score': 0.8457831325301204, 'support': 64} weighted_avg {'precision': 0.873062865497076, 'recall': 0.859375, 'f1-score': 0.8543674698795181, 'support': 64}
 
----------
Epoch 4/40
time = 10.25 secondes

Train loss 0.2334421037724524 accuracy 0.9205426573753357 macro_avg {'precision': 0.9129641588634162, 'recall': 0.9157632105066398, 'f1-score': 0.9143256322514022, 'support': 516} weighted_avg {'precision': 0.920876980223422, 'recall': 0.9205426356589147, 'f1-score': 0.9206768155885733, 'support': 516}
 
time = 0.35 secondes

Val loss 0.38521486334502697 accuracy 0.890625 macro_avg {'precision': 0.9222222222222223, 'recall': 0.8653846153846154, 'f1-score': 0.880053547523427, 'support': 64} weighted_avg {'precision': 0.9076388888888889, 'recall': 0.890625, 'f1-score': 0.8867302543507363, 'support': 64}
 
----------
Epoch 5/40
time = 10.30 secondes

Train loss 0.15545727261765438 accuracy 0.9573643207550049 macro_avg {'precision': 0.9570050300981281, 'recall': 0.9504087902085399, 'f1-score': 0.953542430591933, 'support': 516} weighted_avg {'precision': 0.9573363428265328, 'recall': 0.9573643410852714, 'f1-score': 0.9572093987679738, 'support': 516}
 
time = 0.35 secondes

Val loss 0.506194207817316 accuracy 0.875 macro_avg {'precision': 0.8831168831168831, 'recall': 0.8582995951417004, 'f1-score': 0.8666666666666667, 'support': 64} weighted_avg {'precision': 0.8782467532467533, 'recall': 0.875, 'f1-score': 0.8729166666666667, 'support': 64}
 
----------
Epoch 6/40
time = 11.13 secondes

Train loss 0.15576991033881452 accuracy 0.961240291595459 macro_avg {'precision': 0.9590593614762799, 'recall': 0.9569104237439656, 'f1-score': 0.9579667644183774, 'support': 516} weighted_avg {'precision': 0.9611805580610471, 'recall': 0.9612403100775194, 'f1-score': 0.9611948441655869, 'support': 516}
 
time = 0.35 secondes

Val loss 0.8835277408361435 accuracy 0.78125 macro_avg {'precision': 0.78125, 'recall': 0.791497975708502, 'f1-score': 0.7793103448275862, 'support': 64} weighted_avg {'precision': 0.798828125, 'recall': 0.78125, 'f1-score': 0.7831896551724138, 'support': 64}
 
----------
Epoch 7/40
time = 10.21 secondes

Train loss 0.09492219314261367 accuracy 0.9786821603775024 macro_avg {'precision': 0.9774682306940372, 'recall': 0.976358435056808, 'f1-score': 0.9769087129333176, 'support': 516} weighted_avg {'precision': 0.9786653102669607, 'recall': 0.9786821705426356, 'f1-score': 0.978669768741172, 'support': 516}
 
time = 0.35 secondes

Val loss 0.7648078128695488 accuracy 0.859375 macro_avg {'precision': 0.9042553191489362, 'recall': 0.8269230769230769, 'f1-score': 0.8424076607387141, 'support': 64} weighted_avg {'precision': 0.8863031914893617, 'recall': 0.859375, 'f1-score': 0.8521032831737346, 'support': 64}
 
----------
Epoch 8/40
time = 10.27 secondes

Train loss 0.1425645431418988 accuracy 0.9670542478561401 macro_avg {'precision': 0.9648582600195503, 'recall': 0.9637777741657592, 'f1-score': 0.9643134654423999, 'support': 516} weighted_avg {'precision': 0.9670237635166368, 'recall': 0.9670542635658915, 'f1-score': 0.9670350971454476, 'support': 516}
 
time = 0.35 secondes

Val loss 1.5560875535011292 accuracy 0.71875 macro_avg {'precision': 0.8392857142857143, 'recall': 0.6538461538461539, 'f1-score': 0.6395494367959951, 'support': 64} weighted_avg {'precision': 0.8091517857142858, 'recall': 0.71875, 'f1-score': 0.671229662077597, 'support': 64}
 
----------
Epoch 9/40
time = 10.28 secondes

Train loss 0.2720543022450963 accuracy 0.9437984228134155 macro_avg {'precision': 0.9539218403547671, 'recall': 0.9259220129057426, 'f1-score': 0.9373972413619991, 'support': 516} weighted_avg {'precision': 0.9462754494748964, 'recall': 0.9437984496124031, 'f1-score': 0.9429061599774984, 'support': 516}
 
time = 0.35 secondes

Val loss 0.706640899181366 accuracy 0.8125 macro_avg {'precision': 0.8055555555555556, 'recall': 0.811740890688259, 'f1-score': 0.8078078078078078, 'support': 64} weighted_avg {'precision': 0.8159722222222222, 'recall': 0.8125, 'f1-score': 0.8134384384384383, 'support': 64}
 
----------
Epoch 10/40
time = 10.21 secondes

Train loss 0.16539354994187527 accuracy 0.961240291595459 macro_avg {'precision': 0.9547567697100408, 'recall': 0.9626806235066561, 'f1-score': 0.9584373741441804, 'support': 516} weighted_avg {'precision': 0.9620636167908501, 'recall': 0.9612403100775194, 'f1-score': 0.961407649536226, 'support': 516}
 
time = 0.36 secondes

Val loss 0.7619165778160095 accuracy 0.828125 macro_avg {'precision': 0.8877551020408163, 'recall': 0.7884615384615384, 'f1-score': 0.8026352677319877, 'support': 64} weighted_avg {'precision': 0.8667091836734694, 'recall': 0.828125, 'f1-score': 0.8159342584805158, 'support': 64}
 
----------
Epoch 11/40
time = 10.24 secondes

Train loss 0.23868048205181505 accuracy 0.9399224519729614 macro_avg {'precision': 0.9419741883444243, 'recall': 0.9274986590380834, 'f1-score': 0.9339638609426538, 'support': 516} weighted_avg {'precision': 0.9402400068155776, 'recall': 0.939922480620155, 'f1-score': 0.9394227254213968, 'support': 516}
 
time = 0.35 secondes

Val loss 1.065360739827156 accuracy 0.8125 macro_avg {'precision': 0.88, 'recall': 0.7692307692307692, 'f1-score': 0.7818181818181819, 'support': 64} weighted_avg {'precision': 0.8574999999999999, 'recall': 0.8125, 'f1-score': 0.797159090909091, 'support': 64}
 
----------
Epoch 12/40
time = 10.19 secondes

Train loss 0.16896550675543645 accuracy 0.961240291595459 macro_avg {'precision': 0.9624687101105714, 'recall': 0.9534483038863515, 'f1-score': 0.9576625806134003, 'support': 516} weighted_avg {'precision': 0.9613647050175752, 'recall': 0.9612403100775194, 'f1-score': 0.961049497839433, 'support': 516}
 
time = 0.35 secondes

Val loss 0.678168497979641 accuracy 0.84375 macro_avg {'precision': 0.8958333333333333, 'recall': 0.8076923076923077, 'f1-score': 0.8228128460686601, 'support': 64} weighted_avg {'precision': 0.8763020833333333, 'recall': 0.84375, 'f1-score': 0.834233111849391, 'support': 64}
 
----------
Epoch 13/40
time = 10.23 secondes

Train loss 0.23978783486227534 accuracy 0.9515503644943237 macro_avg {'precision': 0.9471328489880644, 'recall': 0.9481575995968987, 'f1-score': 0.947640791476408, 'support': 516} weighted_avg {'precision': 0.9516134952913111, 'recall': 0.9515503875968992, 'f1-score': 0.9515781152289595, 'support': 516}
 
time = 0.36 secondes

Val loss 0.8192405924201012 accuracy 0.859375 macro_avg {'precision': 0.8558974358974358, 'recall': 0.8512145748987854, 'f1-score': 0.8533231474407945, 'support': 64} weighted_avg {'precision': 0.8588782051282051, 'recall': 0.859375, 'f1-score': 0.858909472880061, 'support': 64}
 
----------
Epoch 14/40
time = 10.20 secondes

Train loss 0.29133964876051655 accuracy 0.9244186282157898 macro_avg {'precision': 0.9385683760683761, 'recall': 0.9003380849438422, 'f1-score': 0.9148468012541944, 'support': 516} weighted_avg {'precision': 0.9287190253760021, 'recall': 0.9244186046511628, 'f1-score': 0.9227034260077754, 'support': 516}
 
time = 0.35 secondes

Val loss 0.7275266721844673 accuracy 0.84375 macro_avg {'precision': 0.8380566801619433, 'recall': 0.8380566801619433, 'f1-score': 0.8380566801619433, 'support': 64} weighted_avg {'precision': 0.84375, 'recall': 0.84375, 'f1-score': 0.84375, 'support': 64}
 
----------
Epoch 15/40
time = 11.13 secondes

Train loss 0.08204524827570739 accuracy 0.9825581312179565 macro_avg {'precision': 0.9816715542521994, 'recall': 0.9805519886871576, 'f1-score': 0.9811071287636235, 'support': 516} weighted_avg {'precision': 0.982545825850402, 'recall': 0.9825581395348837, 'f1-score': 0.9825479926064133, 'support': 516}
 
time = 0.35 secondes

Val loss 1.2354170083999634 accuracy 0.796875 macro_avg {'precision': 0.7942326490713587, 'recall': 0.8046558704453441, 'f1-score': 0.7944156165060539, 'support': 64} weighted_avg {'precision': 0.8100867546432062, 'recall': 0.796875, 'f1-score': 0.7986317024956758, 'support': 64}
 
----------
Epoch 16/40
time = 12.15 secondes

Train loss 0.07157075121843566 accuracy 0.9806201457977295 macro_avg {'precision': 0.9780107761759138, 'recall': 0.98018627180079, 'f1-score': 0.9790801764400623, 'support': 516} weighted_avg {'precision': 0.9806957892086973, 'recall': 0.9806201550387597, 'f1-score': 0.9806421547330268, 'support': 516}
 
time = 0.37 secondes

Val loss 1.657694786787033 accuracy 0.734375 macro_avg {'precision': 0.7375366568914956, 'recall': 0.7459514170040487, 'f1-score': 0.7327437975927291, 'support': 64} weighted_avg {'precision': 0.7565065982404693, 'recall': 0.734375, 'f1-score': 0.7366586833701794, 'support': 64}
 
----------
Epoch 17/40
time = 11.26 secondes

Train loss 0.19614019250553666 accuracy 0.9515503644943237 macro_avg {'precision': 0.9490243583027763, 'recall': 0.9458495196918226, 'f1-score': 0.9473965363269734, 'support': 516} weighted_avg {'precision': 0.9514479810038943, 'recall': 0.9515503875968992, 'f1-score': 0.9514644458464872, 'support': 516}
 
time = 0.35 secondes

Val loss 0.9684134051203728 accuracy 0.84375 macro_avg {'precision': 0.8380566801619433, 'recall': 0.8380566801619433, 'f1-score': 0.8380566801619433, 'support': 64} weighted_avg {'precision': 0.84375, 'recall': 0.84375, 'f1-score': 0.84375, 'support': 64}
 
----------
Epoch 18/40
time = 10.16 secondes

Train loss 0.17578042218828518 accuracy 0.9496123790740967 macro_avg {'precision': 0.9484950935928094, 'recall': 0.9420216829478407, 'f1-score': 0.9450955997904662, 'support': 516} weighted_avg {'precision': 0.9495253400222323, 'recall': 0.9496124031007752, 'f1-score': 0.9494292894530599, 'support': 516}
 
time = 0.35 secondes

Val loss 1.3420154005289078 accuracy 0.765625 macro_avg {'precision': 0.8584905660377358, 'recall': 0.7115384615384616, 'f1-score': 0.7148797148797148, 'support': 64} weighted_avg {'precision': 0.8319575471698113, 'recall': 0.765625, 'f1-score': 0.737433174933175, 'support': 64}
 
----------
Epoch 19/40
time = 9.94 secondes

Train loss 0.04375558732444364 accuracy 0.9883720874786377 macro_avg {'precision': 0.988552298522087, 'recall': 0.9862652991564131, 'f1-score': 0.9873900293255132, 'support': 516} weighted_avg {'precision': 0.9883770301602101, 'recall': 0.9883720930232558, 'f1-score': 0.9883584532496761, 'support': 516}
 
time = 0.37 secondes

Val loss 1.278950497508049 accuracy 0.796875 macro_avg {'precision': 0.8725490196078431, 'recall': 0.75, 'f1-score': 0.7602996254681648, 'support': 64} weighted_avg {'precision': 0.8486519607843137, 'recall': 0.796875, 'f1-score': 0.7778558052434457, 'support': 64}
 
----------
Epoch 20/40
time = 10.21 secondes

Train loss 0.04225691087159441 accuracy 0.9922480583190918 macro_avg {'precision': 0.9905344400757244, 'recall': 0.9927669326918388, 'f1-score': 0.9916320705760249, 'support': 516} weighted_avg {'precision': 0.9922977322166568, 'recall': 0.9922480620155039, 'f1-score': 0.9922568618932107, 'support': 516}
 
time = 0.35 secondes

Val loss 0.9926723428070545 accuracy 0.859375 macro_avg {'precision': 0.8709856035437431, 'recall': 0.8390688259109311, 'f1-score': 0.8486997635933806, 'support': 64} weighted_avg {'precision': 0.8646525470653379, 'recall': 0.859375, 'f1-score': 0.8562352245862884, 'support': 64}
 
----------
Epoch 21/40
time = 10.25 secondes

Train loss 0.16646827836556247 accuracy 0.963178277015686 macro_avg {'precision': 0.9558319039451115, 'recall': 0.966508460250638, 'f1-score': 0.9606415223107649, 'support': 516} weighted_avg {'precision': 0.9645251328555409, 'recall': 0.9631782945736435, 'f1-score': 0.9633913059850301, 'support': 516}
 
time = 0.36 secondes

Val loss 1.7010359466075897 accuracy 0.75 macro_avg {'precision': 0.7450980392156863, 'recall': 0.7530364372469636, 'f1-score': 0.746031746031746, 'support': 64} weighted_avg {'precision': 0.7598039215686274, 'recall': 0.75, 'f1-score': 0.751984126984127, 'support': 64}
 
----------
Epoch 22/40
time = 10.25 secondes

Train loss 0.1792215887407744 accuracy 0.9651162624359131 macro_avg {'precision': 0.9740634005763689, 'recall': 0.9518716577540107, 'f1-score': 0.9614054916561399, 'support': 516} weighted_avg {'precision': 0.9669258092621138, 'recall': 0.9651162790697675, 'f1-score': 0.9646988154857342, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0367650240659714 accuracy 0.8125 macro_avg {'precision': 0.8056680161943319, 'recall': 0.8056680161943319, 'f1-score': 0.8056680161943319, 'support': 64} weighted_avg {'precision': 0.8125, 'recall': 0.8125, 'f1-score': 0.8125, 'support': 64}
 
----------
Epoch 23/40
time = 10.13 secondes

Train loss 0.10416818605727078 accuracy 0.9767441749572754 macro_avg {'precision': 0.9759124683595983, 'recall': 0.9736846382653641, 'f1-score': 0.9747800586510263, 'support': 516} weighted_avg {'precision': 0.9767213992605689, 'recall': 0.9767441860465116, 'f1-score': 0.9767169064993521, 'support': 516}
 
time = 0.36 secondes

Val loss 1.2534790337085724 accuracy 0.84375 macro_avg {'precision': 0.8958333333333333, 'recall': 0.8076923076923077, 'f1-score': 0.8228128460686601, 'support': 64} weighted_avg {'precision': 0.8763020833333333, 'recall': 0.84375, 'f1-score': 0.834233111849391, 'support': 64}
 
----------
Epoch 24/40
time = 9.64 secondes

Train loss 0.03913549366795147 accuracy 0.9903100728988647 macro_avg {'precision': 0.9869791666666667, 'recall': 0.9924012158054711, 'f1-score': 0.9895752100110309, 'support': 516} weighted_avg {'precision': 0.990562419250646, 'recall': 0.9903100775193798, 'f1-score': 0.9903368975014364, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0336718335747719 accuracy 0.84375 macro_avg {'precision': 0.8484848484848485, 'recall': 0.8259109311740891, 'f1-score': 0.8333333333333333, 'support': 64} weighted_avg {'precision': 0.8456439393939394, 'recall': 0.84375, 'f1-score': 0.8411458333333333, 'support': 64}
 
----------
Epoch 25/40
time = 9.82 secondes

Train loss 0.018631240235690988 accuracy 0.998062014579773 macro_avg {'precision': 0.9973404255319149, 'recall': 0.9984802431610942, 'f1-score': 0.9979056316590563, 'support': 516} weighted_avg {'precision': 0.9980723239320469, 'recall': 0.998062015503876, 'f1-score': 0.9980631246091585, 'support': 516}
 
time = 0.35 secondes

Val loss 1.3545930683612823 accuracy 0.828125 macro_avg {'precision': 0.8355481727574751, 'recall': 0.8066801619433198, 'f1-score': 0.8150774888363541, 'support': 64} weighted_avg {'precision': 0.831499169435216, 'recall': 0.828125, 'f1-score': 0.8242874967165746, 'support': 64}
 
----------
Epoch 26/40
time = 10.26 secondes

Train loss 0.07160963764931005 accuracy 0.9883720874786377 macro_avg {'precision': 0.988552298522087, 'recall': 0.9862652991564131, 'f1-score': 0.9873900293255132, 'support': 516} weighted_avg {'precision': 0.9883770301602101, 'recall': 0.9883720930232558, 'f1-score': 0.9883584532496761, 'support': 516}
 
time = 0.35 secondes

Val loss 1.6110130548477173 accuracy 0.78125 macro_avg {'precision': 0.776470588235294, 'recall': 0.785425101214575, 'f1-score': 0.7777777777777777, 'support': 64} weighted_avg {'precision': 0.7908088235294117, 'recall': 0.78125, 'f1-score': 0.782986111111111, 'support': 64}
 
----------
Epoch 27/40
time = 10.68 secondes

Train loss 0.05164639652392714 accuracy 0.9883720874786377 macro_avg {'precision': 0.988552298522087, 'recall': 0.9862652991564131, 'f1-score': 0.9873900293255132, 'support': 516} weighted_avg {'precision': 0.9883770301602101, 'recall': 0.9883720930232558, 'f1-score': 0.9883584532496761, 'support': 516}
 
time = 0.35 secondes

Val loss 1.1040720418095589 accuracy 0.859375 macro_avg {'precision': 0.8616118769883351, 'recall': 0.8451417004048583, 'f1-score': 0.8512012399896668, 'support': 64} weighted_avg {'precision': 0.8601206256627784, 'recall': 0.859375, 'f1-score': 0.8577402479979334, 'support': 64}
 
----------
Epoch 28/40
time = 10.28 secondes

Train loss 0.024736434463799622 accuracy 0.9961240291595459 macro_avg {'precision': 0.9947089947089947, 'recall': 0.9969604863221885, 'f1-score': 0.9958160352880125, 'support': 516} weighted_avg {'precision': 0.9961650465526435, 'recall': 0.9961240310077519, 'f1-score': 0.9961284309466054, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0591801404953003 accuracy 0.859375 macro_avg {'precision': 0.8709856035437431, 'recall': 0.8390688259109311, 'f1-score': 0.8486997635933806, 'support': 64} weighted_avg {'precision': 0.8646525470653379, 'recall': 0.859375, 'f1-score': 0.8562352245862884, 'support': 64}
 
----------
Epoch 29/40
time = 10.23 secondes

Train loss 0.1688951042826485 accuracy 0.9728682041168213 macro_avg {'precision': 0.9795918367346939, 'recall': 0.9625668449197862, 'f1-score': 0.9701388888888889, 'support': 516} weighted_avg {'precision': 0.9739756367663344, 'recall': 0.9728682170542635, 'f1-score': 0.9726232773471145, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0716452151536942 accuracy 0.859375 macro_avg {'precision': 0.8709856035437431, 'recall': 0.8390688259109311, 'f1-score': 0.8486997635933806, 'support': 64} weighted_avg {'precision': 0.8646525470653379, 'recall': 0.859375, 'f1-score': 0.8562352245862884, 'support': 64}
 
----------
Epoch 30/40
time = 10.24 secondes

Train loss 0.004497200151408007 accuracy 0.998062014579773 macro_avg {'precision': 0.9973404255319149, 'recall': 0.9984802431610942, 'f1-score': 0.9979056316590563, 'support': 516} weighted_avg {'precision': 0.9980723239320469, 'recall': 0.998062015503876, 'f1-score': 0.9980631246091585, 'support': 516}
 
time = 0.35 secondes

Val loss 1.2894841432571411 accuracy 0.828125 macro_avg {'precision': 0.8877551020408163, 'recall': 0.7884615384615384, 'f1-score': 0.8026352677319877, 'support': 64} weighted_avg {'precision': 0.8667091836734694, 'recall': 0.828125, 'f1-score': 0.8159342584805158, 'support': 64}
 
----------
Epoch 31/40
time = 10.18 secondes

Train loss 0.04712742646921525 accuracy 0.9903100728988647 macro_avg {'precision': 0.9925149700598803, 'recall': 0.9866310160427807, 'f1-score': 0.9894541931844658, 'support': 516} weighted_avg {'precision': 0.9904551362391496, 'recall': 0.9903100775193798, 'f1-score': 0.990280965807308, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0260947495698929 accuracy 0.859375 macro_avg {'precision': 0.8558974358974358, 'recall': 0.8512145748987854, 'f1-score': 0.8533231474407945, 'support': 64} weighted_avg {'precision': 0.8588782051282051, 'recall': 0.859375, 'f1-score': 0.858909472880061, 'support': 64}
 
----------
Epoch 32/40
time = 10.23 secondes

Train loss 0.0007942933130614234 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 1.4118098318576813 accuracy 0.828125 macro_avg {'precision': 0.8877551020408163, 'recall': 0.7884615384615384, 'f1-score': 0.8026352677319877, 'support': 64} weighted_avg {'precision': 0.8667091836734694, 'recall': 0.828125, 'f1-score': 0.8159342584805158, 'support': 64}
 
----------
Epoch 33/40
time = 10.19 secondes

Train loss 0.1170651621897846 accuracy 0.9748061895370483 macro_avg {'precision': 0.9809941520467836, 'recall': 0.96524064171123, 'f1-score': 0.9723074255565969, 'support': 516} weighted_avg {'precision': 0.9757638605557822, 'recall': 0.9748062015503876, 'f1-score': 0.9745966267896181, 'support': 516}
 
time = 0.35 secondes

Val loss 1.0850688070058823 accuracy 0.875 macro_avg {'precision': 0.9130434782608696, 'recall': 0.8461538461538461, 'f1-score': 0.8614718614718614, 'support': 64} weighted_avg {'precision': 0.8967391304347826, 'recall': 0.875, 'f1-score': 0.8695887445887445, 'support': 64}
 
----------
Epoch 34/40
time = 10.75 secondes

Train loss 0.032973263766369644 accuracy 0.9941860437393188 macro_avg {'precision': 0.9921052631578947, 'recall': 0.9954407294832827, 'f1-score': 0.9937311438232733, 'support': 516} weighted_avg {'precision': 0.9942778457772338, 'recall': 0.9941860465116279, 'f1-score': 0.9941958645552614, 'support': 516}
 
time = 0.36 secondes

Val loss 1.736567810177803 accuracy 0.796875 macro_avg {'precision': 0.7942326490713587, 'recall': 0.8046558704453441, 'f1-score': 0.7944156165060539, 'support': 64} weighted_avg {'precision': 0.8100867546432062, 'recall': 0.796875, 'f1-score': 0.7986317024956758, 'support': 64}
 
----------
Epoch 35/40
time = 9.66 secondes

Train loss 0.0001258901620255501 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 1.3809583485126495 accuracy 0.84375 macro_avg {'precision': 0.8958333333333333, 'recall': 0.8076923076923077, 'f1-score': 0.8228128460686601, 'support': 64} weighted_avg {'precision': 0.8763020833333333, 'recall': 0.84375, 'f1-score': 0.834233111849391, 'support': 64}
 
----------
Epoch 36/40
time = 9.66 secondes

Train loss 0.0014388959420047645 accuracy 0.998062014579773 macro_avg {'precision': 0.9984848484848485, 'recall': 0.9973262032085561, 'f1-score': 0.997900792084847, 'support': 516} weighted_avg {'precision': 0.9980678881841673, 'recall': 0.998062015503876, 'f1-score': 0.9980608880673791, 'support': 516}
 
time = 0.35 secondes

Val loss 1.136267527937889 accuracy 0.875 macro_avg {'precision': 0.9130434782608696, 'recall': 0.8461538461538461, 'f1-score': 0.8614718614718614, 'support': 64} weighted_avg {'precision': 0.8967391304347826, 'recall': 0.875, 'f1-score': 0.8695887445887445, 'support': 64}
 
----------
Epoch 37/40
time = 9.66 secondes

Train loss 8.561122751526648e-05 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 1.2048007100820541 accuracy 0.875 macro_avg {'precision': 0.875, 'recall': 0.8643724696356275, 'f1-score': 0.8687179487179488, 'support': 64} weighted_avg {'precision': 0.875, 'recall': 0.875, 'f1-score': 0.8741025641025642, 'support': 64}
 
----------
Epoch 38/40
time = 9.67 secondes

Train loss 8.713858837974399e-05 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 1.3309474885463715 accuracy 0.859375 macro_avg {'precision': 0.8558974358974358, 'recall': 0.8512145748987854, 'f1-score': 0.8533231474407945, 'support': 64} weighted_avg {'precision': 0.8588782051282051, 'recall': 0.859375, 'f1-score': 0.858909472880061, 'support': 64}
 
----------
Epoch 39/40
time = 9.64 secondes

Train loss 0.012139470742554156 accuracy 0.998062014579773 macro_avg {'precision': 0.9973404255319149, 'recall': 0.9984802431610942, 'f1-score': 0.9979056316590563, 'support': 516} weighted_avg {'precision': 0.9980723239320469, 'recall': 0.998062015503876, 'f1-score': 0.9980631246091585, 'support': 516}
 
time = 0.35 secondes

Val loss 1.23781917989254 accuracy 0.875 macro_avg {'precision': 0.875, 'recall': 0.8643724696356275, 'f1-score': 0.8687179487179488, 'support': 64} weighted_avg {'precision': 0.875, 'recall': 0.875, 'f1-score': 0.8741025641025642, 'support': 64}
 
----------
Epoch 40/40
time = 9.73 secondes

Train loss 5.9675244825264215e-05 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.35 secondes

Val loss 1.2316387295722961 accuracy 0.875 macro_avg {'precision': 0.875, 'recall': 0.8643724696356275, 'f1-score': 0.8687179487179488, 'support': 64} weighted_avg {'precision': 0.875, 'recall': 0.875, 'f1-score': 0.8741025641025642, 'support': 64}
 
----------
best_accuracy 0.890625 best_epoch 4 macro_avg {'precision': 0.9222222222222223, 'recall': 0.8653846153846154, 'f1-score': 0.880053547523427, 'support': 64} weighted_avg {'precision': 0.9076388888888889, 'recall': 0.890625, 'f1-score': 0.8867302543507363, 'support': 64}

average train time 10.265740948915482

average val time 0.3560186386108398
 
time = 0.39 secondes

test_accuracy 0.892307698726654 macro_avg {'precision': 0.9075052854122622, 'recall': 0.8757309941520468, 'f1-score': 0.8853615520282188, 'support': 65} weighted_avg {'precision': 0.8995446414051065, 'recall': 0.8923076923076924, 'f1-score': 0.8901370234703568, 'support': 65}

----------
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (11). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
20newsgroups_BERT_head_bert_summarizer_2
----------
Epoch 1/40
time = 206.93 secondes

Train loss 1.308757720646716 accuracy 0.6602161526679993 macro_avg {'precision': 0.6900733923832763, 'recall': 0.6443857650775403, 'f1-score': 0.6413905953065024, 'support': 10180} weighted_avg {'precision': 0.6938982598509799, 'recall': 0.6602161100196464, 'f1-score': 0.6555079777443953, 'support': 10180}
 
time = 5.32 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss 0.6809298354135432 accuracy 0.8045977354049683 macro_avg {'precision': 0.7757731811313818, 'recall': 0.7976091556708154, 'f1-score': 0.7787436209680857, 'support': 1131} weighted_avg {'precision': 0.7881948023989755, 'recall': 0.8045977011494253, 'f1-score': 0.7885284137436782, 'support': 1131}
 
----------
Epoch 2/40
time = 202.71 secondes

Train loss 0.47511351341308566 accuracy 0.8594303131103516 macro_avg {'precision': 0.8493989979153961, 'recall': 0.84856098768767, 'f1-score': 0.8473432693731191, 'support': 10180} weighted_avg {'precision': 0.8569342142116004, 'recall': 0.8594302554027505, 'f1-score': 0.8569287740760345, 'support': 10180}
 
time = 5.13 secondes

Val loss 0.5468336605060269 accuracy 0.8532272577285767 macro_avg {'precision': 0.8482106516395904, 'recall': 0.8481068102857385, 'f1-score': 0.8446921304769897, 'support': 1131} weighted_avg {'precision': 0.8581848172465826, 'recall': 0.8532272325375774, 'f1-score': 0.8519207078951305, 'support': 1131}
 
----------
Epoch 3/40
time = 199.13 secondes

Train loss 0.2997342594526416 accuracy 0.9142436385154724 macro_avg {'precision': 0.9082525394900378, 'recall': 0.9074426461784428, 'f1-score': 0.9076199295777636, 'support': 10180} weighted_avg {'precision': 0.9138351122404561, 'recall': 0.9142436149312377, 'f1-score': 0.9138412491972218, 'support': 10180}
 
time = 5.12 secondes

Val loss 0.5272193888700764 accuracy 0.8717948794364929 macro_avg {'precision': 0.8742648964027188, 'recall': 0.8698801514918115, 'f1-score': 0.8685455617424145, 'support': 1131} weighted_avg {'precision': 0.8792721902076693, 'recall': 0.8717948717948718, 'f1-score': 0.8721081065637503, 'support': 1131}
 
----------
Epoch 4/40
time = 202.31 secondes

Train loss 0.22187700125811333 accuracy 0.940078616142273 macro_avg {'precision': 0.9371632348377947, 'recall': 0.9368093974697475, 'f1-score': 0.936927399852863, 'support': 10180} weighted_avg {'precision': 0.9403825500835715, 'recall': 0.9400785854616895, 'f1-score': 0.9401731107701389, 'support': 10180}
 
time = 5.08 secondes

Val loss 0.5671631915455448 accuracy 0.8859416842460632 macro_avg {'precision': 0.8935508122493039, 'recall': 0.8889755167947646, 'f1-score': 0.8862364678718169, 'support': 1131} weighted_avg {'precision': 0.8962442071586182, 'recall': 0.8859416445623343, 'f1-score': 0.885606848249197, 'support': 1131}
 
----------
Epoch 5/40
time = 204.07 secondes

Train loss 0.19383388183304814 accuracy 0.9515717625617981 macro_avg {'precision': 0.9489343640531356, 'recall': 0.9490236136713589, 'f1-score': 0.9488497088121903, 'support': 10180} weighted_avg {'precision': 0.9517829229573317, 'recall': 0.9515717092337918, 'f1-score': 0.9515557849625733, 'support': 10180}
 
time = 5.10 secondes

Val loss 0.5836577023064692 accuracy 0.895667552947998 macro_avg {'precision': 0.8951558888713974, 'recall': 0.8940957125719512, 'f1-score': 0.8929903092704162, 'support': 1131} weighted_avg {'precision': 0.8977686158062468, 'recall': 0.8956675508399646, 'f1-score': 0.8950573917712835, 'support': 1131}
 
----------
Epoch 6/40
time = 200.31 secondes

Train loss 0.17925363197612754 accuracy 0.9589391350746155 macro_avg {'precision': 0.9563597662378778, 'recall': 0.9567866318959473, 'f1-score': 0.9564845942493617, 'support': 10180} weighted_avg {'precision': 0.9593372224344896, 'recall': 0.9589390962671905, 'f1-score': 0.9590623382776232, 'support': 10180}
 
time = 5.13 secondes

Val loss 0.5818407031031929 accuracy 0.8947833776473999 macro_avg {'precision': 0.8973371392621388, 'recall': 0.8942202669565713, 'f1-score': 0.893939749418909, 'support': 1131} weighted_avg {'precision': 0.8979390760399885, 'recall': 0.8947833775419982, 'f1-score': 0.8946261941059803, 'support': 1131}
 
----------
Epoch 7/40
time = 202.73 secondes

Train loss 0.1607166352808648 accuracy 0.9645383358001709 macro_avg {'precision': 0.9633930698833291, 'recall': 0.963268996150712, 'f1-score': 0.9632898945863427, 'support': 10180} weighted_avg {'precision': 0.9645134204639596, 'recall': 0.9645383104125737, 'f1-score': 0.9644834550133986, 'support': 10180}
 
time = 5.12 secondes

Val loss 0.6386864285761963 accuracy 0.890362560749054 macro_avg {'precision': 0.8920956715708254, 'recall': 0.8917203856493214, 'f1-score': 0.8886157152930961, 'support': 1131} weighted_avg {'precision': 0.8943965103293572, 'recall': 0.8903625110521662, 'f1-score': 0.889071462518167, 'support': 1131}
 
----------
Epoch 8/40
time = 203.68 secondes

Train loss 0.14054635946528862 accuracy 0.9706287384033203 macro_avg {'precision': 0.9701669774827864, 'recall': 0.9696480243336485, 'f1-score': 0.9698748131915028, 'support': 10180} weighted_avg {'precision': 0.9706961020975616, 'recall': 0.9706286836935167, 'f1-score': 0.9706304188418443, 'support': 10180}
 
time = 5.13 secondes

Val loss 0.6787606224124189 accuracy 0.8894783854484558 macro_avg {'precision': 0.8956919819398778, 'recall': 0.8925466096581065, 'f1-score': 0.8901601462424799, 'support': 1131} weighted_avg {'precision': 0.8955692947860074, 'recall': 0.8894783377541998, 'f1-score': 0.8880028520128452, 'support': 1131}
 
----------
Epoch 9/40
time = 200.64 secondes

Train loss 0.13484178011542663 accuracy 0.9714145660400391 macro_avg {'precision': 0.9710665044191668, 'recall': 0.9708663929240082, 'f1-score': 0.970930906983462, 'support': 10180} weighted_avg {'precision': 0.9714459573627829, 'recall': 0.9714145383104126, 'f1-score': 0.9713943263260425, 'support': 10180}
 
time = 5.14 secondes

Val loss 0.5933434898501039 accuracy 0.9027409553527832 macro_avg {'precision': 0.9049743060091867, 'recall': 0.9018556188925346, 'f1-score': 0.9010176442001239, 'support': 1131} weighted_avg {'precision': 0.9065116440364566, 'recall': 0.9027409372236959, 'f1-score': 0.9022251496353115, 'support': 1131}
 
----------
Epoch 10/40
time = 201.83 secondes

Train loss 0.13652551357371745 accuracy 0.9729862809181213 macro_avg {'precision': 0.97284974354108, 'recall': 0.9725299592346234, 'f1-score': 0.9726271493155183, 'support': 10180} weighted_avg {'precision': 0.9730473741160471, 'recall': 0.9729862475442044, 'f1-score': 0.9729528147334494, 'support': 10180}
 
time = 5.10 secondes

Val loss 0.7564684541233566 accuracy 0.8824049830436707 macro_avg {'precision': 0.8905596436269084, 'recall': 0.8807734299238182, 'f1-score': 0.8800500244125871, 'support': 1131} weighted_avg {'precision': 0.8917415090266833, 'recall': 0.8824049513704686, 'f1-score': 0.8818272171257461, 'support': 1131}
 
----------
Epoch 11/40
time = 203.60 secondes

Train loss 0.12942551905350516 accuracy 0.9735756516456604 macro_avg {'precision': 0.9730179156250754, 'recall': 0.9732208927909592, 'f1-score': 0.9730647263815421, 'support': 10180} weighted_avg {'precision': 0.9736386401917255, 'recall': 0.9735756385068762, 'f1-score': 0.9735549165038373, 'support': 10180}
 
time = 5.13 secondes

Val loss 0.6997910261311582 accuracy 0.895667552947998 macro_avg {'precision': 0.9024806647521496, 'recall': 0.8963117527695805, 'f1-score': 0.8969628667114848, 'support': 1131} weighted_avg {'precision': 0.9036312695603718, 'recall': 0.8956675508399646, 'f1-score': 0.8971344159703596, 'support': 1131}
 
----------
Epoch 12/40
time = 200.78 secondes

Train loss 0.09466773921334524 accuracy 0.97956782579422 macro_avg {'precision': 0.9788564096802315, 'recall': 0.9789249887028543, 'f1-score': 0.9788724542302353, 'support': 10180} weighted_avg {'precision': 0.979632325871314, 'recall': 0.9795677799607073, 'f1-score': 0.9795818896140153, 'support': 10180}
 
time = 5.13 secondes

Val loss 0.7460697964698726 accuracy 0.8938992023468018 macro_avg {'precision': 0.8965286965342367, 'recall': 0.8953578911228217, 'f1-score': 0.8940261301959488, 'support': 1131} weighted_avg {'precision': 0.8982365910074482, 'recall': 0.8938992042440318, 'f1-score': 0.8940082148897289, 'support': 1131}
 
----------
Epoch 13/40
time = 203.60 secondes

Train loss 0.09491010419802347 accuracy 0.9809430837631226 macro_avg {'precision': 0.9805558648986603, 'recall': 0.9805985869403978, 'f1-score': 0.9805516024622529, 'support': 10180} weighted_avg {'precision': 0.9809473171732931, 'recall': 0.980943025540275, 'f1-score': 0.9809210674478155, 'support': 10180}
 
time = 5.14 secondes

Val loss 0.7306513062106039 accuracy 0.8930150270462036 macro_avg {'precision': 0.8957513226365263, 'recall': 0.8938582873555122, 'f1-score': 0.8933105462580414, 'support': 1131} weighted_avg {'precision': 0.8969364736172327, 'recall': 0.8930150309460654, 'f1-score': 0.8935222800486224, 'support': 1131}
 
----------
Epoch 14/40
time = 203.53 secondes

Train loss 0.11090248774175969 accuracy 0.9779961109161377 macro_avg {'precision': 0.9775910470954731, 'recall': 0.9770339611705209, 'f1-score': 0.9772789263362178, 'support': 10180} weighted_avg {'precision': 0.9780523427172524, 'recall': 0.9779960707269155, 'f1-score': 0.9779922607595543, 'support': 10180}
 
time = 5.11 secondes

Val loss 0.7173022210161845 accuracy 0.8912467360496521 macro_avg {'precision': 0.9010792760345098, 'recall': 0.8924888221649615, 'f1-score': 0.8947515566571134, 'support': 1131} weighted_avg {'precision': 0.8983688508773497, 'recall': 0.8912466843501327, 'f1-score': 0.8927068806711709, 'support': 1131}
 
----------
Epoch 15/40
time = 204.98 secondes

Train loss 0.09936943748035908 accuracy 0.9805501103401184 macro_avg {'precision': 0.9794936128862568, 'recall': 0.9799260600267263, 'f1-score': 0.9796313607221423, 'support': 10180} weighted_avg {'precision': 0.9807745944952185, 'recall': 0.9805500982318271, 'f1-score': 0.9805902986572027, 'support': 10180}
 
time = 5.13 secondes

Val loss 0.8888020043074428 accuracy 0.8841733336448669 macro_avg {'precision': 0.8863574624923307, 'recall': 0.8866668928269583, 'f1-score': 0.883596934433772, 'support': 1131} weighted_avg {'precision': 0.8905672371923945, 'recall': 0.8841732979664014, 'f1-score': 0.8844099296293548, 'support': 1131}
 
----------
Epoch 16/40
time = 199.02 secondes

Train loss 0.08826269166638381 accuracy 0.9820235967636108 macro_avg {'precision': 0.9815191338096978, 'recall': 0.9810983538999729, 'f1-score': 0.9812845006046453, 'support': 10180} weighted_avg {'precision': 0.9820607497258814, 'recall': 0.9820235756385068, 'f1-score': 0.9820183317520953, 'support': 10180}
 
time = 5.15 secondes

Val loss 0.9671365349763811 accuracy 0.8815208077430725 macro_avg {'precision': 0.8886722931900982, 'recall': 0.883914244028783, 'f1-score': 0.8816007239530629, 'support': 1131} weighted_avg {'precision': 0.8907539883144935, 'recall': 0.8815207780725022, 'f1-score': 0.8808810845285862, 'support': 1131}
 
----------
Epoch 17/40
time = 203.39 secondes

Train loss 0.09877504752317003 accuracy 0.9811395406723022 macro_avg {'precision': 0.9805715189575753, 'recall': 0.9803401753088558, 'f1-score': 0.9803804930829216, 'support': 10180} weighted_avg {'precision': 0.98131968168362, 'recall': 0.981139489194499, 'f1-score': 0.9811527411520895, 'support': 10180}
 
time = 5.17 secondes

Val loss 0.8453732852956845 accuracy 0.8885942101478577 macro_avg {'precision': 0.902082337714034, 'recall': 0.8890730667921061, 'f1-score': 0.891617305679333, 'support': 1131} weighted_avg {'precision': 0.8986998294899342, 'recall': 0.8885941644562334, 'f1-score': 0.8897343777392337, 'support': 1131}
 
----------
Epoch 18/40
time = 203.54 secondes

Train loss 0.08381922495627205 accuracy 0.9843811988830566 macro_avg {'precision': 0.984272782688097, 'recall': 0.9839578681197978, 'f1-score': 0.9840843668630347, 'support': 10180} weighted_avg {'precision': 0.9845186191159584, 'recall': 0.9843811394891945, 'f1-score': 0.9844185202533181, 'support': 10180}
 
time = 5.15 secondes

Val loss 0.7698563011186886 accuracy 0.9009726047515869 macro_avg {'precision': 0.9032306036189622, 'recall': 0.9017162111618072, 'f1-score': 0.9005140557890028, 'support': 1131} weighted_avg {'precision': 0.9044900945968019, 'recall': 0.900972590627763, 'f1-score': 0.9008585131382594, 'support': 1131}
 
----------
Epoch 19/40
time = 200.12 secondes

Train loss 0.07774621636402158 accuracy 0.9844794273376465 macro_avg {'precision': 0.9844705608546137, 'recall': 0.9842467424786147, 'f1-score': 0.9843402799281398, 'support': 10180} weighted_avg {'precision': 0.9845289034152427, 'recall': 0.9844793713163065, 'f1-score': 0.9844853244541902, 'support': 10180}
 
time = 5.13 secondes

Val loss 0.9282612968291546 accuracy 0.8859416842460632 macro_avg {'precision': 0.8857641924052956, 'recall': 0.8866983142542354, 'f1-score': 0.8830840096781509, 'support': 1131} weighted_avg {'precision': 0.8903587567562425, 'recall': 0.8859416445623343, 'f1-score': 0.884972125081747, 'support': 1131}
 
----------
Epoch 20/40
time = 202.44 secondes

Train loss 0.07806041470714514 accuracy 0.9846758842468262 macro_avg {'precision': 0.9846186731251763, 'recall': 0.984203946336258, 'f1-score': 0.9843833116725482, 'support': 10180} weighted_avg {'precision': 0.9847298026116429, 'recall': 0.9846758349705305, 'f1-score': 0.9846758006290506, 'support': 10180}
 
time = 5.09 secondes

Val loss 0.9195620465839632 accuracy 0.8885942101478577 macro_avg {'precision': 0.8996189934575861, 'recall': 0.886956677784112, 'f1-score': 0.8884017869481887, 'support': 1131} weighted_avg {'precision': 0.8987616023052974, 'recall': 0.8885941644562334, 'f1-score': 0.8888897867065624, 'support': 1131}
 
----------
Epoch 21/40
time = 202.99 secondes

Train loss 0.08466162592932155 accuracy 0.9856581687927246 macro_avg {'precision': 0.9857069692761831, 'recall': 0.9854515041184667, 'f1-score': 0.9855341462551713, 'support': 10180} weighted_avg {'precision': 0.9857615426847307, 'recall': 0.9856581532416503, 'f1-score': 0.9856641817400487, 'support': 10180}
 
time = 5.11 secondes

Val loss 0.8583344481202928 accuracy 0.9000884294509888 macro_avg {'precision': 0.9103529944816569, 'recall': 0.9030539611900477, 'f1-score': 0.9034922109438431, 'support': 1131} weighted_avg {'precision': 0.9104089726486763, 'recall': 0.9000884173297966, 'f1-score': 0.9016667465871281, 'support': 1131}
 
----------
Epoch 22/40
time = 200.16 secondes

Train loss 0.062444606036043473 accuracy 0.9881139993667603 macro_avg {'precision': 0.9881060584729632, 'recall': 0.9880187421066028, 'f1-score': 0.9880446733458962, 'support': 10180} weighted_avg {'precision': 0.9881748355993345, 'recall': 0.9881139489194499, 'f1-score': 0.9881258378457359, 'support': 10180}
 
time = 5.09 secondes

Val loss 0.8839668158231087 accuracy 0.8938992023468018 macro_avg {'precision': 0.9029307156821147, 'recall': 0.8955877889952214, 'f1-score': 0.8959547951627373, 'support': 1131} weighted_avg {'precision': 0.9022589807903496, 'recall': 0.8938992042440318, 'f1-score': 0.8946573727447141, 'support': 1131}
 
----------
Epoch 23/40
time = 201.82 secondes

Train loss 0.06451394246867885 accuracy 0.9873281121253967 macro_avg {'precision': 0.9869195081537958, 'recall': 0.9871622146698338, 'f1-score': 0.9870055099112929, 'support': 10180} weighted_avg {'precision': 0.9873893808907674, 'recall': 0.9873280943025541, 'f1-score': 0.9873238839060552, 'support': 10180}
 
time = 5.10 secondes

Val loss 0.7839221800978764 accuracy 0.9045093059539795 macro_avg {'precision': 0.9138796554422448, 'recall': 0.9062734455558689, 'f1-score': 0.9074250583852054, 'support': 1131} weighted_avg {'precision': 0.9128909837987678, 'recall': 0.9045092838196287, 'f1-score': 0.9059313179156934, 'support': 1131}
 
----------
Epoch 24/40
time = 203.28 secondes

Train loss 0.05616433390866378 accuracy 0.9901768565177917 macro_avg {'precision': 0.9897648684360794, 'recall': 0.9898570035153524, 'f1-score': 0.9897961802494368, 'support': 10180} weighted_avg {'precision': 0.9902191981040206, 'recall': 0.9901768172888016, 'f1-score': 0.9901835447877939, 'support': 10180}
 
time = 5.12 secondes

Val loss 0.7533812486371074 accuracy 0.9089301824569702 macro_avg {'precision': 0.9111128596680805, 'recall': 0.9084077999412925, 'f1-score': 0.9077680076134204, 'support': 1131} weighted_avg {'precision': 0.912196754383972, 'recall': 0.9089301503094607, 'f1-score': 0.9085963750024066, 'support': 1131}
 
----------
Epoch 25/40
time = 199.80 secondes

Train loss 0.06518361428625298 accuracy 0.9887033700942993 macro_avg {'precision': 0.9888477577962522, 'recall': 0.9886421941035713, 'f1-score': 0.9887194721905523, 'support': 10180} weighted_avg {'precision': 0.9887486375845935, 'recall': 0.9887033398821218, 'f1-score': 0.9886994898822425, 'support': 10180}
 
time = 5.14 secondes

Val loss 0.9257794599364222 accuracy 0.890362560749054 macro_avg {'precision': 0.8962508743055159, 'recall': 0.8927472579343527, 'f1-score': 0.8912577862645863, 'support': 1131} weighted_avg {'precision': 0.897170479915219, 'recall': 0.8903625110521662, 'f1-score': 0.8904102847421176, 'support': 1131}
 
----------
Epoch 26/40
time = 203.06 secondes

Train loss 0.05242736583701114 accuracy 0.9895874857902527 macro_avg {'precision': 0.9894717501466228, 'recall': 0.9893638904003715, 'f1-score': 0.9894007266653899, 'support': 10180} weighted_avg {'precision': 0.9896290208780252, 'recall': 0.9895874263261296, 'f1-score': 0.9895912808586813, 'support': 10180}
 
time = 5.10 secondes

Val loss 0.8603688037004708 accuracy 0.8965517282485962 macro_avg {'precision': 0.9014813871580236, 'recall': 0.8978610110214522, 'f1-score': 0.8974082926000495, 'support': 1131} weighted_avg {'precision': 0.9015800079762006, 'recall': 0.896551724137931, 'f1-score': 0.8968304179249954, 'support': 1131}
 
----------
Epoch 27/40
time = 202.64 secondes

Train loss 0.04529899698223641 accuracy 0.9913556575775146 macro_avg {'precision': 0.9915820023883208, 'recall': 0.9913454685501621, 'f1-score': 0.9914442250104646, 'support': 10180} weighted_avg {'precision': 0.9914038717742452, 'recall': 0.9913555992141454, 'f1-score': 0.9913599607664816, 'support': 10180}
 
time = 5.11 secondes

Val loss 0.8969891099716311 accuracy 0.8974359035491943 macro_avg {'precision': 0.9008399417747723, 'recall': 0.8994961134168488, 'f1-score': 0.8979826120386065, 'support': 1131} weighted_avg {'precision': 0.904149252481342, 'recall': 0.8974358974358975, 'f1-score': 0.8985378863699996, 'support': 1131}
 
----------
Epoch 28/40
time = 204.18 secondes

Train loss 0.05108175813879047 accuracy 0.9910609126091003 macro_avg {'precision': 0.9911567742850164, 'recall': 0.9909569769341549, 'f1-score': 0.9910368444842204, 'support': 10180} weighted_avg {'precision': 0.9911252047140241, 'recall': 0.9910609037328094, 'f1-score': 0.9910720873985971, 'support': 10180}
 
time = 5.29 secondes

Val loss 0.8190806688882064 accuracy 0.9027409553527832 macro_avg {'precision': 0.9114920096131831, 'recall': 0.9032974755098918, 'f1-score': 0.9047965022741252, 'support': 1131} weighted_avg {'precision': 0.911274930137704, 'recall': 0.9027409372236959, 'f1-score': 0.9043044309372125, 'support': 1131}
 
----------
Epoch 29/40
time = 199.58 secondes

Train loss 0.04091004007148932 accuracy 0.9912574291229248 macro_avg {'precision': 0.991267257233911, 'recall': 0.9910872720404041, 'f1-score': 0.9911544197124653, 'support': 10180} weighted_avg {'precision': 0.9913235873069574, 'recall': 0.9912573673870334, 'f1-score': 0.9912665362002464, 'support': 10180}
 
time = 5.11 secondes

Val loss 0.8354962245120219 accuracy 0.9018567800521851 macro_avg {'precision': 0.9080536032495857, 'recall': 0.9026414414904862, 'f1-score': 0.9030298035970935, 'support': 1131} weighted_avg {'precision': 0.9071284200096666, 'recall': 0.9018567639257294, 'f1-score': 0.9021490078380779, 'support': 1131}
 
----------
Epoch 30/40
time = 203.47 secondes

Train loss 0.044382697005883764 accuracy 0.9918467998504639 macro_avg {'precision': 0.992032495409681, 'recall': 0.9916834991755031, 'f1-score': 0.9918335244764119, 'support': 10180} weighted_avg {'precision': 0.9919227672612572, 'recall': 0.9918467583497053, 'f1-score': 0.9918598203585008, 'support': 10180}
 
time = 5.18 secondes

Val loss 0.8414506924661035 accuracy 0.9018567800521851 macro_avg {'precision': 0.9066980418856456, 'recall': 0.901754959845119, 'f1-score': 0.9023424366021773, 'support': 1131} weighted_avg {'precision': 0.906523376102606, 'recall': 0.9018567639257294, 'f1-score': 0.9022974685718887, 'support': 1131}
 
----------
Epoch 31/40
time = 202.78 secondes

Train loss 0.045455777819670105 accuracy 0.991748571395874 macro_avg {'precision': 0.9918570462898003, 'recall': 0.991663467892878, 'f1-score': 0.9917475653810481, 'support': 10180} weighted_avg {'precision': 0.991778853814034, 'recall': 0.9917485265225933, 'f1-score': 0.9917512959031628, 'support': 10180}
 
time = 5.13 secondes

Val loss 0.7785432401130531 accuracy 0.8974359035491943 macro_avg {'precision': 0.9022909698988879, 'recall': 0.8980866650753297, 'f1-score': 0.8975610748783026, 'support': 1131} weighted_avg {'precision': 0.904015982287828, 'recall': 0.8974358974358975, 'f1-score': 0.8979656423928776, 'support': 1131}
 
----------
Epoch 32/40
time = 198.11 secondes

Train loss 0.03615850402612627 accuracy 0.993516743183136 macro_avg {'precision': 0.9936520710086351, 'recall': 0.993520158486368, 'f1-score': 0.9935758796184124, 'support': 10180} weighted_avg {'precision': 0.9935561402611987, 'recall': 0.993516699410609, 'f1-score': 0.9935258842102437, 'support': 10180}
 
time = 5.12 secondes

Val loss 0.8019973540455833 accuracy 0.9036251306533813 macro_avg {'precision': 0.9116655078397933, 'recall': 0.9036691927418984, 'f1-score': 0.9057895596090348, 'support': 1131} weighted_avg {'precision': 0.9104416578125923, 'recall': 0.9036251105216623, 'f1-score': 0.9050122736834665, 'support': 1131}
 
----------
Epoch 33/40
time = 202.33 secondes

Train loss 0.028757735602177526 accuracy 0.9937132000923157 macro_avg {'precision': 0.9937388788153891, 'recall': 0.9936870688340098, 'f1-score': 0.9937021228218802, 'support': 10180} weighted_avg {'precision': 0.9937479341009763, 'recall': 0.993713163064833, 'f1-score': 0.9937193933349128, 'support': 10180}
 
time = 5.10 secondes

Val loss 0.944094180917219 accuracy 0.8930150270462036 macro_avg {'precision': 0.89367724623068, 'recall': 0.8938621645056513, 'f1-score': 0.891707494006878, 'support': 1131} weighted_avg {'precision': 0.8982591799052255, 'recall': 0.8930150309460654, 'f1-score': 0.8936891738856838, 'support': 1131}
 
----------
Epoch 34/40
time = 203.44 secondes

Train loss 0.03135880327386397 accuracy 0.9944007992744446 macro_avg {'precision': 0.9940756997860698, 'recall': 0.9941234494243387, 'f1-score': 0.9940955455739438, 'support': 10180} weighted_avg {'precision': 0.9944169895135516, 'recall': 0.9944007858546169, 'f1-score': 0.9944047903091855, 'support': 10180}
 
time = 5.10 secondes

Val loss 0.8709927225034745 accuracy 0.9027409553527832 macro_avg {'precision': 0.9063682116545909, 'recall': 0.9027020841955166, 'f1-score': 0.903084378435769, 'support': 1131} weighted_avg {'precision': 0.9062916208015509, 'recall': 0.9027409372236959, 'f1-score': 0.9030600352929172, 'support': 1131}
 
----------
Epoch 35/40
time = 199.96 secondes

Train loss 0.019141043258265253 accuracy 0.9953831434249878 macro_avg {'precision': 0.995525994274038, 'recall': 0.9955114258975598, 'f1-score': 0.9955136333790179, 'support': 10180} weighted_avg {'precision': 0.995402714057357, 'recall': 0.9953831041257367, 'f1-score': 0.9953877178517208, 'support': 10180}
 
time = 5.12 secondes

Val loss 0.9813913225456751 accuracy 0.8974359035491943 macro_avg {'precision': 0.9018511639597117, 'recall': 0.8990366170653811, 'f1-score': 0.8975594732921642, 'support': 1131} weighted_avg {'precision': 0.903134162816471, 'recall': 0.8974358974358975, 'f1-score': 0.8973137701921458, 'support': 1131}
 
----------
Epoch 36/40
time = 201.96 secondes

Train loss 0.024867460250053704 accuracy 0.9953831434249878 macro_avg {'precision': 0.9954837387319329, 'recall': 0.9953693559209797, 'f1-score': 0.9954089723450805, 'support': 10180} weighted_avg {'precision': 0.9954404213798782, 'recall': 0.9953831041257367, 'f1-score': 0.9953934220829588, 'support': 10180}
 
time = 5.16 secondes

Val loss 0.890746635889435 accuracy 0.9062776565551758 macro_avg {'precision': 0.9061261697523093, 'recall': 0.9063408881291497, 'f1-score': 0.9051870394526655, 'support': 1131} weighted_avg {'precision': 0.9085154322156456, 'recall': 0.9062776304155614, 'f1-score': 0.9062398910563184, 'support': 1131}
 
----------
Epoch 37/40
time = 201.06 secondes

Train loss 0.018488128964641727 accuracy 0.995874285697937 macro_avg {'precision': 0.9961134147039161, 'recall': 0.9959204753361279, 'f1-score': 0.9960031652681884, 'support': 10180} weighted_avg {'precision': 0.9959227276705296, 'recall': 0.9958742632612967, 'f1-score': 0.9958838104155373, 'support': 10180}
 
time = 5.13 secondes

Val loss 0.8630679304725151 accuracy 0.9071618318557739 macro_avg {'precision': 0.9060581299736145, 'recall': 0.9078235142280515, 'f1-score': 0.9059416730276804, 'support': 1131} weighted_avg {'precision': 0.9097402580482051, 'recall': 0.9071618037135278, 'f1-score': 0.9073932799203719, 'support': 1131}
 
----------
Epoch 38/40
time = 199.64 secondes

Train loss 0.012778235976163576 accuracy 0.9968566298484802 macro_avg {'precision': 0.9970429204475788, 'recall': 0.9968582336398619, 'f1-score': 0.996936891497759, 'support': 10180} weighted_avg {'precision': 0.9969053432362469, 'recall': 0.9968565815324165, 'f1-score': 0.9968664192357776, 'support': 10180}
 
time = 5.12 secondes

Val loss 0.8419327125734462 accuracy 0.9000884294509888 macro_avg {'precision': 0.9030556750087054, 'recall': 0.9019582204596409, 'f1-score': 0.9009785215138241, 'support': 1131} weighted_avg {'precision': 0.9054322415054575, 'recall': 0.9000884173297966, 'f1-score': 0.9011073733809659, 'support': 1131}
 
----------
Epoch 39/40
time = 202.26 secondes

Train loss 0.009799071392466305 accuracy 0.9971513152122498 macro_avg {'precision': 0.9973121096449835, 'recall': 0.9971647545605096, 'f1-score': 0.9972248807314523, 'support': 10180} weighted_avg {'precision': 0.9972007305898344, 'recall': 0.9971512770137525, 'f1-score': 0.9971615194677298, 'support': 10180}
 
time = 5.12 secondes

Val loss 0.9242355882245096 accuracy 0.9000884294509888 macro_avg {'precision': 0.9041258031835306, 'recall': 0.9018346904671353, 'f1-score': 0.9011479909164557, 'support': 1131} weighted_avg {'precision': 0.9048447081192499, 'recall': 0.9000884173297966, 'f1-score': 0.900553094354703, 'support': 1131}
 
----------
Epoch 40/40
time = 202.68 secondes

Train loss 0.008669216860359128 accuracy 0.9972495436668396 macro_avg {'precision': 0.9973990868917099, 'recall': 0.9972493736020038, 'f1-score': 0.9973098341838338, 'support': 10180} weighted_avg {'precision': 0.9972999482248248, 'recall': 0.9972495088408644, 'f1-score': 0.9972593823252186, 'support': 10180}
 
time = 5.13 secondes

Val loss 0.9152871895986068 accuracy 0.9053934812545776 macro_avg {'precision': 0.9087437608176081, 'recall': 0.9068829928389958, 'f1-score': 0.9064969550277236, 'support': 1131} weighted_avg {'precision': 0.9087243911973374, 'recall': 0.905393457117595, 'f1-score': 0.9057197115544622, 'support': 1131}
 
----------
best_accuracy 0.9089301824569702 best_epoch 24 macro_avg {'precision': 0.9111128596680805, 'recall': 0.9084077999412925, 'f1-score': 0.9077680076134204, 'support': 1131} weighted_avg {'precision': 0.912196754383972, 'recall': 0.9089301503094607, 'f1-score': 0.9085963750024066, 'support': 1131}

average train time 202.1133913934231

average val time 5.132225251197815
 
time = 35.91 secondes

test_accuracy 0.837583065032959 macro_avg {'precision': 0.8341593719947882, 'recall': 0.8297976883894324, 'f1-score': 0.8293828422775631, 'support': 7530} weighted_avg {'precision': 0.8423791824826796, 'recall': 0.8375830013280212, 'f1-score': 0.8375193235190962, 'support': 7530}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_head_text_rank_2
----------
Epoch 1/40
time = 192.83 secondes

Train loss 1.304877951203186 accuracy 0.6632612943649292 macro_avg {'precision': 0.684933497082136, 'recall': 0.6470760431923603, 'f1-score': 0.6392342208995447, 'support': 10180} weighted_avg {'precision': 0.6896227435484238, 'recall': 0.6632612966601179, 'f1-score': 0.654054500742433, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.7109028545903487 accuracy 0.790450930595398 macro_avg {'precision': 0.7685794972416977, 'recall': 0.7883143240720665, 'f1-score': 0.7708945635081637, 'support': 1131} weighted_avg {'precision': 0.779186058260377, 'recall': 0.7904509283819628, 'f1-score': 0.7773928433979437, 'support': 1131}
 
----------
Epoch 2/40
time = 195.16 secondes

Train loss 0.4851818757959213 accuracy 0.8592337965965271 macro_avg {'precision': 0.8518489148785025, 'recall': 0.8486204368576697, 'f1-score': 0.8474816910840277, 'support': 10180} weighted_avg {'precision': 0.8579268278886886, 'recall': 0.8592337917485265, 'f1-score': 0.8565330693422355, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.5463822750558316 accuracy 0.8523430824279785 macro_avg {'precision': 0.8587167780323058, 'recall': 0.8499688963284779, 'f1-score': 0.8473253621745476, 'support': 1131} weighted_avg {'precision': 0.8648649083803034, 'recall': 0.852343059239611, 'f1-score': 0.8514851361318142, 'support': 1131}
 
----------
Epoch 3/40
time = 192.44 secondes

Train loss 0.2975011235351559 accuracy 0.9143418669700623 macro_avg {'precision': 0.9094641694149533, 'recall': 0.9081857184694551, 'f1-score': 0.9085423932892647, 'support': 10180} weighted_avg {'precision': 0.9144514509398142, 'recall': 0.9143418467583497, 'f1-score': 0.9141346842031165, 'support': 10180}
 
time = 4.90 secondes

Val loss 0.5503215362960604 accuracy 0.8638373613357544 macro_avg {'precision': 0.8690956219760364, 'recall': 0.869029184410685, 'f1-score': 0.8622609367523463, 'support': 1131} weighted_avg {'precision': 0.873505671504152, 'recall': 0.8638373121131742, 'f1-score': 0.8608414115982411, 'support': 1131}
 
----------
Epoch 4/40
time = 194.44 secondes

Train loss 0.23079245617127797 accuracy 0.9368369579315186 macro_avg {'precision': 0.9336894899716537, 'recall': 0.9330118278535016, 'f1-score': 0.9332247930848098, 'support': 10180} weighted_avg {'precision': 0.9369028029720807, 'recall': 0.9368369351669941, 'f1-score': 0.9367514726619277, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.5498901026349672 accuracy 0.8841733336448669 macro_avg {'precision': 0.8910246308442067, 'recall': 0.8827578591771834, 'f1-score': 0.8823322387420867, 'support': 1131} weighted_avg {'precision': 0.8970499948394581, 'recall': 0.8841732979664014, 'f1-score': 0.8856455974158017, 'support': 1131}
 
----------
Epoch 5/40
time = 194.89 secondes

Train loss 0.19125110974492945 accuracy 0.9512770771980286 macro_avg {'precision': 0.9490857030703481, 'recall': 0.9488569404339838, 'f1-score': 0.9488686505490265, 'support': 10180} weighted_avg {'precision': 0.9515235133127261, 'recall': 0.9512770137524558, 'f1-score': 0.9512998176897711, 'support': 10180}
 
time = 5.00 secondes

Val loss 0.5853671391878251 accuracy 0.8921308517456055 macro_avg {'precision': 0.8988715381059789, 'recall': 0.8903467960569076, 'f1-score': 0.8909531997334609, 'support': 1131} weighted_avg {'precision': 0.9007538182172069, 'recall': 0.8921308576480991, 'f1-score': 0.8928232903946814, 'support': 1131}
 
----------
Epoch 6/40
time = 192.49 secondes

Train loss 0.17305464694185804 accuracy 0.957465648651123 macro_avg {'precision': 0.9561430217424183, 'recall': 0.9561534170080191, 'f1-score': 0.9560160781926743, 'support': 10180} weighted_avg {'precision': 0.9579203502247616, 'recall': 0.9574656188605108, 'f1-score': 0.9575768028793289, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.5384426162050198 accuracy 0.8965517282485962 macro_avg {'precision': 0.8970997716831652, 'recall': 0.8985662976172323, 'f1-score': 0.8955418061550201, 'support': 1131} weighted_avg {'precision': 0.9020957593703197, 'recall': 0.896551724137931, 'f1-score': 0.8968065712335053, 'support': 1131}
 
----------
Epoch 7/40
time = 193.66 secondes

Train loss 0.1558872132588235 accuracy 0.9632613062858582 macro_avg {'precision': 0.9617330245668256, 'recall': 0.9615263060789048, 'f1-score': 0.9616051518631655, 'support': 10180} weighted_avg {'precision': 0.9632301484760426, 'recall': 0.9632612966601178, 'f1-score': 0.9632211606930143, 'support': 10180}
 
time = 4.88 secondes

Val loss 0.5603375958022989 accuracy 0.8983200788497925 macro_avg {'precision': 0.9015834935571032, 'recall': 0.8976433315723449, 'f1-score': 0.8976750595441395, 'support': 1131} weighted_avg {'precision': 0.9029877169616488, 'recall': 0.8983200707338639, 'f1-score': 0.8989289429342041, 'support': 1131}
 
----------
Epoch 8/40
time = 195.84 secondes

Train loss 0.14265692315603135 accuracy 0.9699410796165466 macro_avg {'precision': 0.9686031160169378, 'recall': 0.96841143711183, 'f1-score': 0.9684852141517026, 'support': 10180} weighted_avg {'precision': 0.9699470481649717, 'recall': 0.9699410609037328, 'f1-score': 0.9699213077421919, 'support': 10180}
 
time = 4.91 secondes

Val loss 0.7148574950275156 accuracy 0.8894783854484558 macro_avg {'precision': 0.8926504916962823, 'recall': 0.8892060115460778, 'f1-score': 0.8886965842108194, 'support': 1131} weighted_avg {'precision': 0.894748907746499, 'recall': 0.8894783377541998, 'f1-score': 0.8898568543444971, 'support': 1131}
 
----------
Epoch 9/40
time = 191.64 secondes

Train loss 0.12764359245768872 accuracy 0.9720039367675781 macro_avg {'precision': 0.9709568383903655, 'recall': 0.9704080801965997, 'f1-score': 0.9706249497025988, 'support': 10180} weighted_avg {'precision': 0.972047921568969, 'recall': 0.9720039292730844, 'f1-score': 0.9719691746253513, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.7088952393338255 accuracy 0.8868258595466614 macro_avg {'precision': 0.8904023631022581, 'recall': 0.8874370962056284, 'f1-score': 0.8872352190089072, 'support': 1131} weighted_avg {'precision': 0.8893330956334756, 'recall': 0.8868258178603006, 'f1-score': 0.886294723972137, 'support': 1131}
 
----------
Epoch 10/40
time = 195.22 secondes

Train loss 0.1301909174944283 accuracy 0.9722004532814026 macro_avg {'precision': 0.970997002098389, 'recall': 0.970968528513637, 'f1-score': 0.9709606719879333, 'support': 10180} weighted_avg {'precision': 0.9722897831933406, 'recall': 0.9722003929273084, 'f1-score': 0.9722234408736692, 'support': 10180}
 
time = 4.90 secondes

Val loss 0.7758845753662786 accuracy 0.8921308517456055 macro_avg {'precision': 0.8991447743624077, 'recall': 0.8915787024496036, 'f1-score': 0.8918972389345127, 'support': 1131} weighted_avg {'precision': 0.9006546056369158, 'recall': 0.8921308576480991, 'f1-score': 0.8929169556352162, 'support': 1131}
 
----------
Epoch 11/40
time = 195.07 secondes

Train loss 0.1292074029846519 accuracy 0.9736738801002502 macro_avg {'precision': 0.9725238402565146, 'recall': 0.9730521175486077, 'f1-score': 0.9727384764218339, 'support': 10180} weighted_avg {'precision': 0.9738773111992729, 'recall': 0.9736738703339882, 'f1-score': 0.973731722777175, 'support': 10180}
 
time = 4.91 secondes

Val loss 0.6178360231606861 accuracy 0.9080460071563721 macro_avg {'precision': 0.9127514496440181, 'recall': 0.9059455396478382, 'f1-score': 0.9074173542209287, 'support': 1131} weighted_avg {'precision': 0.910988337787929, 'recall': 0.9080459770114943, 'f1-score': 0.9077723444984355, 'support': 1131}
 
----------
Epoch 12/40
time = 195.88 secondes

Train loss 0.11294008893063372 accuracy 0.9771119952201843 macro_avg {'precision': 0.9770297880709196, 'recall': 0.9770340443424562, 'f1-score': 0.9769953438647189, 'support': 10180} weighted_avg {'precision': 0.9772644177329516, 'recall': 0.9771119842829077, 'f1-score': 0.9771526811453771, 'support': 10180}
 
time = 4.89 secondes

Val loss 0.6537154234386653 accuracy 0.9053934812545776 macro_avg {'precision': 0.9068212439607477, 'recall': 0.9056309001689664, 'f1-score': 0.9048517001181338, 'support': 1131} weighted_avg {'precision': 0.9085531644500696, 'recall': 0.905393457117595, 'f1-score': 0.9055424845202568, 'support': 1131}
 
----------
Epoch 13/40
time = 190.79 secondes

Train loss 0.12594138921144912 accuracy 0.9757367968559265 macro_avg {'precision': 0.9757767889297663, 'recall': 0.9757337622476777, 'f1-score': 0.9757014027112307, 'support': 10180} weighted_avg {'precision': 0.9757997102766107, 'recall': 0.9757367387033399, 'f1-score': 0.9757136150874384, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.7221975133692417 accuracy 0.895667552947998 macro_avg {'precision': 0.8997749432427252, 'recall': 0.89703074147178, 'f1-score': 0.8962751182766147, 'support': 1131} weighted_avg {'precision': 0.9013957053063822, 'recall': 0.8956675508399646, 'f1-score': 0.896375211462796, 'support': 1131}
 
----------
Epoch 14/40
time = 193.58 secondes

Train loss 0.09506121437999002 accuracy 0.9803536534309387 macro_avg {'precision': 0.9806149025219094, 'recall': 0.9806056062790158, 'f1-score': 0.9805531070859341, 'support': 10180} weighted_avg {'precision': 0.9805008688550549, 'recall': 0.9803536345776032, 'f1-score': 0.980369230116816, 'support': 10180}
 
time = 4.98 secondes

Val loss 0.8786322029728055 accuracy 0.8779841065406799 macro_avg {'precision': 0.8832712342467406, 'recall': 0.8786955851699169, 'f1-score': 0.8771090100235147, 'support': 1131} weighted_avg {'precision': 0.887463312622396, 'recall': 0.8779840848806366, 'f1-score': 0.8787284111314068, 'support': 1131}
 
----------
Epoch 15/40
time = 193.53 secondes

Train loss 0.12032892334225055 accuracy 0.9756385684013367 macro_avg {'precision': 0.974867242525176, 'recall': 0.974691584311733, 'f1-score': 0.9747326582364335, 'support': 10180} weighted_avg {'precision': 0.9757528944456827, 'recall': 0.9756385068762279, 'f1-score': 0.9756487786263084, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.6578347568865865 accuracy 0.9000884294509888 macro_avg {'precision': 0.9023512312040787, 'recall': 0.9017020362334522, 'f1-score': 0.9010082651513981, 'support': 1131} weighted_avg {'precision': 0.9028224063743967, 'recall': 0.9000884173297966, 'f1-score': 0.9003596611776701, 'support': 1131}
 
----------
Epoch 16/40
time = 191.71 secondes

Train loss 0.10052321000978477 accuracy 0.9803536534309387 macro_avg {'precision': 0.9798546277794487, 'recall': 0.979789133562849, 'f1-score': 0.9797799382898127, 'support': 10180} weighted_avg {'precision': 0.9804408849093486, 'recall': 0.9803536345776032, 'f1-score': 0.9803559802545864, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.6084573058920592 accuracy 0.9106985330581665 macro_avg {'precision': 0.9207105245939541, 'recall': 0.9104123150311475, 'f1-score': 0.9129805994531415, 'support': 1131} weighted_avg {'precision': 0.9178444636093449, 'recall': 0.9106984969053935, 'f1-score': 0.9118070171111914, 'support': 1131}
 
----------
Epoch 17/40
time = 194.34 secondes

Train loss 0.08154411166397839 accuracy 0.9816306829452515 macro_avg {'precision': 0.9813221863656423, 'recall': 0.9812434045657794, 'f1-score': 0.9812382154224535, 'support': 10180} weighted_avg {'precision': 0.9817148269233423, 'recall': 0.981630648330059, 'f1-score': 0.9816291729823192, 'support': 10180}
 
time = 4.88 secondes

Val loss 0.6662284459018568 accuracy 0.9062776565551758 macro_avg {'precision': 0.9097929312276737, 'recall': 0.9074237138076301, 'f1-score': 0.906505812253142, 'support': 1131} weighted_avg {'precision': 0.9122916939037101, 'recall': 0.9062776304155614, 'f1-score': 0.9071314311815903, 'support': 1131}
 
----------
Epoch 18/40
time = 195.83 secondes

Train loss 0.0910333588439889 accuracy 0.9829077124595642 macro_avg {'precision': 0.982235491809784, 'recall': 0.9824748484889865, 'f1-score': 0.9822965946537228, 'support': 10180} weighted_avg {'precision': 0.9830535314549346, 'recall': 0.9829076620825148, 'f1-score': 0.9829291307110839, 'support': 10180}
 
time = 4.90 secondes

Val loss 0.6384353222178696 accuracy 0.9080460071563721 macro_avg {'precision': 0.9127108446706254, 'recall': 0.9083507182498651, 'f1-score': 0.9084917895605831, 'support': 1131} weighted_avg {'precision': 0.9132944982134296, 'recall': 0.9080459770114943, 'f1-score': 0.9089031681598919, 'support': 1131}
 
----------
Epoch 19/40
time = 190.75 secondes

Train loss 0.07542711656563568 accuracy 0.984774112701416 macro_avg {'precision': 0.9846226399491027, 'recall': 0.9842171869425606, 'f1-score': 0.9843933699985549, 'support': 10180} weighted_avg {'precision': 0.984846335879688, 'recall': 0.9847740667976425, 'f1-score': 0.9847839046548484, 'support': 10180}
 
time = 4.91 secondes

Val loss 0.9013998420539305 accuracy 0.8912467360496521 macro_avg {'precision': 0.901663252233957, 'recall': 0.8904500981422514, 'f1-score': 0.8927496327311815, 'support': 1131} weighted_avg {'precision': 0.9014672181095184, 'recall': 0.8912466843501327, 'f1-score': 0.8929937648585805, 'support': 1131}
 
----------
Epoch 20/40
time = 194.99 secondes

Train loss 0.07989221564737824 accuracy 0.9841846823692322 macro_avg {'precision': 0.9838235783683895, 'recall': 0.9835466020317121, 'f1-score': 0.9836639725293072, 'support': 10180} weighted_avg {'precision': 0.9842495216500903, 'recall': 0.9841846758349705, 'f1-score': 0.9841965971464258, 'support': 10180}
 
time = 4.90 secondes

Val loss 0.7523394067370838 accuracy 0.8921308517456055 macro_avg {'precision': 0.894175594618431, 'recall': 0.8946113871084304, 'f1-score': 0.89194495659049, 'support': 1131} weighted_avg {'precision': 0.8979287799792292, 'recall': 0.8921308576480991, 'f1-score': 0.8925436723770708, 'support': 1131}
 
----------
Epoch 21/40
time = 191.38 secondes

Train loss 0.06720184422701833 accuracy 0.987131655216217 macro_avg {'precision': 0.9866221540676602, 'recall': 0.9868829615835815, 'f1-score': 0.9867284722774226, 'support': 10180} weighted_avg {'precision': 0.9872214316246142, 'recall': 0.9871316306483301, 'f1-score': 0.987154124550412, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.7407748821239859 accuracy 0.9062776565551758 macro_avg {'precision': 0.9113668500405447, 'recall': 0.903556100226251, 'f1-score': 0.9053544444609262, 'support': 1131} weighted_avg {'precision': 0.9109945170830356, 'recall': 0.9062776304155614, 'f1-score': 0.9065602495728219, 'support': 1131}
 
----------
Epoch 22/40
time = 194.70 secondes

Train loss 0.06487486221072512 accuracy 0.9864440560340881 macro_avg {'precision': 0.9862251819000788, 'recall': 0.986496107140726, 'f1-score': 0.9863117371759632, 'support': 10180} weighted_avg {'precision': 0.9865907773788603, 'recall': 0.9864440078585461, 'f1-score': 0.9864723989807473, 'support': 10180}
 
time = 4.91 secondes

Val loss 0.761476838041911 accuracy 0.9053934812545776 macro_avg {'precision': 0.9100655880154644, 'recall': 0.9077473537656677, 'f1-score': 0.9059643446738226, 'support': 1131} weighted_avg {'precision': 0.913688813611767, 'recall': 0.905393457117595, 'f1-score': 0.9066524959525804, 'support': 1131}
 
----------
Epoch 23/40
time = 195.43 secondes

Train loss 0.060323261091883296 accuracy 0.9878193140029907 macro_avg {'precision': 0.9877549910093911, 'recall': 0.9876815712121685, 'f1-score': 0.9876986218297492, 'support': 10180} weighted_avg {'precision': 0.9878726485073702, 'recall': 0.9878192534381139, 'f1-score': 0.9878271970289536, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.8697210245963118 accuracy 0.8965517282485962 macro_avg {'precision': 0.9063754289975101, 'recall': 0.89718249756239, 'f1-score': 0.897542678190552, 'support': 1131} weighted_avg {'precision': 0.9097981314441611, 'recall': 0.896551724137931, 'f1-score': 0.8989518424211319, 'support': 1131}
 
----------
Epoch 24/40
time = 192.14 secondes

Train loss 0.07722641360683791 accuracy 0.9860511422157288 macro_avg {'precision': 0.9858085349758662, 'recall': 0.9853257429884545, 'f1-score': 0.9855297208077317, 'support': 10180} weighted_avg {'precision': 0.9861079522951649, 'recall': 0.9860510805500983, 'f1-score': 0.9860488577459507, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.832579533476362 accuracy 0.895667552947998 macro_avg {'precision': 0.9027902081754794, 'recall': 0.8975278897092558, 'f1-score': 0.8982997219253317, 'support': 1131} weighted_avg {'precision': 0.9032244100557658, 'recall': 0.8956675508399646, 'f1-score': 0.8974961240136377, 'support': 1131}
 
----------
Epoch 25/40
time = 195.41 secondes

Train loss 0.061803958053290156 accuracy 0.9885069131851196 macro_avg {'precision': 0.9881892754505909, 'recall': 0.9880734053749156, 'f1-score': 0.9881130073055335, 'support': 10180} weighted_avg {'precision': 0.9885695837649797, 'recall': 0.9885068762278978, 'f1-score': 0.9885189741703992, 'support': 10180}
 
time = 4.91 secondes

Val loss 0.7158826509220886 accuracy 0.9106985330581665 macro_avg {'precision': 0.9139448849828075, 'recall': 0.9101632560293945, 'f1-score': 0.9104311818909407, 'support': 1131} weighted_avg {'precision': 0.9141875462822123, 'recall': 0.9106984969053935, 'f1-score': 0.9109482564781154, 'support': 1131}
 
----------
Epoch 26/40
time = 193.08 secondes

Train loss 0.05361533028953029 accuracy 0.9897839426994324 macro_avg {'precision': 0.9895770300906788, 'recall': 0.9895164712460183, 'f1-score': 0.9895167604645412, 'support': 10180} weighted_avg {'precision': 0.989879580226626, 'recall': 0.9897838899803536, 'f1-score': 0.9898022802308987, 'support': 10180}
 
time = 4.98 secondes

Val loss 0.7579738540513626 accuracy 0.9062776565551758 macro_avg {'precision': 0.9122240975828568, 'recall': 0.9076127711482413, 'f1-score': 0.9086752530118017, 'support': 1131} weighted_avg {'precision': 0.9103666139454673, 'recall': 0.9062776304155614, 'f1-score': 0.9071903714854466, 'support': 1131}
 
----------
Epoch 27/40
time = 192.36 secondes

Train loss 0.03763685244274636 accuracy 0.9914538860321045 macro_avg {'precision': 0.9911704807662176, 'recall': 0.9911209902079235, 'f1-score': 0.9911284773638043, 'support': 10180} weighted_avg {'precision': 0.9915277362460394, 'recall': 0.9914538310412574, 'f1-score': 0.9914733364625007, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.7569977905525224 accuracy 0.9098143577575684 macro_avg {'precision': 0.9119653515917934, 'recall': 0.9122396465153466, 'f1-score': 0.9105069693100036, 'support': 1131} weighted_avg {'precision': 0.9133462887522767, 'recall': 0.9098143236074271, 'f1-score': 0.9100999530350418, 'support': 1131}
 
----------
Epoch 28/40
time = 193.43 secondes

Train loss 0.053733352700682774 accuracy 0.9903733134269714 macro_avg {'precision': 0.9904440441927835, 'recall': 0.9901822919487205, 'f1-score': 0.9903012066958528, 'support': 10180} weighted_avg {'precision': 0.990403119290574, 'recall': 0.9903732809430256, 'f1-score': 0.9903768627447153, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.7873508231264488 accuracy 0.9045093059539795 macro_avg {'precision': 0.9086755112090448, 'recall': 0.9079668765444511, 'f1-score': 0.9067680220395802, 'support': 1131} weighted_avg {'precision': 0.9089524489409418, 'recall': 0.9045092838196287, 'f1-score': 0.9051578384291663, 'support': 1131}
 
----------
Epoch 29/40
time = 196.26 secondes

Train loss 0.042345079359633137 accuracy 0.9920432567596436 macro_avg {'precision': 0.992247024526647, 'recall': 0.9921044016292543, 'f1-score': 0.9921645082270283, 'support': 10180} weighted_avg {'precision': 0.9920888744642621, 'recall': 0.9920432220039292, 'f1-score': 0.9920543686154943, 'support': 10180}
 
time = 5.16 secondes

Val loss 0.9422463250070713 accuracy 0.8983200788497925 macro_avg {'precision': 0.9068031608813172, 'recall': 0.901737960391358, 'f1-score': 0.9010481241364158, 'support': 1131} weighted_avg {'precision': 0.9099179654274951, 'recall': 0.8983200707338639, 'f1-score': 0.9008662099702266, 'support': 1131}
 
----------
Epoch 30/40
time = 194.68 secondes

Train loss 0.04205482981460577 accuracy 0.9914538860321045 macro_avg {'precision': 0.9910017920923136, 'recall': 0.9912142781524249, 'f1-score': 0.991086052344625, 'support': 10180} weighted_avg {'precision': 0.9915125235973962, 'recall': 0.9914538310412574, 'f1-score': 0.9914617294775792, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.6977900874640705 accuracy 0.9062776565551758 macro_avg {'precision': 0.9077141031301939, 'recall': 0.908530194018684, 'f1-score': 0.9070618580025105, 'support': 1131} weighted_avg {'precision': 0.9104052712524603, 'recall': 0.9062776304155614, 'f1-score': 0.9072879053023886, 'support': 1131}
 
----------
Epoch 31/40
time = 196.43 secondes

Train loss 0.03835557566215434 accuracy 0.9919450283050537 macro_avg {'precision': 0.991846230293118, 'recall': 0.9918851770329493, 'f1-score': 0.9918561788722696, 'support': 10180} weighted_avg {'precision': 0.9919905679091481, 'recall': 0.9919449901768173, 'f1-score': 0.9919579713795728, 'support': 10180}
 
time = 5.14 secondes

Val loss 0.7668636810076734 accuracy 0.9053934812545776 macro_avg {'precision': 0.9096455694815896, 'recall': 0.9062649693171757, 'f1-score': 0.9054483659040485, 'support': 1131} weighted_avg {'precision': 0.9082429430920668, 'recall': 0.905393457117595, 'f1-score': 0.9044015088305736, 'support': 1131}
 
----------
Epoch 32/40
time = 197.55 secondes

Train loss 0.03791173723401751 accuracy 0.9936149716377258 macro_avg {'precision': 0.9937495810306215, 'recall': 0.9934956265387953, 'f1-score': 0.9936043367475718, 'support': 10180} weighted_avg {'precision': 0.9936656711144367, 'recall': 0.993614931237721, 'f1-score': 0.9936216050022687, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.7937778046464918 accuracy 0.9036251306533813 macro_avg {'precision': 0.9056037501535797, 'recall': 0.9054862808645787, 'f1-score': 0.9038611024891268, 'support': 1131} weighted_avg {'precision': 0.9065836532516758, 'recall': 0.9036251105216623, 'f1-score': 0.9033938115265311, 'support': 1131}
 
----------
Epoch 33/40
time = 194.07 secondes

Train loss 0.03197248526983178 accuracy 0.9928290843963623 macro_avg {'precision': 0.9930132863016901, 'recall': 0.9929644912433024, 'f1-score': 0.9929654617997311, 'support': 10180} weighted_avg {'precision': 0.992874530231181, 'recall': 0.9928290766208252, 'f1-score': 0.9928273576162562, 'support': 10180}
 
time = 5.03 secondes

Val loss 0.6823563730148136 accuracy 0.9213085770606995 macro_avg {'precision': 0.9213082146614191, 'recall': 0.9231626935596567, 'f1-score': 0.9215719675270402, 'support': 1131} weighted_avg {'precision': 0.922561729036036, 'recall': 0.9213085764809903, 'f1-score': 0.9213314904236007, 'support': 1131}
 
----------
Epoch 34/40
time = 196.88 secondes

Train loss 0.024707890651417805 accuracy 0.9943025708198547 macro_avg {'precision': 0.9944244618937962, 'recall': 0.9943907329168056, 'f1-score': 0.9943926414995472, 'support': 10180} weighted_avg {'precision': 0.9943608151781491, 'recall': 0.9943025540275049, 'f1-score': 0.9943162607384323, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.828356506125819 accuracy 0.9071618318557739 macro_avg {'precision': 0.914461369822255, 'recall': 0.9086297633818337, 'f1-score': 0.9093551275596287, 'support': 1131} weighted_avg {'precision': 0.912958411775704, 'recall': 0.9071618037135278, 'f1-score': 0.9078297236884193, 'support': 1131}
 
----------
Epoch 35/40
time = 197.10 secondes

Train loss 0.025072278798086264 accuracy 0.9951866865158081 macro_avg {'precision': 0.9953951816145338, 'recall': 0.9951726738926304, 'f1-score': 0.9952680197148431, 'support': 10180} weighted_avg {'precision': 0.995235945130417, 'recall': 0.9951866404715127, 'f1-score': 0.995195157697333, 'support': 10180}
 
time = 5.01 secondes

Val loss 0.7957512441644725 accuracy 0.9106985330581665 macro_avg {'precision': 0.9150305668097667, 'recall': 0.9133473265179436, 'f1-score': 0.9124088248940959, 'support': 1131} weighted_avg {'precision': 0.916823002984134, 'recall': 0.9106984969053935, 'f1-score': 0.9119463536588648, 'support': 1131}
 
----------
Epoch 36/40
time = 194.14 secondes

Train loss 0.01843293466192858 accuracy 0.995874285697937 macro_avg {'precision': 0.9960115079714121, 'recall': 0.9958795108063171, 'f1-score': 0.9959345782516105, 'support': 10180} weighted_avg {'precision': 0.9959106112658359, 'recall': 0.9958742632612967, 'f1-score': 0.9958809525360908, 'support': 10180}
 
time = 4.97 secondes

Val loss 0.8271791080475761 accuracy 0.9098143577575684 macro_avg {'precision': 0.9123347346153704, 'recall': 0.9128105567660612, 'f1-score': 0.9114405176789664, 'support': 1131} weighted_avg {'precision': 0.9121019565069725, 'recall': 0.9098143236074271, 'f1-score': 0.9098203338999097, 'support': 1131}
 
----------
Epoch 37/40
time = 196.64 secondes

Train loss 0.019590134617314882 accuracy 0.9953831434249878 macro_avg {'precision': 0.9955366113354266, 'recall': 0.9951811842278403, 'f1-score': 0.9953428488328905, 'support': 10180} weighted_avg {'precision': 0.9954377201548369, 'recall': 0.9953831041257367, 'f1-score': 0.9953944201676749, 'support': 10180}
 
time = 4.91 secondes

Val loss 0.7891755461697729 accuracy 0.9142352342605591 macro_avg {'precision': 0.9150728842635694, 'recall': 0.9174724188988023, 'f1-score': 0.9154598781067392, 'support': 1131} weighted_avg {'precision': 0.9161684680066517, 'recall': 0.9142351900972591, 'f1-score': 0.9143855153348519, 'support': 1131}
 
----------
Epoch 38/40
time = 194.96 secondes

Train loss 0.015046037014774182 accuracy 0.9964637160301208 macro_avg {'precision': 0.9966481033529183, 'recall': 0.996548810612752, 'f1-score': 0.9965733140351292, 'support': 10180} weighted_avg {'precision': 0.9965544484885882, 'recall': 0.9964636542239685, 'f1-score': 0.9964829849015028, 'support': 10180}
 
time = 4.91 secondes

Val loss 0.8118475012771329 accuracy 0.9106985330581665 macro_avg {'precision': 0.9159218503131521, 'recall': 0.9136039446291082, 'f1-score': 0.9129902614571316, 'support': 1131} weighted_avg {'precision': 0.9163938061903955, 'recall': 0.9106984969053935, 'f1-score': 0.9117338363508234, 'support': 1131}
 
----------
Epoch 39/40
time = 192.61 secondes

Train loss 0.01060794002212508 accuracy 0.9967584013938904 macro_avg {'precision': 0.996939455820461, 'recall': 0.9968261182617812, 'f1-score': 0.9968750749923666, 'support': 10180} weighted_avg {'precision': 0.9967955536588224, 'recall': 0.9967583497053045, 'f1-score': 0.9967689660033393, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.740017601515332 accuracy 0.9177719354629517 macro_avg {'precision': 0.9199450898696323, 'recall': 0.9208532968032607, 'f1-score': 0.9194884626302692, 'support': 1131} weighted_avg {'precision': 0.9206615759454426, 'recall': 0.9177718832891246, 'f1-score': 0.9182700744677217, 'support': 1131}
 
----------
Epoch 40/40
time = 195.15 secondes

Train loss 0.01157112315878027 accuracy 0.9967584013938904 macro_avg {'precision': 0.9969483425323208, 'recall': 0.9968251254685047, 'f1-score': 0.9968799780785419, 'support': 10180} weighted_avg {'precision': 0.9967878377226913, 'recall': 0.9967583497053045, 'f1-score': 0.9967659965523454, 'support': 10180}
 
time = 4.97 secondes

Val loss 0.7612453966031725 accuracy 0.9160035848617554 macro_avg {'precision': 0.9179240593787922, 'recall': 0.9185778496752116, 'f1-score': 0.9172607142088948, 'support': 1131} weighted_avg {'precision': 0.9186704011743678, 'recall': 0.9160035366931919, 'f1-score': 0.9163632517299587, 'support': 1131}
 
----------
best_accuracy 0.9213085770606995 best_epoch 33 macro_avg {'precision': 0.9213082146614191, 'recall': 0.9231626935596567, 'f1-score': 0.9215719675270402, 'support': 1131} weighted_avg {'precision': 0.922561729036036, 'recall': 0.9213085764809903, 'f1-score': 0.9213314904236007, 'support': 1131}

average train time 194.23683955073358

average val time 4.9457509338855745
 
time = 33.69 secondes

test_accuracy 0.8501992225646973 macro_avg {'precision': 0.8448287581046536, 'recall': 0.8431471211815271, 'f1-score': 0.8423884803843162, 'support': 7530} weighted_avg {'precision': 0.8521869131508221, 'recall': 0.850199203187251, 'f1-score': 0.8497055481872604, 'support': 7530}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_head_tail_2
----------
Epoch 1/40
time = 201.89 secondes

Train loss 1.3104310929775238 accuracy 0.6687623262405396 macro_avg {'precision': 0.6803656021311654, 'recall': 0.6536414297936949, 'f1-score': 0.6468877489078239, 'support': 10180} weighted_avg {'precision': 0.6882988987218477, 'recall': 0.668762278978389, 'f1-score': 0.6611063816135129, 'support': 10180}
 
time = 5.24 secondes

Val loss 0.6915737432493291 accuracy 0.7869142293930054 macro_avg {'precision': 0.7943768045970518, 'recall': 0.7820833628889579, 'f1-score': 0.7656513071645962, 'support': 1131} weighted_avg {'precision': 0.7962767264559686, 'recall': 0.7869142351900973, 'f1-score': 0.771316967767875, 'support': 1131}
 
----------
Epoch 2/40
time = 199.00 secondes

Train loss 0.48990026293570993 accuracy 0.8547151684761047 macro_avg {'precision': 0.8464498676577014, 'recall': 0.8431405952765493, 'f1-score': 0.8415145084633497, 'support': 10180} weighted_avg {'precision': 0.8534581341592405, 'recall': 0.8547151277013753, 'f1-score': 0.8516444049713717, 'support': 10180}
 
time = 5.02 secondes

Val loss 0.5474100624486594 accuracy 0.8461538553237915 macro_avg {'precision': 0.8497016606536949, 'recall': 0.8437684731662001, 'f1-score': 0.8408854837652354, 'support': 1131} weighted_avg {'precision': 0.857783198772939, 'recall': 0.8461538461538461, 'f1-score': 0.8459339243463342, 'support': 1131}
 
----------
Epoch 3/40
time = 194.51 secondes

Train loss 0.30651459392171876 accuracy 0.9119843244552612 macro_avg {'precision': 0.9059740404668828, 'recall': 0.9048048845978807, 'f1-score': 0.9051468071469799, 'support': 10180} weighted_avg {'precision': 0.9116878869093797, 'recall': 0.9119842829076621, 'f1-score': 0.9116154265378426, 'support': 10180}
 
time = 5.34 secondes

Val loss 0.5138607894293439 accuracy 0.8815208077430725 macro_avg {'precision': 0.8815759410898625, 'recall': 0.8827502858165344, 'f1-score': 0.8807460326188578, 'support': 1131} weighted_avg {'precision': 0.8851236306714597, 'recall': 0.8815207780725022, 'f1-score': 0.8818648427888173, 'support': 1131}
 
----------
Epoch 4/40
time = 197.45 secondes

Train loss 0.24357488397011764 accuracy 0.9342829585075378 macro_avg {'precision': 0.930477280173004, 'recall': 0.9296874309424512, 'f1-score': 0.9299572718915584, 'support': 10180} weighted_avg {'precision': 0.934228288074467, 'recall': 0.9342829076620826, 'f1-score': 0.9341352777562262, 'support': 10180}
 
time = 5.05 secondes

Val loss 0.6084069815254443 accuracy 0.8744474053382874 macro_avg {'precision': 0.8795527868834185, 'recall': 0.8742924212997638, 'f1-score': 0.8732085874118132, 'support': 1131} weighted_avg {'precision': 0.8840423077691218, 'recall': 0.874447391688771, 'f1-score': 0.8758132930880445, 'support': 1131}
 
----------
Epoch 5/40
time = 199.23 secondes

Train loss 0.20296593727375814 accuracy 0.9497053623199463 macro_avg {'precision': 0.946832969074203, 'recall': 0.9459666252544876, 'f1-score': 0.9462456106748259, 'support': 10180} weighted_avg {'precision': 0.9499910196358379, 'recall': 0.9497053045186641, 'f1-score': 0.949695905367524, 'support': 10180}
 
time = 5.11 secondes

Val loss 0.5956223548363737 accuracy 0.8806366324424744 macro_avg {'precision': 0.883726936780478, 'recall': 0.8759323296671797, 'f1-score': 0.8758001350294806, 'support': 1131} weighted_avg {'precision': 0.8861209042795507, 'recall': 0.8806366047745358, 'f1-score': 0.8797202478670639, 'support': 1131}
 
----------
Epoch 6/40
time = 194.07 secondes

Train loss 0.17571292348678283 accuracy 0.9578585624694824 macro_avg {'precision': 0.9563992278650023, 'recall': 0.9559987427059203, 'f1-score': 0.956020536676753, 'support': 10180} weighted_avg {'precision': 0.9583205218827672, 'recall': 0.9578585461689587, 'f1-score': 0.9579252543861168, 'support': 10180}
 
time = 5.01 secondes

Val loss 0.6059911149868767 accuracy 0.8868258595466614 macro_avg {'precision': 0.8965085680007949, 'recall': 0.8856854471406189, 'f1-score': 0.8885273443004389, 'support': 1131} weighted_avg {'precision': 0.8937835660164661, 'recall': 0.8868258178603006, 'f1-score': 0.8877271108586264, 'support': 1131}
 
----------
Epoch 7/40
time = 198.72 secondes

Train loss 0.16035521311577253 accuracy 0.9645383358001709 macro_avg {'precision': 0.9634779647680884, 'recall': 0.9634918935673763, 'f1-score': 0.9634378300974642, 'support': 10180} weighted_avg {'precision': 0.9646616961769805, 'recall': 0.9645383104125737, 'f1-score': 0.9645559236865671, 'support': 10180}
 
time = 5.13 secondes

Val loss 0.814018240892066 accuracy 0.8638373613357544 macro_avg {'precision': 0.8829072337089305, 'recall': 0.8623779554784792, 'f1-score': 0.8643308371414014, 'support': 1131} weighted_avg {'precision': 0.883677052176049, 'recall': 0.8638373121131742, 'f1-score': 0.8658900676115842, 'support': 1131}
 
----------
Epoch 8/40
time = 198.24 secondes

Train loss 0.15085839960754088 accuracy 0.9666011929512024 macro_avg {'precision': 0.9654632036554889, 'recall': 0.9654505288184904, 'f1-score': 0.9653531228923192, 'support': 10180} weighted_avg {'precision': 0.9669618005293356, 'recall': 0.9666011787819253, 'f1-score': 0.9666905201894136, 'support': 10180}
 
time = 5.15 secondes

Val loss 0.6394971773736704 accuracy 0.8974359035491943 macro_avg {'precision': 0.9004087422183152, 'recall': 0.8944842035663634, 'f1-score': 0.8953294884374827, 'support': 1131} weighted_avg {'precision': 0.9012899561637775, 'recall': 0.8974358974358975, 'f1-score': 0.8973615759922685, 'support': 1131}
 
----------
Epoch 9/40
time = 194.12 secondes

Train loss 0.13406884379347192 accuracy 0.9704322814941406 macro_avg {'precision': 0.9694038211249835, 'recall': 0.9689700961468597, 'f1-score': 0.9691490053131787, 'support': 10180} weighted_avg {'precision': 0.9705064002154327, 'recall': 0.9704322200392927, 'f1-score': 0.9704304649935808, 'support': 10180}
 
time = 5.96 secondes

Val loss 0.6244148219268257 accuracy 0.9018567800521851 macro_avg {'precision': 0.9062089839860745, 'recall': 0.9017779975187169, 'f1-score': 0.9025292695623788, 'support': 1131} weighted_avg {'precision': 0.9060469336326354, 'recall': 0.9018567639257294, 'f1-score': 0.9025778420599296, 'support': 1131}
 
----------
Epoch 10/40
time = 198.02 secondes

Train loss 0.13613690361890923 accuracy 0.9716110229492188 macro_avg {'precision': 0.9706599928720359, 'recall': 0.9704602074277341, 'f1-score': 0.9705386424948497, 'support': 10180} weighted_avg {'precision': 0.9716804833945586, 'recall': 0.9716110019646366, 'f1-score': 0.9716241120077351, 'support': 10180}
 
time = 5.03 secondes

Val loss 0.7063044648892968 accuracy 0.8983200788497925 macro_avg {'precision': 0.9054146687851654, 'recall': 0.8972006196456703, 'f1-score': 0.8979978909109472, 'support': 1131} weighted_avg {'precision': 0.9057332924400191, 'recall': 0.8983200707338639, 'f1-score': 0.899137458013837, 'support': 1131}
 
----------
Epoch 11/40
time = 195.11 secondes

Train loss 0.13309289666349244 accuracy 0.9726915955543518 macro_avg {'precision': 0.9718669760007808, 'recall': 0.9716187144183405, 'f1-score': 0.9717164662928974, 'support': 10180} weighted_avg {'precision': 0.9727673921240362, 'recall': 0.9726915520628684, 'f1-score': 0.9727027596360153, 'support': 10180}
 
time = 5.27 secondes

Val loss 0.705178757603559 accuracy 0.8992042541503906 macro_avg {'precision': 0.901592944603863, 'recall': 0.8985366685947745, 'f1-score': 0.8976617506286706, 'support': 1131} weighted_avg {'precision': 0.9048296287079475, 'recall': 0.8992042440318302, 'f1-score': 0.8998738984854352, 'support': 1131}
 
----------
Epoch 12/40
time = 197.44 secondes

Train loss 0.10932253335392048 accuracy 0.9771119952201843 macro_avg {'precision': 0.976318511841486, 'recall': 0.9760999038628068, 'f1-score': 0.9761593514241174, 'support': 10180} weighted_avg {'precision': 0.9772148641860722, 'recall': 0.9771119842829077, 'f1-score': 0.9771129392265032, 'support': 10180}
 
time = 4.97 secondes

Val loss 0.868708328116836 accuracy 0.8806366324424744 macro_avg {'precision': 0.8944883179916345, 'recall': 0.8821653498185726, 'f1-score': 0.8799636774987031, 'support': 1131} weighted_avg {'precision': 0.8990073323411766, 'recall': 0.8806366047745358, 'f1-score': 0.8820114739860578, 'support': 1131}
 
----------
Epoch 13/40
time = 196.62 secondes

Train loss 0.11580295292374582 accuracy 0.9762279391288757 macro_avg {'precision': 0.9754977396585327, 'recall': 0.9756486111338631, 'f1-score': 0.9755266544089839, 'support': 10180} weighted_avg {'precision': 0.976365489285184, 'recall': 0.9762278978388998, 'f1-score': 0.9762505440070616, 'support': 10180}
 
time = 4.99 secondes

Val loss 0.6833037772633195 accuracy 0.9062776565551758 macro_avg {'precision': 0.9070675109084538, 'recall': 0.9070153353139482, 'f1-score': 0.9057700898101633, 'support': 1131} weighted_avg {'precision': 0.9101434046827834, 'recall': 0.9062776304155614, 'f1-score': 0.9069631201676491, 'support': 1131}
 
----------
Epoch 14/40
time = 197.59 secondes

Train loss 0.10039704449997401 accuracy 0.9802554249763489 macro_avg {'precision': 0.9799650485666971, 'recall': 0.9797139180963068, 'f1-score': 0.9797945950474338, 'support': 10180} weighted_avg {'precision': 0.9804382324993113, 'recall': 0.9802554027504912, 'f1-score': 0.9803011921163103, 'support': 10180}
 
time = 5.09 secondes

Val loss 0.7195363809978901 accuracy 0.9018567800521851 macro_avg {'precision': 0.9067784592451125, 'recall': 0.9052683329546003, 'f1-score': 0.9047416795068063, 'support': 1131} weighted_avg {'precision': 0.9073422916132174, 'recall': 0.9018567639257294, 'f1-score': 0.9032961913682404, 'support': 1131}
 
----------
Epoch 15/40
time = 194.91 secondes

Train loss 0.09986430119500211 accuracy 0.9789783954620361 macro_avg {'precision': 0.9786736288861739, 'recall': 0.9787715777166495, 'f1-score': 0.9786611324766087, 'support': 10180} weighted_avg {'precision': 0.9792094270896281, 'recall': 0.9789783889980354, 'f1-score': 0.9790323468746119, 'support': 10180}
 
time = 5.09 secondes

Val loss 0.6660014939323088 accuracy 0.8965517282485962 macro_avg {'precision': 0.9022303389229176, 'recall': 0.9000391106685643, 'f1-score': 0.8987369219252997, 'support': 1131} weighted_avg {'precision': 0.9066895948495836, 'recall': 0.896551724137931, 'f1-score': 0.8992499940632461, 'support': 1131}
 
----------
Epoch 16/40
time = 198.04 secondes

Train loss 0.09003877760393778 accuracy 0.9816306829452515 macro_avg {'precision': 0.9814651932084043, 'recall': 0.9812174233665578, 'f1-score': 0.981302587894886, 'support': 10180} weighted_avg {'precision': 0.9817670330193687, 'recall': 0.981630648330059, 'f1-score': 0.9816591047206332, 'support': 10180}
 
time = 5.01 secondes

Val loss 0.8136994040721386 accuracy 0.8974359035491943 macro_avg {'precision': 0.9086685241427664, 'recall': 0.8990856462645219, 'f1-score': 0.899664497049236, 'support': 1131} weighted_avg {'precision': 0.9100355103789624, 'recall': 0.8974358974358975, 'f1-score': 0.8994412942946252, 'support': 1131}
 
----------
Epoch 17/40
time = 197.07 secondes

Train loss 0.08441453486581565 accuracy 0.983005940914154 macro_avg {'precision': 0.9829272643620515, 'recall': 0.9826602250067994, 'f1-score': 0.9827738599145966, 'support': 10180} weighted_avg {'precision': 0.9830545693976044, 'recall': 0.9830058939096267, 'f1-score': 0.983009898415209, 'support': 10180}
 
time = 5.01 secondes

Val loss 0.7743234470485412 accuracy 0.9009726047515869 macro_avg {'precision': 0.9019919644847401, 'recall': 0.9018067413618489, 'f1-score': 0.9004804158363685, 'support': 1131} weighted_avg {'precision': 0.9027760163977147, 'recall': 0.900972590627763, 'f1-score': 0.9003812468529969, 'support': 1131}
 
----------
Epoch 18/40
time = 193.12 secondes

Train loss 0.07030165351269954 accuracy 0.9859529137611389 macro_avg {'precision': 0.9852889511516285, 'recall': 0.9853911441374354, 'f1-score': 0.9853094317719808, 'support': 10180} weighted_avg {'precision': 0.9860483500188899, 'recall': 0.9859528487229863, 'f1-score': 0.9859707312398764, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.8515962679247564 accuracy 0.8965517282485962 macro_avg {'precision': 0.9036575861360108, 'recall': 0.8932463260602, 'f1-score': 0.8963838736051688, 'support': 1131} weighted_avg {'precision': 0.9020455573370008, 'recall': 0.896551724137931, 'f1-score': 0.8971958773981403, 'support': 1131}
 
----------
Epoch 19/40
time = 195.29 secondes

Train loss 0.08539217136748616 accuracy 0.9831041693687439 macro_avg {'precision': 0.982287131770922, 'recall': 0.9822786403627347, 'f1-score': 0.9822495919534063, 'support': 10180} weighted_avg {'precision': 0.9832269935737704, 'recall': 0.9831041257367387, 'f1-score': 0.9831326885979634, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.7759503930168469 accuracy 0.9036251306533813 macro_avg {'precision': 0.907945266703287, 'recall': 0.90348742306498, 'f1-score': 0.9034268298541202, 'support': 1131} weighted_avg {'precision': 0.9106659441169065, 'recall': 0.9036251105216623, 'f1-score': 0.9047213948717718, 'support': 1131}
 
----------
Epoch 20/40
time = 195.05 secondes

Train loss 0.06908085206674767 accuracy 0.9853634834289551 macro_avg {'precision': 0.9854304461323057, 'recall': 0.9848931899370834, 'f1-score': 0.9850995162909344, 'support': 10180} weighted_avg {'precision': 0.9855164807472147, 'recall': 0.9853634577603143, 'f1-score': 0.9853772505367845, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.8961048136499274 accuracy 0.8938992023468018 macro_avg {'precision': 0.8992928187327367, 'recall': 0.8921707364107097, 'f1-score': 0.8937073409637494, 'support': 1131} weighted_avg {'precision': 0.8984172110513909, 'recall': 0.8938992042440318, 'f1-score': 0.8943541831090805, 'support': 1131}
 
----------
Epoch 21/40
time = 192.50 secondes

Train loss 0.0724513212200171 accuracy 0.9857563972473145 macro_avg {'precision': 0.9855749754360437, 'recall': 0.9855107898776341, 'f1-score': 0.9854998233073358, 'support': 10180} weighted_avg {'precision': 0.9859161990426486, 'recall': 0.9857563850687623, 'f1-score': 0.9857918378155094, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.8149012982815382 accuracy 0.8983200788497925 macro_avg {'precision': 0.9026293090009945, 'recall': 0.9001441576591128, 'f1-score': 0.8993362877337369, 'support': 1131} weighted_avg {'precision': 0.9044624343282066, 'recall': 0.8983200707338639, 'f1-score': 0.8994609371432615, 'support': 1131}
 
----------
Epoch 22/40
time = 195.11 secondes

Train loss 0.07657423331709272 accuracy 0.9862475991249084 macro_avg {'precision': 0.985963469625221, 'recall': 0.9855413918393049, 'f1-score': 0.9857088185650564, 'support': 10180} weighted_avg {'precision': 0.9863109714988214, 'recall': 0.9862475442043221, 'f1-score': 0.9862374598249817, 'support': 10180}
 
time = 4.97 secondes

Val loss 0.7661336666339119 accuracy 0.8992042541503906 macro_avg {'precision': 0.9026676341400627, 'recall': 0.9003793316572498, 'f1-score': 0.8990043030048813, 'support': 1131} weighted_avg {'precision': 0.9057632913413628, 'recall': 0.8992042440318302, 'f1-score': 0.8999396290799172, 'support': 1131}
 
----------
Epoch 23/40
time = 195.81 secondes

Train loss 0.06953531137415714 accuracy 0.9867387413978577 macro_avg {'precision': 0.9861880048278847, 'recall': 0.9860262905013085, 'f1-score': 0.9860799894775519, 'support': 10180} weighted_avg {'precision': 0.9868424504572911, 'recall': 0.9867387033398821, 'f1-score': 0.9867638669099145, 'support': 10180}
 
time = 4.97 secondes

Val loss 1.0778152664196325 accuracy 0.8753315806388855 macro_avg {'precision': 0.8872735913644687, 'recall': 0.8730850756683, 'f1-score': 0.8740774726627277, 'support': 1131} weighted_avg {'precision': 0.8877190807768033, 'recall': 0.8753315649867374, 'f1-score': 0.8755591985410727, 'support': 1131}
 
----------
Epoch 24/40
time = 193.12 secondes

Train loss 0.0666617230918536 accuracy 0.9868369698524475 macro_avg {'precision': 0.9871772369536658, 'recall': 0.9868805102093001, 'f1-score': 0.9869883434694771, 'support': 10180} weighted_avg {'precision': 0.9869760255218321, 'recall': 0.9868369351669941, 'f1-score': 0.9868647608779624, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.8834795278309915 accuracy 0.8921308517456055 macro_avg {'precision': 0.900761692058933, 'recall': 0.8943484749787836, 'f1-score': 0.8931775507448428, 'support': 1131} weighted_avg {'precision': 0.9032309934652561, 'recall': 0.8921308576480991, 'f1-score': 0.8931460060566925, 'support': 1131}
 
----------
Epoch 25/40
time = 194.48 secondes

Train loss 0.06579377672321615 accuracy 0.9872298836708069 macro_avg {'precision': 0.9874526981424528, 'recall': 0.9869973396519546, 'f1-score': 0.9871863976939685, 'support': 10180} weighted_avg {'precision': 0.9873250979711821, 'recall': 0.9872298624754421, 'f1-score': 0.9872388741534543, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.8559125727117365 accuracy 0.9027409553527832 macro_avg {'precision': 0.9044736610753537, 'recall': 0.9038397137033252, 'f1-score': 0.9022855968060947, 'support': 1131} weighted_avg {'precision': 0.9085095063917421, 'recall': 0.9027409372236959, 'f1-score': 0.9036905387616528, 'support': 1131}
 
----------
Epoch 26/40
time = 196.57 secondes

Train loss 0.07171529337539774 accuracy 0.9869351983070374 macro_avg {'precision': 0.986967576192835, 'recall': 0.9869351490929154, 'f1-score': 0.9868931352144286, 'support': 10180} weighted_avg {'precision': 0.9870548466393134, 'recall': 0.9869351669941061, 'f1-score': 0.9869349188425005, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.8022878376147455 accuracy 0.9018567800521851 macro_avg {'precision': 0.9077715796581435, 'recall': 0.9037216744573623, 'f1-score': 0.9039564646761402, 'support': 1131} weighted_avg {'precision': 0.9072652303845143, 'recall': 0.9018567639257294, 'f1-score': 0.902682929517342, 'support': 1131}
 
----------
Epoch 27/40
time = 193.57 secondes

Train loss 0.042009955411132685 accuracy 0.9912574291229248 macro_avg {'precision': 0.9914505709591988, 'recall': 0.9912914259173033, 'f1-score': 0.9913583842897516, 'support': 10180} weighted_avg {'precision': 0.9912946594739132, 'recall': 0.9912573673870334, 'f1-score': 0.9912627146570716, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.9692224716736618 accuracy 0.895667552947998 macro_avg {'precision': 0.9019169712068509, 'recall': 0.8973115935639425, 'f1-score': 0.8957688178927027, 'support': 1131} weighted_avg {'precision': 0.9044568838010953, 'recall': 0.8956675508399646, 'f1-score': 0.896335818773899, 'support': 1131}
 
----------
Epoch 28/40
time = 196.00 secondes

Train loss 0.05256506279383443 accuracy 0.9901768565177917 macro_avg {'precision': 0.9904195512246753, 'recall': 0.9901598164854584, 'f1-score': 0.9902624496339136, 'support': 10180} weighted_avg {'precision': 0.9902732874302903, 'recall': 0.9901768172888016, 'f1-score': 0.9901961178585098, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.9767965449110038 accuracy 0.8921308517456055 macro_avg {'precision': 0.8983806710808084, 'recall': 0.8895545129549258, 'f1-score': 0.8886169115255148, 'support': 1131} weighted_avg {'precision': 0.9057132331837957, 'recall': 0.8921308576480991, 'f1-score': 0.8949347602998732, 'support': 1131}
 
----------
Epoch 29/40
time = 196.83 secondes

Train loss 0.0466960550563059 accuracy 0.9904715418815613 macro_avg {'precision': 0.9906059886583541, 'recall': 0.9904986719469193, 'f1-score': 0.9905301427475528, 'support': 10180} weighted_avg {'precision': 0.9905316577890064, 'recall': 0.9904715127701376, 'f1-score': 0.9904791682145985, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.8100785377594637 accuracy 0.9106985330581665 macro_avg {'precision': 0.914836268575419, 'recall': 0.9096615893352448, 'f1-score': 0.9107644479174348, 'support': 1131} weighted_avg {'precision': 0.9139810397934938, 'recall': 0.9106984969053935, 'f1-score': 0.9109729291858465, 'support': 1131}
 
----------
Epoch 30/40
time = 191.45 secondes

Train loss 0.039345683902269044 accuracy 0.9925343990325928 macro_avg {'precision': 0.9926582734188303, 'recall': 0.9922693887483334, 'f1-score': 0.9924511916825915, 'support': 10180} weighted_avg {'precision': 0.9925711474081322, 'recall': 0.9925343811394892, 'f1-score': 0.9925410911746068, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.7917593481837634 accuracy 0.9089301824569702 macro_avg {'precision': 0.9119303613487432, 'recall': 0.9107229093554775, 'f1-score': 0.910210764040461, 'support': 1131} weighted_avg {'precision': 0.9112522612989282, 'recall': 0.9089301503094607, 'f1-score': 0.9089520643689094, 'support': 1131}
 
----------
Epoch 31/40
time = 195.43 secondes

Train loss 0.02697620562349089 accuracy 0.9939096570014954 macro_avg {'precision': 0.9941606787444174, 'recall': 0.9939575345472356, 'f1-score': 0.9940298067072126, 'support': 10180} weighted_avg {'precision': 0.9940071561586719, 'recall': 0.993909626719057, 'f1-score': 0.9939272551086329, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.8222356812791372 accuracy 0.9115827083587646 macro_avg {'precision': 0.9160843045590614, 'recall': 0.9119324960876088, 'f1-score': 0.9121119305086666, 'support': 1131} weighted_avg {'precision': 0.915563439441073, 'recall': 0.9115826702033598, 'f1-score': 0.911796129592111, 'support': 1131}
 
----------
Epoch 32/40
time = 195.96 secondes

Train loss 0.029129980768187437 accuracy 0.9929273724555969 macro_avg {'precision': 0.9928703821809656, 'recall': 0.9927121710567736, 'f1-score': 0.9927700374721832, 'support': 10180} weighted_avg {'precision': 0.9929920468121883, 'recall': 0.9929273084479371, 'f1-score': 0.9929372305638963, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.8174968643672119 accuracy 0.9106985330581665 macro_avg {'precision': 0.9129031574862753, 'recall': 0.9117930465022775, 'f1-score': 0.9110635085000265, 'support': 1131} weighted_avg {'precision': 0.9135136103047474, 'recall': 0.9106984969053935, 'f1-score': 0.9107805840856159, 'support': 1131}
 
----------
Epoch 33/40
time = 192.68 secondes

Train loss 0.02770141280306915 accuracy 0.9938114285469055 macro_avg {'precision': 0.9940315580646149, 'recall': 0.9938335605228371, 'f1-score': 0.9939075126496084, 'support': 10180} weighted_avg {'precision': 0.9938893298342777, 'recall': 0.993811394891945, 'f1-score': 0.9938236286370147, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.7457522937251603 accuracy 0.9124668836593628 macro_avg {'precision': 0.9144063401298178, 'recall': 0.912702804375028, 'f1-score': 0.9124613649407662, 'support': 1131} weighted_avg {'precision': 0.9152500612213923, 'recall': 0.9124668435013262, 'f1-score': 0.912748853015942, 'support': 1131}
 
----------
Epoch 34/40
time = 195.01 secondes

Train loss 0.024563685978476885 accuracy 0.9946955442428589 macro_avg {'precision': 0.9950231349662868, 'recall': 0.994809865714003, 'f1-score': 0.9948919998747426, 'support': 10180} weighted_avg {'precision': 0.9947796272704234, 'recall': 0.9946954813359529, 'f1-score': 0.9947113788791733, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.7858836118873921 accuracy 0.9160035848617554 macro_avg {'precision': 0.9186504649446393, 'recall': 0.9159561574831653, 'f1-score': 0.9161899122909466, 'support': 1131} weighted_avg {'precision': 0.9176972597978623, 'recall': 0.9160035366931919, 'f1-score': 0.9158427190243654, 'support': 1131}
 
----------
Epoch 35/40
time = 194.70 secondes

Train loss 0.02090043489826692 accuracy 0.9946955442428589 macro_avg {'precision': 0.9947336883433145, 'recall': 0.9947440880753267, 'f1-score': 0.9947183805801533, 'support': 10180} weighted_avg {'precision': 0.9947684861838525, 'recall': 0.9946954813359529, 'f1-score': 0.9947109146759727, 'support': 10180}
 
time = 5.01 secondes

Val loss 0.8536491581568735 accuracy 0.9124668836593628 macro_avg {'precision': 0.9129969897263536, 'recall': 0.9140499638505393, 'f1-score': 0.912092165560154, 'support': 1131} weighted_avg {'precision': 0.9147221905170758, 'recall': 0.9124668435013262, 'f1-score': 0.9120702955151293, 'support': 1131}
 
----------
Epoch 36/40
time = 191.55 secondes

Train loss 0.024550055782512837 accuracy 0.9945972561836243 macro_avg {'precision': 0.9948275862860452, 'recall': 0.9945292651801161, 'f1-score': 0.9946498371815015, 'support': 10180} weighted_avg {'precision': 0.9946911450380757, 'recall': 0.9945972495088409, 'f1-score': 0.9946139380543869, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.8102763119022711 accuracy 0.9106985330581665 macro_avg {'precision': 0.9118886801241738, 'recall': 0.9095604196229902, 'f1-score': 0.9094079933820194, 'support': 1131} weighted_avg {'precision': 0.9137769821982737, 'recall': 0.9106984969053935, 'f1-score': 0.9109702094684786, 'support': 1131}
 
----------
Epoch 37/40
time = 195.73 secondes

Train loss 0.01905034394742126 accuracy 0.9956778287887573 macro_avg {'precision': 0.995853908757144, 'recall': 0.9956474652593631, 'f1-score': 0.995726121243283, 'support': 10180} weighted_avg {'precision': 0.995765256886163, 'recall': 0.9956777996070727, 'f1-score': 0.9956955440720943, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.8438813475859462 accuracy 0.9142352342605591 macro_avg {'precision': 0.9161719007835172, 'recall': 0.9157885320772348, 'f1-score': 0.9149244039225419, 'support': 1131} weighted_avg {'precision': 0.9164317072695124, 'recall': 0.9142351900972591, 'f1-score': 0.9142661713071406, 'support': 1131}
 
----------
Epoch 38/40
time = 196.24 secondes

Train loss 0.014879338781320028 accuracy 0.9961689710617065 macro_avg {'precision': 0.9964036131137096, 'recall': 0.9961803349625686, 'f1-score': 0.9962762398395, 'support': 10180} weighted_avg {'precision': 0.9962243123821186, 'recall': 0.9961689587426326, 'f1-score': 0.9961798873798466, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.8165366725605335 accuracy 0.9106985330581665 macro_avg {'precision': 0.9125568508343468, 'recall': 0.9119120493628836, 'f1-score': 0.9115225958366768, 'support': 1131} weighted_avg {'precision': 0.9121017786901324, 'recall': 0.9106984969053935, 'f1-score': 0.9106996589173932, 'support': 1131}
 
----------
Epoch 39/40
time = 192.12 secondes

Train loss 0.014046352405980607 accuracy 0.996365487575531 macro_avg {'precision': 0.9966250075521016, 'recall': 0.9964147883021418, 'f1-score': 0.9964887827446178, 'support': 10180} weighted_avg {'precision': 0.9964839062381264, 'recall': 0.9963654223968565, 'f1-score': 0.9963913954656964, 'support': 10180}
 
time = 5.00 secondes

Val loss 0.8414974864437912 accuracy 0.9151194095611572 macro_avg {'precision': 0.9171445319106859, 'recall': 0.9156433515550816, 'f1-score': 0.9155348578068935, 'support': 1131} weighted_avg {'precision': 0.9171085660638577, 'recall': 0.9151193633952255, 'f1-score': 0.9152798427745614, 'support': 1131}
 
----------
Epoch 40/40
time = 194.58 secondes

Train loss 0.010539570034647117 accuracy 0.9969548583030701 macro_avg {'precision': 0.9971754232599199, 'recall': 0.9969835877873612, 'f1-score': 0.9970630033417486, 'support': 10180} weighted_avg {'precision': 0.9970218717108268, 'recall': 0.9969548133595285, 'f1-score': 0.9969707142219604, 'support': 10180}
 
time = 4.97 secondes

Val loss 0.8314545739007192 accuracy 0.9160035848617554 macro_avg {'precision': 0.9186907459036373, 'recall': 0.9168709044008401, 'f1-score': 0.9168723830663443, 'support': 1131} weighted_avg {'precision': 0.91809387619082, 'recall': 0.9160035366931919, 'f1-score': 0.9161193677905446, 'support': 1131}
 
----------
best_accuracy 0.9160035848617554 best_epoch 34 macro_avg {'precision': 0.9186504649446393, 'recall': 0.9159561574831653, 'f1-score': 0.9161899122909466, 'support': 1131} weighted_avg {'precision': 0.9176972597978623, 'recall': 0.9160035366931919, 'f1-score': 0.9158427190243654, 'support': 1131}

average train time 195.62301164269448

average val time 5.033631634712219
 
time = 34.90 secondes

test_accuracy 0.8407703042030334 macro_avg {'precision': 0.8402086997931161, 'recall': 0.8326064644935652, 'f1-score': 0.8343020345637411, 'support': 7530} weighted_avg {'precision': 0.8461325075663628, 'recall': 0.8407702523240372, 'f1-score': 0.8413733456522646, 'support': 7530}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_tail_2
----------
Epoch 1/40
time = 199.70 secondes

Train loss 1.3658077671913376 accuracy 0.6464636921882629 macro_avg {'precision': 0.6506878283796456, 'recall': 0.6312559108132024, 'f1-score': 0.6215088665515527, 'support': 10180} weighted_avg {'precision': 0.6572266308423661, 'recall': 0.6464636542239686, 'f1-score': 0.6355964467082085, 'support': 10180}
 
time = 5.17 secondes

Val loss 0.7445581227960721 accuracy 0.7789567112922668 macro_avg {'precision': 0.7531505200498022, 'recall': 0.7738284541599473, 'f1-score': 0.7575471745344083, 'support': 1131} weighted_avg {'precision': 0.7643180087911867, 'recall': 0.7789566755083996, 'f1-score': 0.7658238513324921, 'support': 1131}
 
----------
Epoch 2/40
time = 194.94 secondes

Train loss 0.5409444398897315 accuracy 0.8421415090560913 macro_avg {'precision': 0.8337179934562153, 'recall': 0.8309568292072754, 'f1-score': 0.8287933939661342, 'support': 10180} weighted_avg {'precision': 0.8401765576323059, 'recall': 0.8421414538310412, 'f1-score': 0.8386109259676074, 'support': 10180}
 
time = 4.99 secondes

Val loss 0.5679806550623665 accuracy 0.8426171541213989 macro_avg {'precision': 0.8463193503327178, 'recall': 0.840374148741844, 'f1-score': 0.8387919073080845, 'support': 1131} weighted_avg {'precision': 0.8538069386727943, 'recall': 0.8426171529619806, 'f1-score': 0.8436258131599175, 'support': 1131}
 
----------
Epoch 3/40
time = 191.68 secondes

Train loss 0.349517018499154 accuracy 0.8976424932479858 macro_avg {'precision': 0.8909280865970984, 'recall': 0.8903109692900804, 'f1-score': 0.8904354269654465, 'support': 10180} weighted_avg {'precision': 0.8975083556293162, 'recall': 0.8976424361493124, 'f1-score': 0.8974085201169489, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.5778500468693148 accuracy 0.86118483543396 macro_avg {'precision': 0.8629293842773335, 'recall': 0.8554910925164327, 'f1-score': 0.8567316946851035, 'support': 1131} weighted_avg {'precision': 0.8657918320046816, 'recall': 0.861184792219275, 'f1-score': 0.8609173901405573, 'support': 1131}
 
----------
Epoch 4/40
time = 195.49 secondes

Train loss 0.26098053408581967 accuracy 0.9278978705406189 macro_avg {'precision': 0.9236790841465312, 'recall': 0.9230673711091517, 'f1-score': 0.9232305965664052, 'support': 10180} weighted_avg {'precision': 0.9280256290360055, 'recall': 0.9278978388998035, 'f1-score': 0.9278219448826699, 'support': 10180}
 
time = 4.97 secondes

Val loss 0.6751514844412745 accuracy 0.8505747318267822 macro_avg {'precision': 0.8541845385951209, 'recall': 0.8435374290353655, 'f1-score': 0.8439368636569189, 'support': 1131} weighted_avg {'precision': 0.8571898477041765, 'recall': 0.8505747126436781, 'f1-score': 0.8489920423571141, 'support': 1131}
 
----------
Epoch 5/40
time = 195.53 secondes

Train loss 0.22359391959351918 accuracy 0.9423379302024841 macro_avg {'precision': 0.9388062102719784, 'recall': 0.9388157220410187, 'f1-score': 0.9387286817241373, 'support': 10180} weighted_avg {'precision': 0.9424027279234849, 'recall': 0.9423379174852652, 'f1-score': 0.9422869003666953, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.632386051990669 accuracy 0.8726790547370911 macro_avg {'precision': 0.8763695556139369, 'recall': 0.8738125777614556, 'f1-score': 0.8719876864053697, 'support': 1131} weighted_avg {'precision': 0.8799641526626083, 'recall': 0.8726790450928382, 'f1-score': 0.8731418013576008, 'support': 1131}
 
----------
Epoch 6/40
time = 190.55 secondes

Train loss 0.18989913592267096 accuracy 0.954616904258728 macro_avg {'precision': 0.9525990702388658, 'recall': 0.9528384588602312, 'f1-score': 0.9526524792067453, 'support': 10180} weighted_avg {'precision': 0.9548770416277147, 'recall': 0.9546168958742632, 'f1-score': 0.9546856846266225, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.6204290559964621 accuracy 0.8770999312400818 macro_avg {'precision': 0.8782805575600108, 'recall': 0.8739162123416909, 'f1-score': 0.8743515025720509, 'support': 1131} weighted_avg {'precision': 0.8824084786435491, 'recall': 0.8770999115826702, 'f1-score': 0.8779873069584139, 'support': 1131}
 
----------
Epoch 7/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 11.24 GiB already allocated; 119.31 MiB free; 11.40 GiB reserved in total by PyTorch)
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
ECtHR_BERT_head_bert_summarizer_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.81 GiB already allocated; 115.31 MiB free; 5.88 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_head_text_rank_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.81 GiB already allocated; 115.31 MiB free; 5.88 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_head_tail_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 117.31 MiB free; 5.88 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_tail_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.81 GiB already allocated; 117.31 MiB free; 5.88 GiB reserved in total by PyTorch)
516 516
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
Hyperpartisan_BERT_head_bert_summarizer_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.82 GiB already allocated; 149.31 MiB free; 5.85 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_head_text_rank_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.81 GiB already allocated; 109.31 MiB free; 5.89 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_head_tail_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.81 GiB already allocated; 109.31 MiB free; 5.89 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_tail_2
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 123.31 MiB free; 5.88 GiB reserved in total by PyTorch)
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (11). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
20newsgroups_BERT_head_bert_summarizer_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.82 GiB already allocated; 79.31 MiB free; 5.92 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_head_text_rank_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.81 GiB already allocated; 89.31 MiB free; 5.91 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_head_tail_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 89.31 MiB free; 5.91 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_tail_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 89.31 MiB free; 5.91 GiB reserved in total by PyTorch)
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
ECtHR_BERT_head_bert_summarizer_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.81 GiB already allocated; 9.31 MiB free; 5.99 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_head_text_rank_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 169.31 MiB free; 5.83 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_head_tail_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 169.31 MiB free; 5.83 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_tail_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 169.31 MiB free; 5.83 GiB reserved in total by PyTorch)
516 516
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
Hyperpartisan_BERT_head_bert_summarizer_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 145.31 MiB free; 5.85 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_head_text_rank_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 145.31 MiB free; 5.85 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_head_tail_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 121.31 MiB free; 5.88 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_tail_3
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 121.31 MiB free; 5.88 GiB reserved in total by PyTorch)
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (11). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
20newsgroups_BERT_head_bert_summarizer_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 93.31 MiB free; 5.90 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_head_text_rank_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 117.31 MiB free; 5.88 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_head_tail_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 69.31 MiB free; 5.93 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_tail_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 69.31 MiB free; 5.93 GiB reserved in total by PyTorch)
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
ECtHR_BERT_head_bert_summarizer_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 73.31 MiB free; 5.92 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_head_text_rank_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 73.31 MiB free; 5.92 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_head_tail_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 73.31 MiB free; 5.92 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_tail_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 73.31 MiB free; 5.92 GiB reserved in total by PyTorch)
516 516
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
Hyperpartisan_BERT_head_bert_summarizer_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 123.31 MiB free; 5.88 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_head_text_rank_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 115.31 MiB free; 5.88 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_head_tail_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 91.31 MiB free; 5.91 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_tail_4
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 91.31 MiB free; 5.91 GiB reserved in total by PyTorch)
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (11). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
20newsgroups_BERT_head_bert_summarizer_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 67.31 MiB free; 5.93 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_head_text_rank_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 99.31 MiB free; 5.90 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_head_tail_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 75.31 MiB free; 5.92 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_tail_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 75.31 MiB free; 5.92 GiB reserved in total by PyTorch)
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
ECtHR_BERT_head_bert_summarizer_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 79.31 MiB free; 5.92 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_head_text_rank_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 79.31 MiB free; 5.92 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_head_tail_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 79.31 MiB free; 5.92 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
ECtHR_BERT_tail_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 79.31 MiB free; 5.92 GiB reserved in total by PyTorch)
516 516
datasets imported
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
Hyperpartisan_BERT_head_bert_summarizer_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.63 GiB already allocated; 99.31 MiB free; 5.90 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_head_text_rank_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 105.31 MiB free; 5.89 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_head_tail_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 105.31 MiB free; 5.89 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
Hyperpartisan_BERT_tail_5
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 79.20 GiB total capacity; 5.62 GiB already allocated; 105.31 MiB free; 5.89 GiB reserved in total by PyTorch)
