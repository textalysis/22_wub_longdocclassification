[nltk_data] Downloading package punkt to /vol/fob-
[nltk_data]     vol3/nebenf20/wubingti/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
datasets imported
From: filipows@spk.hp.com (Dennis Filipowski)
Subject: ? Octopus
Organization: Hewlett-Packard
X-Newsreader: TIN [version 1.1.4 PL6]
Lines: 4

   During the Detroit game Mon night there were octopus thrown on
   the ice what is the meaning or symbolism here?  They used to
   throw fish on the ice here in Spokane afew years ago. I never 
   knew where this came from.

Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (11). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (12). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
20newsgroups_BERT_head_bert_summarizer_1
----------
Epoch 1/40
time = 264.11 secondes

Train loss 1.2754744588860911 accuracy 0.6783890128135681 macro_avg {'precision': 0.701831888648878, 'recall': 0.6631310265176371, 'f1-score': 0.6556714988901758, 'support': 10180} weighted_avg {'precision': 0.7021710379191725, 'recall': 0.6783889980353635, 'f1-score': 0.6694774040631106, 'support': 10180}
 
time = 6.64 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss 0.6540595223786125 accuracy 0.8196286559104919 macro_avg {'precision': 0.7949882564702239, 'recall': 0.8123239378445957, 'f1-score': 0.7975679977204734, 'support': 1131} weighted_avg {'precision': 0.8063637400211182, 'recall': 0.8196286472148541, 'f1-score': 0.8070193178037786, 'support': 1131}
 
----------
Epoch 2/40
time = 257.87 secondes

Train loss 0.4649639995812528 accuracy 0.8643418550491333 macro_avg {'precision': 0.8558183058680966, 'recall': 0.8533910854778937, 'f1-score': 0.8521519436788239, 'support': 10180} weighted_avg {'precision': 0.8625143041698158, 'recall': 0.8643418467583497, 'f1-score': 0.8615607348951108, 'support': 10180}
 
time = 6.40 secondes

Val loss 0.4910782116504622 accuracy 0.8673740029335022 macro_avg {'precision': 0.8638307981720242, 'recall': 0.8626439046827914, 'f1-score': 0.8596931454302978, 'support': 1131} weighted_avg {'precision': 0.871308687893527, 'recall': 0.8673740053050398, 'f1-score': 0.865944120793141, 'support': 1131}
 
----------
Epoch 3/40
time = 259.28 secondes

Train loss 0.28218848636341715 accuracy 0.9212180972099304 macro_avg {'precision': 0.916672368081802, 'recall': 0.9153017667749884, 'f1-score': 0.9156778795223903, 'support': 10180} weighted_avg {'precision': 0.9211728054230192, 'recall': 0.9212180746561887, 'f1-score': 0.9209356895743103, 'support': 10180}
 
time = 6.42 secondes

Val loss 0.5317556057387675 accuracy 0.8691423535346985 macro_avg {'precision': 0.8763683054713514, 'recall': 0.8701091711957861, 'f1-score': 0.8685482331086772, 'support': 1131} weighted_avg {'precision': 0.8779667557960947, 'recall': 0.8691423519009726, 'f1-score': 0.8681341286629264, 'support': 1131}
 
----------
Epoch 4/40
time = 257.38 secondes

Train loss 0.21210408750431703 accuracy 0.9425343871116638 macro_avg {'precision': 0.9395207154244976, 'recall': 0.9386983107232787, 'f1-score': 0.9389846515422547, 'support': 10180} weighted_avg {'precision': 0.9423959910309652, 'recall': 0.9425343811394892, 'f1-score': 0.9423535657893622, 'support': 10180}
 
time = 6.54 secondes

Val loss 0.4916821052943369 accuracy 0.8921308517456055 macro_avg {'precision': 0.8948644783213154, 'recall': 0.8882307149408559, 'f1-score': 0.8883902336313989, 'support': 1131} weighted_avg {'precision': 0.8972361388669531, 'recall': 0.8921308576480991, 'f1-score': 0.8918426776225501, 'support': 1131}
 
----------
Epoch 5/40
time = 253.84 secondes

Train loss 0.1699967440232354 accuracy 0.9573674201965332 macro_avg {'precision': 0.9554035625732006, 'recall': 0.9552763949195278, 'f1-score': 0.9552710258300829, 'support': 10180} weighted_avg {'precision': 0.9575352666898409, 'recall': 0.9573673870333989, 'f1-score': 0.9573815884466447, 'support': 10180}
 
time = 6.53 secondes

Val loss 0.7318680806374046 accuracy 0.8656056523323059 macro_avg {'precision': 0.879168753297295, 'recall': 0.872606243777974, 'f1-score': 0.8684757790153016, 'support': 1131} weighted_avg {'precision': 0.8880865381175838, 'recall': 0.865605658709107, 'f1-score': 0.869605571564478, 'support': 1131}
 
----------
Epoch 6/40
time = 253.07 secondes

Train loss 0.16014801397043524 accuracy 0.9621807932853699 macro_avg {'precision': 0.9613539716191483, 'recall': 0.9611087609395129, 'f1-score': 0.9611608237472611, 'support': 10180} weighted_avg {'precision': 0.9622626140180315, 'recall': 0.962180746561886, 'f1-score': 0.9621494902354872, 'support': 10180}
 
time = 6.68 secondes

Val loss 0.5683415353865507 accuracy 0.8894783854484558 macro_avg {'precision': 0.8930818464253013, 'recall': 0.8904117992518883, 'f1-score': 0.8894887533745894, 'support': 1131} weighted_avg {'precision': 0.8949500811005436, 'recall': 0.8894783377541998, 'f1-score': 0.8900980453465683, 'support': 1131}
 
----------
Epoch 7/40
time = 256.71 secondes

Train loss 0.14736697123385234 accuracy 0.965618908405304 macro_avg {'precision': 0.9643803853974733, 'recall': 0.9642651035727863, 'f1-score': 0.9642807683068153, 'support': 10180} weighted_avg {'precision': 0.9657680449431586, 'recall': 0.9656188605108055, 'f1-score': 0.9656501485948229, 'support': 10180}
 
time = 6.36 secondes

Val loss 0.6968451252963039 accuracy 0.8921308517456055 macro_avg {'precision': 0.9063356426085759, 'recall': 0.8937872470991044, 'f1-score': 0.8941785159595023, 'support': 1131} weighted_avg {'precision': 0.9050582714174104, 'recall': 0.8921308576480991, 'f1-score': 0.8924090392865673, 'support': 1131}
 
----------
Epoch 8/40
time = 259.14 secondes

Train loss 0.149434083597179 accuracy 0.9678782224655151 macro_avg {'precision': 0.966407037534658, 'recall': 0.9662779013196555, 'f1-score': 0.9662543204348495, 'support': 10180} weighted_avg {'precision': 0.9680710107746947, 'recall': 0.9678781925343811, 'f1-score': 0.9678924182298309, 'support': 10180}
 
time = 6.34 secondes

Val loss 0.690487482419408 accuracy 0.8912467360496521 macro_avg {'precision': 0.8935376720260338, 'recall': 0.8917592254472702, 'f1-score': 0.8893270631436223, 'support': 1131} weighted_avg {'precision': 0.8967139918551417, 'recall': 0.8912466843501327, 'f1-score': 0.890649100548282, 'support': 1131}
 
----------
Epoch 9/40
time = 257.70 secondes

Train loss 0.1305105469352328 accuracy 0.9694499373435974 macro_avg {'precision': 0.9684114293144042, 'recall': 0.9683969323748114, 'f1-score': 0.9683363516156712, 'support': 10180} weighted_avg {'precision': 0.9696923620931471, 'recall': 0.9694499017681729, 'f1-score': 0.9695074270832026, 'support': 10180}
 
time = 6.40 secondes

Val loss 0.6587127394048647 accuracy 0.8983200788497925 macro_avg {'precision': 0.9027547626076584, 'recall': 0.8973897386957432, 'f1-score': 0.897430440845185, 'support': 1131} weighted_avg {'precision': 0.9026821868605123, 'recall': 0.8983200707338639, 'f1-score': 0.8980069402790262, 'support': 1131}
 
----------
Epoch 10/40
time = 260.14 secondes

Train loss 0.12203179339174104 accuracy 0.9747544527053833 macro_avg {'precision': 0.9741849789466108, 'recall': 0.9742464078369247, 'f1-score': 0.9741379815263496, 'support': 10180} weighted_avg {'precision': 0.9748566625632311, 'recall': 0.97475442043222, 'f1-score': 0.974729239817267, 'support': 10180}
 
time = 6.46 secondes

Val loss 0.6077434339685182 accuracy 0.9053934812545776 macro_avg {'precision': 0.9059668006678369, 'recall': 0.9057159120374572, 'f1-score': 0.9053275117016077, 'support': 1131} weighted_avg {'precision': 0.905638077277579, 'recall': 0.905393457117595, 'f1-score': 0.905009388790889, 'support': 1131}
 
----------
Epoch 11/40
time = 261.14 secondes

Train loss 0.11160660652471911 accuracy 0.9771119952201843 macro_avg {'precision': 0.9758678485958214, 'recall': 0.9755373107683823, 'f1-score': 0.9756665161411693, 'support': 10180} weighted_avg {'precision': 0.9771658589687906, 'recall': 0.9771119842829077, 'f1-score': 0.9771017497937003, 'support': 10180}
 
time = 6.73 secondes

Val loss 0.8079591478400661 accuracy 0.8850575089454651 macro_avg {'precision': 0.89045258794388, 'recall': 0.8816807144210552, 'f1-score': 0.8838152781676787, 'support': 1131} weighted_avg {'precision': 0.888387720266583, 'recall': 0.8850574712643678, 'f1-score': 0.8847206569580652, 'support': 1131}
 
----------
Epoch 12/40
time = 260.87 secondes

Train loss 0.12632377943055748 accuracy 0.9751473665237427 macro_avg {'precision': 0.9745381440421189, 'recall': 0.974233942298171, 'f1-score': 0.974341916805068, 'support': 10180} weighted_avg {'precision': 0.9752101804693061, 'recall': 0.975147347740668, 'f1-score': 0.9751355613847025, 'support': 10180}
 
time = 6.46 secondes

Val loss 0.931098633782543 accuracy 0.8815208077430725 macro_avg {'precision': 0.8970737576636652, 'recall': 0.8838306399262079, 'f1-score': 0.8855271950866992, 'support': 1131} weighted_avg {'precision': 0.8945029412498278, 'recall': 0.8815207780725022, 'f1-score': 0.8828067038302735, 'support': 1131}
 
----------
Epoch 13/40
time = 257.29 secondes

Train loss 0.12037763775834738 accuracy 0.9781925678253174 macro_avg {'precision': 0.9777754007727875, 'recall': 0.9778157440297324, 'f1-score': 0.9777460254093409, 'support': 10180} weighted_avg {'precision': 0.9783010497579141, 'recall': 0.9781925343811395, 'f1-score': 0.978199843562036, 'support': 10180}
 
time = 7.81 secondes

Val loss 0.7450073800018159 accuracy 0.8983200788497925 macro_avg {'precision': 0.9031276071646765, 'recall': 0.8997059987468212, 'f1-score': 0.8992933579348271, 'support': 1131} weighted_avg {'precision': 0.9030972305887893, 'recall': 0.8983200707338639, 'f1-score': 0.8987001115748277, 'support': 1131}
 
----------
Epoch 14/40
time = 253.58 secondes

Train loss 0.10893101184164476 accuracy 0.9792731404304504 macro_avg {'precision': 0.9790850147760404, 'recall': 0.9788636696428803, 'f1-score': 0.9789395029663099, 'support': 10180} weighted_avg {'precision': 0.9793480193179145, 'recall': 0.9792730844793713, 'f1-score': 0.9792736544676294, 'support': 10180}
 
time = 6.51 secondes

Val loss 0.7789769347461554 accuracy 0.8930150270462036 macro_avg {'precision': 0.8949167840628757, 'recall': 0.8927886650873067, 'f1-score': 0.8929805169151314, 'support': 1131} weighted_avg {'precision': 0.8958347116712647, 'recall': 0.8930150309460654, 'f1-score': 0.8935877235575865, 'support': 1131}
 
----------
Epoch 15/40
time = 262.29 secondes

Train loss 0.1039695132742762 accuracy 0.9785854816436768 macro_avg {'precision': 0.9775905443922571, 'recall': 0.9771738390935745, 'f1-score': 0.9773274070419842, 'support': 10180} weighted_avg {'precision': 0.9786166067701758, 'recall': 0.9785854616895874, 'f1-score': 0.9785522414127941, 'support': 10180}
 
time = 7.02 secondes

Val loss 0.7463385245732432 accuracy 0.8894783854484558 macro_avg {'precision': 0.8925916386870073, 'recall': 0.8893862030398137, 'f1-score': 0.8890732073971339, 'support': 1131} weighted_avg {'precision': 0.8953705992520125, 'recall': 0.8894783377541998, 'f1-score': 0.890483427827065, 'support': 1131}
 
----------
Epoch 16/40
time = 260.89 secondes

Train loss 0.09988137630955389 accuracy 0.9818271398544312 macro_avg {'precision': 0.9812619160272096, 'recall': 0.9805326417034317, 'f1-score': 0.9808469918242386, 'support': 10180} weighted_avg {'precision': 0.981887306036467, 'recall': 0.981827111984283, 'f1-score': 0.9818159544252804, 'support': 10180}
 
time = 6.53 secondes

Val loss 0.8152506748258634 accuracy 0.9027409553527832 macro_avg {'precision': 0.9053181558290925, 'recall': 0.9047501594067157, 'f1-score': 0.9026635673762324, 'support': 1131} weighted_avg {'precision': 0.9077908002751652, 'recall': 0.9027409372236959, 'f1-score': 0.9027471000730999, 'support': 1131}
 
----------
Epoch 17/40
time = 267.79 secondes

Train loss 0.0886922204541746 accuracy 0.9833988547325134 macro_avg {'precision': 0.9833530276776065, 'recall': 0.9832863350954402, 'f1-score': 0.9833001621057201, 'support': 10180} weighted_avg {'precision': 0.983478715885643, 'recall': 0.9833988212180746, 'f1-score': 0.9834188809711347, 'support': 10180}
 
time = 6.55 secondes

Val loss 0.9432713099723373 accuracy 0.8629531860351562 macro_avg {'precision': 0.8925231818827033, 'recall': 0.8708255109893142, 'f1-score': 0.8651558557737312, 'support': 1131} weighted_avg {'precision': 0.8911623871890729, 'recall': 0.8629531388152077, 'f1-score': 0.858229461132215, 'support': 1131}
 
----------
Epoch 18/40
time = 268.64 secondes

Train loss 0.08340722320919704 accuracy 0.9829077124595642 macro_avg {'precision': 0.9828359763840873, 'recall': 0.9827592498019817, 'f1-score': 0.9827678107485369, 'support': 10180} weighted_avg {'precision': 0.9829458406047644, 'recall': 0.9829076620825148, 'f1-score': 0.9828955247546344, 'support': 10180}
 
time = 6.51 secondes

Val loss 0.7524104989094833 accuracy 0.9009726047515869 macro_avg {'precision': 0.9062670231700627, 'recall': 0.902652014316344, 'f1-score': 0.9029042550174194, 'support': 1131} weighted_avg {'precision': 0.908235142433074, 'recall': 0.900972590627763, 'f1-score': 0.9031068418532432, 'support': 1131}
 
----------
Epoch 19/40
time = 263.60 secondes

Train loss 0.07779163807117727 accuracy 0.9850687980651855 macro_avg {'precision': 0.9852810570702337, 'recall': 0.9852136161175589, 'f1-score': 0.985234775592328, 'support': 10180} weighted_avg {'precision': 0.9850836687980802, 'recall': 0.9850687622789784, 'f1-score': 0.9850630670903067, 'support': 10180}
 
time = 6.59 secondes

Val loss 0.7767728598312676 accuracy 0.9009726047515869 macro_avg {'precision': 0.9071257440423356, 'recall': 0.9027701594776321, 'f1-score': 0.9019082942159622, 'support': 1131} weighted_avg {'precision': 0.907619920272567, 'recall': 0.900972590627763, 'f1-score': 0.9012437294872699, 'support': 1131}
 
----------
Epoch 20/40
time = 267.40 secondes

Train loss 0.09211054434319745 accuracy 0.9833988547325134 macro_avg {'precision': 0.9829935806955795, 'recall': 0.9827889344432448, 'f1-score': 0.9828622685112158, 'support': 10180} weighted_avg {'precision': 0.9834683570672319, 'recall': 0.9833988212180746, 'f1-score': 0.9834041303593538, 'support': 10180}
 
time = 6.52 secondes

Val loss 0.9471402184580269 accuracy 0.8877100348472595 macro_avg {'precision': 0.9036951114490577, 'recall': 0.8892797593250327, 'f1-score': 0.8920724560364528, 'support': 1131} weighted_avg {'precision': 0.9013062299579369, 'recall': 0.887709991158267, 'f1-score': 0.8897129095969999, 'support': 1131}
 
----------
Epoch 21/40
time = 267.58 secondes

Train loss 0.08230172854989065 accuracy 0.9855599403381348 macro_avg {'precision': 0.9853529726921477, 'recall': 0.9847886783824255, 'f1-score': 0.9850168000144036, 'support': 10180} weighted_avg {'precision': 0.985656927796175, 'recall': 0.9855599214145383, 'f1-score': 0.9855614404358672, 'support': 10180}
 
time = 6.55 secondes

Val loss 1.019574353450083 accuracy 0.8806366324424744 macro_avg {'precision': 0.8892261595337432, 'recall': 0.8877893183665175, 'f1-score': 0.8825889632998815, 'support': 1131} weighted_avg {'precision': 0.8940175626876322, 'recall': 0.8806366047745358, 'f1-score': 0.8812119119993267, 'support': 1131}
 
----------
Epoch 22/40
time = 266.02 secondes

Train loss 0.06578444373345052 accuracy 0.9886051416397095 macro_avg {'precision': 0.9885225606992091, 'recall': 0.9883520781995255, 'f1-score': 0.9884163852974203, 'support': 10180} weighted_avg {'precision': 0.9886529886639435, 'recall': 0.9886051080550098, 'f1-score': 0.9886079658618405, 'support': 10180}
 
time = 6.63 secondes

Val loss 0.9699526440645699 accuracy 0.8770999312400818 macro_avg {'precision': 0.885809689469802, 'recall': 0.8811506978038862, 'f1-score': 0.8790562477744459, 'support': 1131} weighted_avg {'precision': 0.8892440962344761, 'recall': 0.8770999115826702, 'f1-score': 0.8785500672995814, 'support': 1131}
 
----------
Epoch 23/40
time = 287.58 secondes

Train loss 0.06905906675108402 accuracy 0.9882122278213501 macro_avg {'precision': 0.9882703538466666, 'recall': 0.9878282144715602, 'f1-score': 0.9880174015394866, 'support': 10180} weighted_avg {'precision': 0.988279082046684, 'recall': 0.9882121807465619, 'f1-score': 0.9882139098080215, 'support': 10180}
 
time = 6.66 secondes

Val loss 0.7738221737497337 accuracy 0.9018567800521851 macro_avg {'precision': 0.9061321033835062, 'recall': 0.9021250376474766, 'f1-score': 0.9020173358755507, 'support': 1131} weighted_avg {'precision': 0.9073615529607723, 'recall': 0.9018567639257294, 'f1-score': 0.9025338808742872, 'support': 1131}
 
----------
Epoch 24/40
time = 266.34 secondes

Train loss 0.057717197855387696 accuracy 0.9887033700942993 macro_avg {'precision': 0.9884049368444655, 'recall': 0.988468787191073, 'f1-score': 0.9884267185417895, 'support': 10180} weighted_avg {'precision': 0.9887345992854603, 'recall': 0.9887033398821218, 'f1-score': 0.9887088195073683, 'support': 10180}
 
time = 7.01 secondes

Val loss 0.7822903664934311 accuracy 0.9098143577575684 macro_avg {'precision': 0.9088361470325133, 'recall': 0.9096673959831382, 'f1-score': 0.9083583989826248, 'support': 1131} weighted_avg {'precision': 0.9109906919673354, 'recall': 0.9098143236074271, 'f1-score': 0.9095932691179359, 'support': 1131}
 
----------
Epoch 25/40
time = 259.94 secondes

Train loss 0.05219531873503047 accuracy 0.9897839426994324 macro_avg {'precision': 0.9895297385685862, 'recall': 0.9894011442199225, 'f1-score': 0.9894540586456971, 'support': 10180} weighted_avg {'precision': 0.9898196453079874, 'recall': 0.9897838899803536, 'f1-score': 0.9897905370220123, 'support': 10180}
 
time = 6.65 secondes

Val loss 0.8607579740879808 accuracy 0.9000884294509888 macro_avg {'precision': 0.904187043489561, 'recall': 0.9015147059042737, 'f1-score': 0.898864201270627, 'support': 1131} weighted_avg {'precision': 0.9111607564842067, 'recall': 0.9000884173297966, 'f1-score': 0.9024275629772084, 'support': 1131}
 
----------
Epoch 26/40
time = 272.39 secondes

Train loss 0.05470014235608653 accuracy 0.9901768565177917 macro_avg {'precision': 0.9901064327313437, 'recall': 0.9898311876640238, 'f1-score': 0.9899394403749197, 'support': 10180} weighted_avg {'precision': 0.990222380565333, 'recall': 0.9901768172888016, 'f1-score': 0.9901725769910653, 'support': 10180}
 
time = 6.52 secondes

Val loss 0.7342938976459702 accuracy 0.9071618318557739 macro_avg {'precision': 0.9140264598453436, 'recall': 0.9089259672909172, 'f1-score': 0.9082717527159719, 'support': 1131} weighted_avg {'precision': 0.9133669422687386, 'recall': 0.9071618037135278, 'f1-score': 0.9069003833849142, 'support': 1131}
 
----------
Epoch 27/40
time = 264.21 secondes

Train loss 0.03424232543807729 accuracy 0.9924361705780029 macro_avg {'precision': 0.9925458932285359, 'recall': 0.9923988265726462, 'f1-score': 0.9924621334963737, 'support': 10180} weighted_avg {'precision': 0.9924603191387785, 'recall': 0.9924361493123772, 'f1-score': 0.9924383884596892, 'support': 10180}
 
time = 6.91 secondes

Val loss 0.9124069905736972 accuracy 0.8983200788497925 macro_avg {'precision': 0.9058407441129136, 'recall': 0.9004744557423466, 'f1-score': 0.9004246044233832, 'support': 1131} weighted_avg {'precision': 0.9052085765572453, 'recall': 0.8983200707338639, 'f1-score': 0.8988943672658435, 'support': 1131}
 
----------
Epoch 28/40
time = 264.07 secondes

Train loss 0.04638121871697822 accuracy 0.9909626841545105 macro_avg {'precision': 0.9911999303815036, 'recall': 0.9908896737399215, 'f1-score': 0.9910300915019304, 'support': 10180} weighted_avg {'precision': 0.9909958675470699, 'recall': 0.9909626719056974, 'f1-score': 0.9909641016803119, 'support': 10180}
 
time = 6.51 secondes

Val loss 0.772117155141375 accuracy 0.9027409553527832 macro_avg {'precision': 0.9043704735060476, 'recall': 0.904551688076895, 'f1-score': 0.9028955258287171, 'support': 1131} weighted_avg {'precision': 0.9063909168521148, 'recall': 0.9027409372236959, 'f1-score': 0.9028494046745401, 'support': 1131}
 
----------
Epoch 29/40
time = 266.69 secondes

Train loss 0.03580565280650288 accuracy 0.9932220578193665 macro_avg {'precision': 0.9932058683609875, 'recall': 0.993019832646544, 'f1-score': 0.9930990143870229, 'support': 10180} weighted_avg {'precision': 0.9932654478115206, 'recall': 0.9932220039292731, 'f1-score': 0.9932291933455317, 'support': 10180}
 
time = 6.23 secondes

Val loss 0.811938214811421 accuracy 0.9036251306533813 macro_avg {'precision': 0.9038415868763193, 'recall': 0.9058537698212266, 'f1-score': 0.9039953855827036, 'support': 1131} weighted_avg {'precision': 0.9066226231895609, 'recall': 0.9036251105216623, 'f1-score': 0.9042850952379607, 'support': 1131}
 
----------
Epoch 30/40
time = 269.07 secondes

Train loss 0.04750907334692645 accuracy 0.9915521144866943 macro_avg {'precision': 0.9915411912254483, 'recall': 0.9912795661122363, 'f1-score': 0.9913806664719175, 'support': 10180} weighted_avg {'precision': 0.9916318560692058, 'recall': 0.9915520628683694, 'f1-score': 0.991561777734496, 'support': 10180}
 
time = 6.52 secondes

Val loss 0.7963354651968677 accuracy 0.9115827083587646 macro_avg {'precision': 0.9094994159908676, 'recall': 0.91188145488087, 'f1-score': 0.9093453591019094, 'support': 1131} weighted_avg {'precision': 0.9136860700034635, 'recall': 0.9115826702033598, 'f1-score': 0.9113197330832334, 'support': 1131}
 
----------
Epoch 31/40
time = 269.73 secondes

Train loss 0.04391564947850505 accuracy 0.9928290843963623 macro_avg {'precision': 0.9930058709377917, 'recall': 0.9928403960147577, 'f1-score': 0.9929019022044047, 'support': 10180} weighted_avg {'precision': 0.9928917429124163, 'recall': 0.9928290766208252, 'f1-score': 0.9928382599342858, 'support': 10180}
 
time = 6.50 secondes

Val loss 0.8067651749122273 accuracy 0.9045093059539795 macro_avg {'precision': 0.9070754974579671, 'recall': 0.9057546438039361, 'f1-score': 0.90483126410968, 'support': 1131} weighted_avg {'precision': 0.9100887440992637, 'recall': 0.9045092838196287, 'f1-score': 0.9056229907102371, 'support': 1131}
 
----------
Epoch 32/40
time = 264.70 secondes

Train loss 0.04151187469106049 accuracy 0.9922397136688232 macro_avg {'precision': 0.9925599118827954, 'recall': 0.9923397571179894, 'f1-score': 0.9924343538691043, 'support': 10180} weighted_avg {'precision': 0.992282922200766, 'recall': 0.9922396856581532, 'f1-score': 0.9922450899029838, 'support': 10180}
 
time = 6.70 secondes

Val loss 0.9184843371082716 accuracy 0.8947833776473999 macro_avg {'precision': 0.9048945267082728, 'recall': 0.8962988732727704, 'f1-score': 0.8974625323171976, 'support': 1131} weighted_avg {'precision': 0.9040308981988416, 'recall': 0.8947833775419982, 'f1-score': 0.8960754993543906, 'support': 1131}
 
----------
Epoch 33/40
time = 262.65 secondes

Train loss 0.035072007098670054 accuracy 0.9943025708198547 macro_avg {'precision': 0.9944256244484881, 'recall': 0.9941652782444084, 'f1-score': 0.9942729694366189, 'support': 10180} weighted_avg {'precision': 0.9943683453539209, 'recall': 0.9943025540275049, 'f1-score': 0.9943122255335798, 'support': 10180}
 
time = 6.62 secondes

Val loss 0.8176990939211695 accuracy 0.9098143577575684 macro_avg {'precision': 0.9104904384322353, 'recall': 0.9107494905391793, 'f1-score': 0.9091169127657295, 'support': 1131} weighted_avg {'precision': 0.9125833962041228, 'recall': 0.9098143236074271, 'f1-score': 0.9097516773988978, 'support': 1131}
 
----------
Epoch 34/40
time = 267.57 secondes

Train loss 0.030135209365323466 accuracy 0.9939096570014954 macro_avg {'precision': 0.9940923802726296, 'recall': 0.9939381576633239, 'f1-score': 0.9939997181032567, 'support': 10180} weighted_avg {'precision': 0.9939585979508212, 'recall': 0.993909626719057, 'f1-score': 0.9939175867013097, 'support': 10180}
 
time = 6.54 secondes

Val loss 0.8748405659075296 accuracy 0.9000884294509888 macro_avg {'precision': 0.9033410279097056, 'recall': 0.9028714954424615, 'f1-score': 0.9014233588124361, 'support': 1131} weighted_avg {'precision': 0.905857965743779, 'recall': 0.9000884173297966, 'f1-score': 0.9012419138425413, 'support': 1131}
 
----------
Epoch 35/40
time = 267.24 secondes

Train loss 0.029428113425719386 accuracy 0.9946955442428589 macro_avg {'precision': 0.9947897343313878, 'recall': 0.9945127052292531, 'f1-score': 0.9946338281004017, 'support': 10180} weighted_avg {'precision': 0.9947456346677007, 'recall': 0.9946954813359529, 'f1-score': 0.9947024709285917, 'support': 10180}
 
time = 6.55 secondes

Val loss 0.7662999795629987 accuracy 0.9115827083587646 macro_avg {'precision': 0.9123744334433528, 'recall': 0.9128225611571633, 'f1-score': 0.9121297855356882, 'support': 1131} weighted_avg {'precision': 0.913229661592086, 'recall': 0.9115826702033598, 'f1-score': 0.9119031556986644, 'support': 1131}
 
----------
Epoch 36/40
time = 264.41 secondes

Train loss 0.017839458019596043 accuracy 0.9959725141525269 macro_avg {'precision': 0.9961238303916831, 'recall': 0.9959211773506402, 'f1-score': 0.9960066132830429, 'support': 10180} weighted_avg {'precision': 0.9960219284554163, 'recall': 0.9959724950884087, 'f1-score': 0.9959806287311922, 'support': 10180}
 
time = 5.29 secondes

Val loss 0.8724513886842596 accuracy 0.9036251306533813 macro_avg {'precision': 0.9046747591319019, 'recall': 0.9049792512897785, 'f1-score': 0.9033386601830518, 'support': 1131} weighted_avg {'precision': 0.908106563383422, 'recall': 0.9036251105216623, 'f1-score': 0.9043520528260459, 'support': 1131}
 
----------
Epoch 37/40
time = 259.69 secondes

Train loss 0.01743872369148831 accuracy 0.995874285697937 macro_avg {'precision': 0.9961045831193773, 'recall': 0.9959045097855658, 'f1-score': 0.9959899367035229, 'support': 10180} weighted_avg {'precision': 0.995927010813156, 'recall': 0.9958742632612967, 'f1-score': 0.99588510163527, 'support': 10180}
 
time = 6.52 secondes

Val loss 0.889854495900655 accuracy 0.9071618318557739 macro_avg {'precision': 0.909658690263347, 'recall': 0.9077122237214088, 'f1-score': 0.9067808222266305, 'support': 1131} weighted_avg {'precision': 0.912376359132009, 'recall': 0.9071618037135278, 'f1-score': 0.907875706092507, 'support': 1131}
 
----------
Epoch 38/40
time = 267.75 secondes

Train loss 0.015314323553605132 accuracy 0.996365487575531 macro_avg {'precision': 0.9965381763564306, 'recall': 0.99632114591566, 'f1-score': 0.9964153730151868, 'support': 10180} weighted_avg {'precision': 0.9964143827045193, 'recall': 0.9963654223968565, 'f1-score': 0.9963748308734635, 'support': 10180}
 
time = 6.53 secondes

Val loss 0.7844555207126357 accuracy 0.9142352342605591 macro_avg {'precision': 0.9146000711846153, 'recall': 0.9144171573352923, 'f1-score': 0.9132359739461533, 'support': 1131} weighted_avg {'precision': 0.9168835850408318, 'recall': 0.9142351900972591, 'f1-score': 0.9143322645236514, 'support': 1131}
 
----------
Epoch 39/40
time = 268.44 secondes

Train loss 0.010156716713642353 accuracy 0.9974460005760193 macro_avg {'precision': 0.9975799673445099, 'recall': 0.9974546492263441, 'f1-score': 0.9975037535640962, 'support': 10180} weighted_avg {'precision': 0.997495305655022, 'recall': 0.9974459724950884, 'f1-score': 0.997456152391225, 'support': 10180}
 
time = 6.55 secondes

Val loss 0.8065748114480248 accuracy 0.9160035848617554 macro_avg {'precision': 0.9165889151337947, 'recall': 0.9163583366606041, 'f1-score': 0.9153374353980087, 'support': 1131} weighted_avg {'precision': 0.9191001009011879, 'recall': 0.9160035366931919, 'f1-score': 0.9164064599415629, 'support': 1131}
 
----------
Epoch 40/40
time = 265.00 secondes

Train loss 0.008151907847998956 accuracy 0.9975442290306091 macro_avg {'precision': 0.9977139055125337, 'recall': 0.9975152884782315, 'f1-score': 0.9976008759907776, 'support': 10180} weighted_avg {'precision': 0.9975922377828589, 'recall': 0.9975442043222004, 'f1-score': 0.9975537233529747, 'support': 10180}
 
time = 6.86 secondes

Val loss 0.804712722515881 accuracy 0.9151194095611572 macro_avg {'precision': 0.9153822919878583, 'recall': 0.915580352739763, 'f1-score': 0.9144660783299431, 'support': 1131} weighted_avg {'precision': 0.9178939280205993, 'recall': 0.9151193633952255, 'f1-score': 0.9154583443944836, 'support': 1131}
 
----------
best_accuracy 0.9160035848617554 best_epoch 39 macro_avg {'precision': 0.9165889151337947, 'recall': 0.9163583366606041, 'f1-score': 0.9153374353980087, 'support': 1131} weighted_avg {'precision': 0.9191001009011879, 'recall': 0.9160035366931919, 'f1-score': 0.9164064599415629, 'support': 1131}

average train time 263.59429161548616

average val time 6.571397632360458
 
time = 45.89 secondes

test_accuracy 0.845418393611908 macro_avg {'precision': 0.8405713605197402, 'recall': 0.8373682971594523, 'f1-score': 0.8380341431731265, 'support': 7530} weighted_avg {'precision': 0.8478652957999896, 'recall': 0.8454183266932271, 'f1-score': 0.8457130807898442, 'support': 7530}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_head_text_rank_1
----------
Epoch 1/40
time = 254.61 secondes

Train loss 1.2901145130676601 accuracy 0.6720039248466492 macro_avg {'precision': 0.7004832424076479, 'recall': 0.6552844427950648, 'f1-score': 0.6485620025943819, 'support': 10180} weighted_avg {'precision': 0.7014853522100034, 'recall': 0.6720039292730845, 'f1-score': 0.6635122176931935, 'support': 10180}
 
time = 6.49 secondes

Val loss 0.6584891693692811 accuracy 0.8019452095031738 macro_avg {'precision': 0.7849212335179093, 'recall': 0.7945690512097936, 'f1-score': 0.7831104030145923, 'support': 1131} weighted_avg {'precision': 0.7904660566670981, 'recall': 0.801945181255526, 'f1-score': 0.7899104219244996, 'support': 1131}
 
----------
Epoch 2/40
time = 250.41 secondes

Train loss 0.48496347675728646 accuracy 0.8553045392036438 macro_avg {'precision': 0.8456177852988912, 'recall': 0.8440849489695162, 'f1-score': 0.8422094881293398, 'support': 10180} weighted_avg {'precision': 0.8525677272790382, 'recall': 0.8553045186640471, 'f1-score': 0.8519623143410425, 'support': 10180}
 
time = 6.44 secondes

Val loss 0.5016119628095291 accuracy 0.8620690107345581 macro_avg {'precision': 0.8648704025028234, 'recall': 0.8564332000402427, 'f1-score': 0.8585833877215496, 'support': 1131} weighted_avg {'precision': 0.8695228420599422, 'recall': 0.8620689655172413, 'f1-score': 0.863746119910853, 'support': 1131}
 
----------
Epoch 3/40
time = 252.85 secondes

Train loss 0.3050526772249999 accuracy 0.91277015209198 macro_avg {'precision': 0.9067210130726362, 'recall': 0.9055151724070978, 'f1-score': 0.9058408400367712, 'support': 10180} weighted_avg {'precision': 0.9127634831459945, 'recall': 0.9127701375245579, 'f1-score': 0.9125101334322827, 'support': 10180}
 
time = 6.31 secondes

Val loss 0.48222303617430823 accuracy 0.8815208077430725 macro_avg {'precision': 0.8799166300485682, 'recall': 0.8764343793848651, 'f1-score': 0.8754881131295917, 'support': 1131} weighted_avg {'precision': 0.8809780799672697, 'recall': 0.8815207780725022, 'f1-score': 0.8786434451640709, 'support': 1131}
 
----------
Epoch 4/40
time = 249.17 secondes

Train loss 0.22725786962670683 accuracy 0.9388015866279602 macro_avg {'precision': 0.9349841388262906, 'recall': 0.9339821258558235, 'f1-score': 0.9343333298424747, 'support': 10180} weighted_avg {'precision': 0.9387889371956779, 'recall': 0.9388015717092338, 'f1-score': 0.9386571937388186, 'support': 10180}
 
time = 5.08 secondes

Val loss 0.5502137617326118 accuracy 0.8815208077430725 macro_avg {'precision': 0.8853744654597024, 'recall': 0.8814566861071456, 'f1-score': 0.878857495553372, 'support': 1131} weighted_avg {'precision': 0.8861868232047749, 'recall': 0.8815207780725022, 'f1-score': 0.8785573751710786, 'support': 1131}
 
----------
Epoch 5/40
time = 249.24 secondes

Train loss 0.19641230086243333 accuracy 0.9507858753204346 macro_avg {'precision': 0.948511981459759, 'recall': 0.9484693777094252, 'f1-score': 0.9484055555860393, 'support': 10180} weighted_avg {'precision': 0.9508358712468861, 'recall': 0.9507858546168959, 'f1-score': 0.9507263598337382, 'support': 10180}
 
time = 6.28 secondes

Val loss 0.5521061757618797 accuracy 0.8832891583442688 macro_avg {'precision': 0.8901687593752688, 'recall': 0.8819169472205081, 'f1-score': 0.8825710852638039, 'support': 1131} weighted_avg {'precision': 0.8878001382409405, 'recall': 0.883289124668435, 'f1-score': 0.8824425151021483, 'support': 1131}
 
----------
Epoch 6/40
time = 254.43 secondes

Train loss 0.16167345823971568 accuracy 0.9603143930435181 macro_avg {'precision': 0.9584992858866789, 'recall': 0.9581818407284111, 'f1-score': 0.9581880224590977, 'support': 10180} weighted_avg {'precision': 0.9605859438917795, 'recall': 0.9603143418467583, 'f1-score': 0.9602990828018237, 'support': 10180}
 
time = 6.30 secondes

Val loss 0.6517427161067042 accuracy 0.8877100348472595 macro_avg {'precision': 0.8918089334082125, 'recall': 0.8835994715497657, 'f1-score': 0.8847546998978622, 'support': 1131} weighted_avg {'precision': 0.8922136177631051, 'recall': 0.887709991158267, 'f1-score': 0.8874180681409564, 'support': 1131}
 
----------
Epoch 7/40
time = 254.55 secondes

Train loss 0.1637788680134439 accuracy 0.9611984491348267 macro_avg {'precision': 0.959826983379559, 'recall': 0.9595446275194824, 'f1-score': 0.9596372693833917, 'support': 10180} weighted_avg {'precision': 0.9612768790557801, 'recall': 0.9611984282907662, 'f1-score': 0.9611888275978184, 'support': 10180}
 
time = 6.22 secondes

Val loss 0.6350101954869362 accuracy 0.890362560749054 macro_avg {'precision': 0.8974570061114584, 'recall': 0.8911933766228829, 'f1-score': 0.8916498405166781, 'support': 1131} weighted_avg {'precision': 0.8976327515896442, 'recall': 0.8903625110521662, 'f1-score': 0.8912939016041167, 'support': 1131}
 
----------
Epoch 8/40
time = 254.42 secondes

Train loss 0.14298796218323187 accuracy 0.9681729078292847 macro_avg {'precision': 0.9672610486536491, 'recall': 0.9670765013080856, 'f1-score': 0.9671317229086431, 'support': 10180} weighted_avg {'precision': 0.968218439394306, 'recall': 0.9681728880157171, 'f1-score': 0.9681571905919177, 'support': 10180}
 
time = 6.25 secondes

Val loss 0.7898611696461596 accuracy 0.8691423535346985 macro_avg {'precision': 0.887899102273823, 'recall': 0.872929445336946, 'f1-score': 0.875007796009038, 'support': 1131} weighted_avg {'precision': 0.8865159873834253, 'recall': 0.8691423519009726, 'f1-score': 0.8718047395706737, 'support': 1131}
 
----------
Epoch 9/40
time = 252.86 secondes

Train loss 0.1332599866179566 accuracy 0.9714145660400391 macro_avg {'precision': 0.9705973446711086, 'recall': 0.9702540668509286, 'f1-score': 0.9704079087676598, 'support': 10180} weighted_avg {'precision': 0.9714010547171026, 'recall': 0.9714145383104126, 'f1-score': 0.9713903787746004, 'support': 10180}
 
time = 6.28 secondes

Val loss 0.7185338385182624 accuracy 0.8983200788497925 macro_avg {'precision': 0.902110171606789, 'recall': 0.896917204141268, 'f1-score': 0.8981652471547363, 'support': 1131} weighted_avg {'precision': 0.9010892209862555, 'recall': 0.8983200707338639, 'f1-score': 0.898337725655038, 'support': 1131}
 
----------
Epoch 10/40
time = 250.87 secondes

Train loss 0.1414438217231935 accuracy 0.9705305099487305 macro_avg {'precision': 0.9699559341235299, 'recall': 0.96940057895849, 'f1-score': 0.9696152410344488, 'support': 10180} weighted_avg {'precision': 0.9706528522389238, 'recall': 0.9705304518664047, 'f1-score': 0.9705283726821283, 'support': 10180}
 
time = 6.23 secondes

Val loss 0.6151147103920894 accuracy 0.8992042541503906 macro_avg {'precision': 0.8994826102984316, 'recall': 0.9005370862342794, 'f1-score': 0.8989742882011498, 'support': 1131} weighted_avg {'precision': 0.9021506827867637, 'recall': 0.8992042440318302, 'f1-score': 0.8995782607227174, 'support': 1131}
 
----------
Epoch 11/40
time = 250.38 secondes

Train loss 0.11268642879636911 accuracy 0.9763261675834656 macro_avg {'precision': 0.9760817847336245, 'recall': 0.9758410864094953, 'f1-score': 0.9759299242931926, 'support': 10180} weighted_avg {'precision': 0.9764596170913139, 'recall': 0.9763261296660117, 'f1-score': 0.9763605584996281, 'support': 10180}
 
time = 6.29 secondes

Val loss 0.6982385573886551 accuracy 0.8947833776473999 macro_avg {'precision': 0.8989023000980456, 'recall': 0.8956732095720019, 'f1-score': 0.8940497072807911, 'support': 1131} weighted_avg {'precision': 0.9016936925423146, 'recall': 0.8947833775419982, 'f1-score': 0.8949007012654155, 'support': 1131}
 
----------
Epoch 12/40
time = 246.61 secondes

Train loss 0.09843571792536958 accuracy 0.9789783954620361 macro_avg {'precision': 0.9785610831417326, 'recall': 0.9783508537114176, 'f1-score': 0.9784126099916428, 'support': 10180} weighted_avg {'precision': 0.9791499345750116, 'recall': 0.9789783889980354, 'f1-score': 0.9790200333994942, 'support': 10180}
 
time = 6.24 secondes

Val loss 0.7867459102343558 accuracy 0.8850575089454651 macro_avg {'precision': 0.8925941427747249, 'recall': 0.8850328606236928, 'f1-score': 0.8838185314873982, 'support': 1131} weighted_avg {'precision': 0.8945909884218335, 'recall': 0.8850574712643678, 'f1-score': 0.8851050000586898, 'support': 1131}
 
----------
Epoch 13/40
time = 254.20 secondes

Train loss 0.10724455605449175 accuracy 0.9783890247344971 macro_avg {'precision': 0.9783251064651622, 'recall': 0.9782299679710753, 'f1-score': 0.9782361880042879, 'support': 10180} weighted_avg {'precision': 0.9784970709930761, 'recall': 0.9783889980353635, 'f1-score': 0.9784011348564265, 'support': 10180}
 
time = 6.25 secondes

Val loss 0.7212337041263466 accuracy 0.9027409553527832 macro_avg {'precision': 0.9079014071521889, 'recall': 0.9015326466345399, 'f1-score': 0.9030796564084236, 'support': 1131} weighted_avg {'precision': 0.9054230981023329, 'recall': 0.9027409372236959, 'f1-score': 0.9026222752660402, 'support': 1131}
 
----------
Epoch 14/40
time = 253.33 secondes

Train loss 0.1060652055026164 accuracy 0.9798625111579895 macro_avg {'precision': 0.9793581434077694, 'recall': 0.9797176864457041, 'f1-score': 0.979474340530917, 'support': 10180} weighted_avg {'precision': 0.9799293410832106, 'recall': 0.9798624754420432, 'f1-score': 0.9798349626955717, 'support': 10180}
 
time = 6.25 secondes

Val loss 0.8513213630887011 accuracy 0.8841733336448669 macro_avg {'precision': 0.9014140173000753, 'recall': 0.8891240563334666, 'f1-score': 0.8898796275869479, 'support': 1131} weighted_avg {'precision': 0.9005492163358568, 'recall': 0.8841732979664014, 'f1-score': 0.8867798437841086, 'support': 1131}
 
----------
Epoch 15/40
time = 255.08 secondes

Train loss 0.0936630969154409 accuracy 0.9824165105819702 macro_avg {'precision': 0.9820840284896712, 'recall': 0.982112758816913, 'f1-score': 0.9820740465033415, 'support': 10180} weighted_avg {'precision': 0.9825242847456129, 'recall': 0.9824165029469548, 'f1-score': 0.9824464880037547, 'support': 10180}
 
time = 6.20 secondes

Val loss 0.8696198455983623 accuracy 0.8850575089454651 macro_avg {'precision': 0.8979367759472334, 'recall': 0.8805349088146277, 'f1-score': 0.8834531094296064, 'support': 1131} weighted_avg {'precision': 0.8955735832829699, 'recall': 0.8850574712643678, 'f1-score': 0.885304344833838, 'support': 1131}
 
----------
Epoch 16/40
time = 253.76 secondes

Train loss 0.09373994883854568 accuracy 0.981925368309021 macro_avg {'precision': 0.9818545297736586, 'recall': 0.9815963040558252, 'f1-score': 0.981681152942382, 'support': 10180} weighted_avg {'precision': 0.982020217541798, 'recall': 0.9819253438113948, 'f1-score': 0.9819284786058913, 'support': 10180}
 
time = 6.26 secondes

Val loss 0.7434266330370954 accuracy 0.9045093059539795 macro_avg {'precision': 0.9055724898863552, 'recall': 0.9087095008155461, 'f1-score': 0.9045722361835002, 'support': 1131} weighted_avg {'precision': 0.910377981351077, 'recall': 0.9045092838196287, 'f1-score': 0.9046385195759782, 'support': 1131}
 
----------
Epoch 17/40
time = 248.63 secondes

Train loss 0.0852073819993534 accuracy 0.983693540096283 macro_avg {'precision': 0.9835227497759416, 'recall': 0.9834703744234401, 'f1-score': 0.9834741211115523, 'support': 10180} weighted_avg {'precision': 0.9837900121857261, 'recall': 0.9836935166994106, 'f1-score': 0.9837186851012693, 'support': 10180}
 
time = 6.28 secondes

Val loss 0.8058772287798822 accuracy 0.8912467360496521 macro_avg {'precision': 0.9001820522899079, 'recall': 0.8883072944353467, 'f1-score': 0.8903394504428587, 'support': 1131} weighted_avg {'precision': 0.9017106802879441, 'recall': 0.8912466843501327, 'f1-score': 0.8930389770539916, 'support': 1131}
 
----------
Epoch 18/40
time = 253.96 secondes

Train loss 0.0862807130051663 accuracy 0.9838899970054626 macro_avg {'precision': 0.9836736472559766, 'recall': 0.9834974424516462, 'f1-score': 0.9835531182558933, 'support': 10180} weighted_avg {'precision': 0.9839724486366203, 'recall': 0.9838899803536346, 'f1-score': 0.9839003568462777, 'support': 10180}
 
time = 6.49 secondes

Val loss 0.7889530622114634 accuracy 0.8938992023468018 macro_avg {'precision': 0.8961824480957692, 'recall': 0.8956483254896697, 'f1-score': 0.8933493777603797, 'support': 1131} weighted_avg {'precision': 0.9000784786420278, 'recall': 0.8938992042440318, 'f1-score': 0.8944611908085989, 'support': 1131}
 
----------
Epoch 19/40
time = 247.64 secondes

Train loss 0.08333922663364413 accuracy 0.9833988547325134 macro_avg {'precision': 0.9833815352336271, 'recall': 0.9830934702812859, 'f1-score': 0.983188727499145, 'support': 10180} weighted_avg {'precision': 0.9835161420745046, 'recall': 0.9833988212180746, 'f1-score': 0.9834069064108586, 'support': 10180}
 
time = 6.30 secondes

Val loss 0.8940603551128962 accuracy 0.890362560749054 macro_avg {'precision': 0.8959684831578176, 'recall': 0.8912495076587799, 'f1-score': 0.8895505308226286, 'support': 1131} weighted_avg {'precision': 0.8956305395390088, 'recall': 0.8903625110521662, 'f1-score': 0.8888008280682489, 'support': 1131}
 
----------
Epoch 20/40
time = 254.33 secondes

Train loss 0.07538770590480949 accuracy 0.9852652549743652 macro_avg {'precision': 0.984924792626364, 'recall': 0.9847436628782493, 'f1-score': 0.9848163840766606, 'support': 10180} weighted_avg {'precision': 0.9853022070941118, 'recall': 0.9852652259332023, 'f1-score': 0.985266480370869, 'support': 10180}
 
time = 6.25 secondes

Val loss 0.8035848262317276 accuracy 0.9036251306533813 macro_avg {'precision': 0.9046379088273312, 'recall': 0.9043215814051022, 'f1-score': 0.9035060805555899, 'support': 1131} weighted_avg {'precision': 0.9059441591669839, 'recall': 0.9036251105216623, 'f1-score': 0.9037694866848515, 'support': 1131}
 
----------
Epoch 21/40
time = 253.69 secondes

Train loss 0.07649617259461425 accuracy 0.9852652549743652 macro_avg {'precision': 0.9849618108779958, 'recall': 0.9841575337330266, 'f1-score': 0.9845255383534376, 'support': 10180} weighted_avg {'precision': 0.9852913460855407, 'recall': 0.9852652259332023, 'f1-score': 0.9852510306207231, 'support': 10180}
 
time = 6.37 secondes

Val loss 0.71709914195157 accuracy 0.9062776565551758 macro_avg {'precision': 0.9091232145368975, 'recall': 0.9059608042946733, 'f1-score': 0.9061926862559659, 'support': 1131} weighted_avg {'precision': 0.9095317157509181, 'recall': 0.9062776304155614, 'f1-score': 0.9065305663651001, 'support': 1131}
 
----------
Epoch 22/40
time = 251.80 secondes

Train loss 0.07508612635005481 accuracy 0.9858546257019043 macro_avg {'precision': 0.9851640974738652, 'recall': 0.9849592286813431, 'f1-score': 0.9850329060585074, 'support': 10180} weighted_avg {'precision': 0.985925618855856, 'recall': 0.9858546168958743, 'f1-score': 0.9858598801858665, 'support': 10180}
 
time = 6.26 secondes

Val loss 0.7771122515981224 accuracy 0.8912467360496521 macro_avg {'precision': 0.8916989219961419, 'recall': 0.8910002393936196, 'f1-score': 0.8896867376986837, 'support': 1131} weighted_avg {'precision': 0.8949465485330963, 'recall': 0.8912466843501327, 'f1-score': 0.8914766632258937, 'support': 1131}
 
----------
Epoch 23/40
time = 253.75 secondes

Train loss 0.06649474734402981 accuracy 0.9868369698524475 macro_avg {'precision': 0.9869598334560823, 'recall': 0.9866141262910209, 'f1-score': 0.9867399756537052, 'support': 10180} weighted_avg {'precision': 0.9869732011776734, 'recall': 0.9868369351669941, 'f1-score': 0.9868562958675936, 'support': 10180}
 
time = 6.59 secondes

Val loss 0.8698853232045609 accuracy 0.8832891583442688 macro_avg {'precision': 0.8936118409954126, 'recall': 0.8868135990105023, 'f1-score': 0.8860164051735173, 'support': 1131} weighted_avg {'precision': 0.8960081927482021, 'recall': 0.883289124668435, 'f1-score': 0.8850063218565538, 'support': 1131}
 
----------
Epoch 24/40
time = 247.39 secondes

Train loss 0.06443913673503525 accuracy 0.9882122278213501 macro_avg {'precision': 0.9884347237536624, 'recall': 0.9878422348110023, 'f1-score': 0.9881050835950571, 'support': 10180} weighted_avg {'precision': 0.9882887699425753, 'recall': 0.9882121807465619, 'f1-score': 0.988216700675349, 'support': 10180}
 
time = 6.81 secondes

Val loss 0.9008349262796503 accuracy 0.8797524571418762 macro_avg {'precision': 0.9013076739343182, 'recall': 0.8860740963559991, 'f1-score': 0.8859691391537234, 'support': 1131} weighted_avg {'precision': 0.9026598609297724, 'recall': 0.8797524314765695, 'f1-score': 0.8831171379518498, 'support': 1131}
 
----------
Epoch 25/40
time = 252.47 secondes

Train loss 0.06199731292798212 accuracy 0.9878193140029907 macro_avg {'precision': 0.9878420547210801, 'recall': 0.9875495747907845, 'f1-score': 0.9876574337046028, 'support': 10180} weighted_avg {'precision': 0.9878944489600443, 'recall': 0.9878192534381139, 'f1-score': 0.987817068005623, 'support': 10180}
 
time = 6.28 secondes

Val loss 0.7781249771592934 accuracy 0.9089301824569702 macro_avg {'precision': 0.9090593540135347, 'recall': 0.9104686617119768, 'f1-score': 0.9086618690075611, 'support': 1131} weighted_avg {'precision': 0.9117031386422307, 'recall': 0.9089301503094607, 'f1-score': 0.9092412723019458, 'support': 1131}
 
----------
Epoch 26/40
time = 259.98 secondes

Train loss 0.04905723339006256 accuracy 0.990667998790741 macro_avg {'precision': 0.9905403380862923, 'recall': 0.9905982765528446, 'f1-score': 0.9905401349249285, 'support': 10180} weighted_avg {'precision': 0.9907890921282178, 'recall': 0.9906679764243614, 'f1-score': 0.990699532536304, 'support': 10180}
 
time = 6.25 secondes

Val loss 0.8718313767422445 accuracy 0.8983200788497925 macro_avg {'precision': 0.9091984748318037, 'recall': 0.9001378818730345, 'f1-score': 0.9009247433880564, 'support': 1131} weighted_avg {'precision': 0.911648437796124, 'recall': 0.8983200707338639, 'f1-score': 0.9009764634535008, 'support': 1131}
 
----------
Epoch 27/40
time = 247.28 secondes

Train loss 0.04683483094658539 accuracy 0.9904715418815613 macro_avg {'precision': 0.9907126952630086, 'recall': 0.9904299653396533, 'f1-score': 0.9905488173846114, 'support': 10180} weighted_avg {'precision': 0.9905742971530102, 'recall': 0.9904715127701376, 'f1-score': 0.9904997879388057, 'support': 10180}
 
time = 6.25 secondes

Val loss 0.7706838865484366 accuracy 0.9071618318557739 macro_avg {'precision': 0.9087197949291651, 'recall': 0.9086153168794034, 'f1-score': 0.9062095197734419, 'support': 1131} weighted_avg {'precision': 0.9131825622493647, 'recall': 0.9071618037135278, 'f1-score': 0.907430363854908, 'support': 1131}
 
----------
Epoch 28/40
time = 253.32 secondes

Train loss 0.03747292974340313 accuracy 0.9918467998504639 macro_avg {'precision': 0.991569028064854, 'recall': 0.9916000467265608, 'f1-score': 0.9915774177590031, 'support': 10180} weighted_avg {'precision': 0.9918813697160291, 'recall': 0.9918467583497053, 'f1-score': 0.991856719737464, 'support': 10180}
 
time = 6.31 secondes

Val loss 0.7398414855038347 accuracy 0.9080460071563721 macro_avg {'precision': 0.9069243953722591, 'recall': 0.9097181379380036, 'f1-score': 0.907413456300068, 'support': 1131} weighted_avg {'precision': 0.910722146862189, 'recall': 0.9080459770114943, 'f1-score': 0.9084153865189453, 'support': 1131}
 
----------
Epoch 29/40
time = 254.61 secondes

Train loss 0.04082086434592702 accuracy 0.9912574291229248 macro_avg {'precision': 0.9908840263386347, 'recall': 0.9904622177896103, 'f1-score': 0.9906428410553001, 'support': 10180} weighted_avg {'precision': 0.991333556937932, 'recall': 0.9912573673870334, 'f1-score': 0.9912639469301884, 'support': 10180}
 
time = 6.35 secondes

Val loss 0.7864075064969935 accuracy 0.9115827083587646 macro_avg {'precision': 0.9109939369694613, 'recall': 0.9107354035841976, 'f1-score': 0.9100794005864048, 'support': 1131} weighted_avg {'precision': 0.9138807059131406, 'recall': 0.9115826702033598, 'f1-score': 0.9119131571503732, 'support': 1131}
 
----------
Epoch 30/40
time = 252.45 secondes

Train loss 0.043603002998475415 accuracy 0.9914538860321045 macro_avg {'precision': 0.9914862324786119, 'recall': 0.9911208860181308, 'f1-score': 0.9912836678342121, 'support': 10180} weighted_avg {'precision': 0.9915106850527104, 'recall': 0.9914538310412574, 'f1-score': 0.9914625654576542, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.873538985305087 accuracy 0.9018567800521851 macro_avg {'precision': 0.9069680122030827, 'recall': 0.9029615375478851, 'f1-score': 0.9022063481094607, 'support': 1131} weighted_avg {'precision': 0.9108884669188059, 'recall': 0.9018567639257294, 'f1-score': 0.903453447366854, 'support': 1131}
 
----------
Epoch 31/40
time = 250.43 secondes

Train loss 0.03251755231354804 accuracy 0.9931238293647766 macro_avg {'precision': 0.9934549970697603, 'recall': 0.9932387881448163, 'f1-score': 0.9933352431575699, 'support': 10180} weighted_avg {'precision': 0.9931774480813609, 'recall': 0.9931237721021611, 'f1-score': 0.9931386197619053, 'support': 10180}
 
time = 6.67 secondes

Val loss 0.7959042240392181 accuracy 0.9062776565551758 macro_avg {'precision': 0.9049985075192964, 'recall': 0.9058403820225622, 'f1-score': 0.904741852052006, 'support': 1131} weighted_avg {'precision': 0.9075861449403145, 'recall': 0.9062776304155614, 'f1-score': 0.9062024467019951, 'support': 1131}
 
----------
Epoch 32/40
time = 241.35 secondes

Train loss 0.03215694315108671 accuracy 0.9932220578193665 macro_avg {'precision': 0.9935482080237248, 'recall': 0.9932583237525188, 'f1-score': 0.9933733138429035, 'support': 10180} weighted_avg {'precision': 0.9933108910178264, 'recall': 0.9932220039292731, 'f1-score': 0.9932346338564448, 'support': 10180}
 
time = 6.29 secondes

Val loss 0.8279811651553527 accuracy 0.9053934812545776 macro_avg {'precision': 0.9053429663916536, 'recall': 0.9072972450571747, 'f1-score': 0.904885999481024, 'support': 1131} weighted_avg {'precision': 0.9091180783269026, 'recall': 0.905393457117595, 'f1-score': 0.9059413832399372, 'support': 1131}
 
----------
Epoch 33/40
time = 252.33 secondes

Train loss 0.022605627455932493 accuracy 0.9944007992744446 macro_avg {'precision': 0.9947063170464153, 'recall': 0.9945088804952447, 'f1-score': 0.9945786006180393, 'support': 10180} weighted_avg {'precision': 0.9944984083067377, 'recall': 0.9944007858546169, 'f1-score': 0.9944186599199637, 'support': 10180}
 
time = 6.26 secondes

Val loss 0.861799381664113 accuracy 0.9053934812545776 macro_avg {'precision': 0.9096562629868984, 'recall': 0.9053746175733179, 'f1-score': 0.905434716557866, 'support': 1131} weighted_avg {'precision': 0.9117097780427611, 'recall': 0.905393457117595, 'f1-score': 0.9064731142078696, 'support': 1131}
 
----------
Epoch 34/40
time = 253.39 secondes

Train loss 0.026123207489779723 accuracy 0.9944007992744446 macro_avg {'precision': 0.9946445835116627, 'recall': 0.9945317380900794, 'f1-score': 0.9945583369770479, 'support': 10180} weighted_avg {'precision': 0.994488155605169, 'recall': 0.9944007858546169, 'f1-score': 0.9944127878725011, 'support': 10180}
 
time = 6.21 secondes

Val loss 0.8158254677536988 accuracy 0.9098143577575684 macro_avg {'precision': 0.9103377385560047, 'recall': 0.9090084323689298, 'f1-score': 0.9091680251985454, 'support': 1131} weighted_avg {'precision': 0.9114969618415405, 'recall': 0.9098143236074271, 'f1-score': 0.9101751960592952, 'support': 1131}
 
----------
Epoch 35/40
time = 253.62 secondes

Train loss 0.023400445663112918 accuracy 0.9949902296066284 macro_avg {'precision': 0.9951466435571173, 'recall': 0.9948831502931883, 'f1-score': 0.9949945969963064, 'support': 10180} weighted_avg {'precision': 0.9950550568605767, 'recall': 0.9949901768172889, 'f1-score': 0.9950023213685603, 'support': 10180}
 
time = 6.23 secondes

Val loss 0.8855101435121827 accuracy 0.9080460071563721 macro_avg {'precision': 0.9054002361845888, 'recall': 0.9075016589616656, 'f1-score': 0.9055368151005633, 'support': 1131} weighted_avg {'precision': 0.9087993345379598, 'recall': 0.9080459770114943, 'f1-score': 0.9074579227576363, 'support': 1131}
 
----------
Epoch 36/40
time = 253.33 secondes

Train loss 0.020801392898910174 accuracy 0.9945972561836243 macro_avg {'precision': 0.9947935765868172, 'recall': 0.9946380468747462, 'f1-score': 0.9946966406339286, 'support': 10180} weighted_avg {'precision': 0.9946668394344048, 'recall': 0.9945972495088409, 'f1-score': 0.9946116199790289, 'support': 10180}
 
time = 6.23 secondes

Val loss 0.8287905697371302 accuracy 0.9106985330581665 macro_avg {'precision': 0.91080338715549, 'recall': 0.9122067667612219, 'f1-score': 0.9102753104430542, 'support': 1131} weighted_avg {'precision': 0.9138932516617063, 'recall': 0.9106984969053935, 'f1-score': 0.9109358310680689, 'support': 1131}
 
----------
Epoch 37/40
time = 253.47 secondes

Train loss 0.01761471263811047 accuracy 0.9959725141525269 macro_avg {'precision': 0.9962944888136255, 'recall': 0.9960447055285859, 'f1-score': 0.9961436024589364, 'support': 10180} weighted_avg {'precision': 0.9960668764823606, 'recall': 0.9959724950884087, 'f1-score': 0.9959919011333696, 'support': 10180}
 
time = 6.26 secondes

Val loss 0.8659169417040201 accuracy 0.9080460071563721 macro_avg {'precision': 0.9070358580006621, 'recall': 0.9085192939222202, 'f1-score': 0.9070149336959737, 'support': 1131} weighted_avg {'precision': 0.9100481944383088, 'recall': 0.9080459770114943, 'f1-score': 0.908270135369527, 'support': 1131}
 
----------
Epoch 38/40
time = 251.97 secondes

Train loss 0.016626731144381006 accuracy 0.995874285697937 macro_avg {'precision': 0.9961114117568609, 'recall': 0.9959046623455079, 'f1-score': 0.9959821525928971, 'support': 10180} weighted_avg {'precision': 0.9959684178198424, 'recall': 0.9958742632612967, 'f1-score': 0.9958936597765506, 'support': 10180}
 
time = 6.85 secondes

Val loss 0.8629623673872963 accuracy 0.9089301824569702 macro_avg {'precision': 0.910901710980801, 'recall': 0.9095121977409397, 'f1-score': 0.9092580710442382, 'support': 1131} weighted_avg {'precision': 0.9122859265560784, 'recall': 0.9089301503094607, 'f1-score': 0.9095944869372058, 'support': 1131}
 
----------
Epoch 39/40
time = 251.75 secondes

Train loss 0.009891796546209914 accuracy 0.9969548583030701 macro_avg {'precision': 0.9971727993459281, 'recall': 0.9969797237776529, 'f1-score': 0.9970515061631096, 'support': 10180} weighted_avg {'precision': 0.9970444980711491, 'recall': 0.9969548133595285, 'f1-score': 0.9969731917859667, 'support': 10180}
 
time = 6.26 secondes

Val loss 0.9459587670333759 accuracy 0.9089301824569702 macro_avg {'precision': 0.9102311522122989, 'recall': 0.9105078815268277, 'f1-score': 0.9087555599315984, 'support': 1131} weighted_avg {'precision': 0.9120584359293354, 'recall': 0.9089301503094607, 'f1-score': 0.9088193433894571, 'support': 1131}
 
----------
Epoch 40/40
time = 248.40 secondes

Train loss 0.010042260334513534 accuracy 0.9965619444847107 macro_avg {'precision': 0.9968306700114397, 'recall': 0.9965498747446304, 'f1-score': 0.9966651361640674, 'support': 10180} weighted_avg {'precision': 0.9966507918322293, 'recall': 0.9965618860510805, 'f1-score': 0.9965796155456967, 'support': 10180}
 
time = 6.55 secondes

Val loss 0.9654523783901054 accuracy 0.9080460071563721 macro_avg {'precision': 0.9084019960206519, 'recall': 0.9085922630781903, 'f1-score': 0.9071073943779963, 'support': 1131} weighted_avg {'precision': 0.9109309255795497, 'recall': 0.9080459770114943, 'f1-score': 0.9080775568541543, 'support': 1131}
 
----------
best_accuracy 0.9115827083587646 best_epoch 29 macro_avg {'precision': 0.9109939369694613, 'recall': 0.9107354035841976, 'f1-score': 0.9100794005864048, 'support': 1131} weighted_avg {'precision': 0.9138807059131406, 'recall': 0.9115826702033598, 'f1-score': 0.9119131571503732, 'support': 1131}

average train time 251.9520350217819

average val time 6.272309643030167
 
time = 43.45 secondes

test_accuracy 0.8431607484817505 macro_avg {'precision': 0.8394473356558748, 'recall': 0.8346197003966953, 'f1-score': 0.8355777163379312, 'support': 7530} weighted_avg {'precision': 0.8458979217887587, 'recall': 0.8431606905710491, 'f1-score': 0.8431169369218939, 'support': 7530}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_head_tail_1
----------
Epoch 1/40
time = 255.84 secondes

Train loss 1.2830585191023407 accuracy 0.674361526966095 macro_avg {'precision': 0.7042590416835216, 'recall': 0.6590775138621932, 'f1-score': 0.6544036528757893, 'support': 10180} weighted_avg {'precision': 0.7068209071841186, 'recall': 0.6743614931237721, 'f1-score': 0.669243462486306, 'support': 10180}
 
time = 6.63 secondes

Val loss 0.6803297761460425 accuracy 0.802829384803772 macro_avg {'precision': 0.7829066236232617, 'recall': 0.7963952747543152, 'f1-score': 0.7825332420681937, 'support': 1131} weighted_avg {'precision': 0.7961894642612414, 'recall': 0.8028293545534925, 'f1-score': 0.7924820789714299, 'support': 1131}
 
----------
Epoch 2/40
time = 263.30 secondes

Train loss 0.4760333944691032 accuracy 0.8636542558670044 macro_avg {'precision': 0.8552114187577553, 'recall': 0.8531098400824131, 'f1-score': 0.8521712455956246, 'support': 10180} weighted_avg {'precision': 0.8622795231272339, 'recall': 0.8636542239685658, 'f1-score': 0.8614585343242591, 'support': 10180}
 
time = 6.31 secondes

Val loss 0.5085004410693343 accuracy 0.8682581782341003 macro_avg {'precision': 0.8730237347390896, 'recall': 0.8677118827473749, 'f1-score': 0.8667095070377094, 'support': 1131} weighted_avg {'precision': 0.877731336027114, 'recall': 0.8682581786030061, 'f1-score': 0.8695298371943511, 'support': 1131}
 
----------
Epoch 3/40
time = 250.97 secondes

Train loss 0.3025209774676962 accuracy 0.9140471816062927 macro_avg {'precision': 0.9082012066775091, 'recall': 0.9073028929809321, 'f1-score': 0.9074763078174733, 'support': 10180} weighted_avg {'precision': 0.9137298295723221, 'recall': 0.9140471512770137, 'f1-score': 0.9136494292328738, 'support': 10180}
 
time = 6.47 secondes

Val loss 0.517105889808334 accuracy 0.8735632300376892 macro_avg {'precision': 0.8730278007006314, 'recall': 0.8689261513603252, 'f1-score': 0.8686954785274986, 'support': 1131} weighted_avg {'precision': 0.8739423766903065, 'recall': 0.8735632183908046, 'f1-score': 0.8717047749984709, 'support': 1131}
 
----------
Epoch 4/40
time = 258.29 secondes

Train loss 0.23388779827202064 accuracy 0.9373281002044678 macro_avg {'precision': 0.9347337704323134, 'recall': 0.9332953590050452, 'f1-score': 0.9338069963566689, 'support': 10180} weighted_avg {'precision': 0.9373312563784226, 'recall': 0.937328094302554, 'f1-score': 0.9371480047208787, 'support': 10180}
 
time = 7.00 secondes

Val loss 0.6222884856380889 accuracy 0.8709107041358948 macro_avg {'precision': 0.8701225204903705, 'recall': 0.8635248045782566, 'f1-score': 0.8619830578495751, 'support': 1131} weighted_avg {'precision': 0.8713772200058817, 'recall': 0.8709106984969054, 'f1-score': 0.866788524616402, 'support': 1131}
 
----------
Epoch 5/40
time = 256.55 secondes

Train loss 0.18871747343062628 accuracy 0.9521611332893372 macro_avg {'precision': 0.9503342929621489, 'recall': 0.9496045574033858, 'f1-score': 0.9499260222212437, 'support': 10180} weighted_avg {'precision': 0.952296480345862, 'recall': 0.9521611001964636, 'f1-score': 0.9521911427638041, 'support': 10180}
 
time = 6.32 secondes

Val loss 0.5795758194454067 accuracy 0.890362560749054 macro_avg {'precision': 0.897684537324418, 'recall': 0.8888184083303443, 'f1-score': 0.8877912779853091, 'support': 1131} weighted_avg {'precision': 0.8990947139198906, 'recall': 0.8903625110521662, 'f1-score': 0.8899815993390532, 'support': 1131}
 
----------
Epoch 6/40
time = 256.67 secondes

Train loss 0.16204462121635546 accuracy 0.9601179361343384 macro_avg {'precision': 0.9583310993324275, 'recall': 0.9581733986960348, 'f1-score': 0.9581780898136877, 'support': 10180} weighted_avg {'precision': 0.9601762479969125, 'recall': 0.9601178781925344, 'f1-score': 0.9600760217268638, 'support': 10180}
 
time = 6.33 secondes

Val loss 0.6753562799782794 accuracy 0.8885942101478577 macro_avg {'precision': 0.8919521791672229, 'recall': 0.8900623761752587, 'f1-score': 0.8881208143193767, 'support': 1131} weighted_avg {'precision': 0.8961500521364948, 'recall': 0.8885941644562334, 'f1-score': 0.8895354499833277, 'support': 1131}
 
----------
Epoch 7/40
time = 258.83 secondes

Train loss 0.16323311493883358 accuracy 0.9629666209220886 macro_avg {'precision': 0.9616248810017298, 'recall': 0.9612335650687485, 'f1-score': 0.9613809945566338, 'support': 10180} weighted_avg {'precision': 0.9630540503718173, 'recall': 0.962966601178782, 'f1-score': 0.9629619260589494, 'support': 10180}
 
time = 6.33 secondes

Val loss 0.6721763610938223 accuracy 0.8868258595466614 macro_avg {'precision': 0.8884019780767936, 'recall': 0.8905161099500083, 'f1-score': 0.8873819065260198, 'support': 1131} weighted_avg {'precision': 0.890759614941706, 'recall': 0.8868258178603006, 'f1-score': 0.8866618966750515, 'support': 1131}
 
----------
Epoch 8/40
time = 256.42 secondes

Train loss 0.15330203396867262 accuracy 0.9667976498603821 macro_avg {'precision': 0.9660405169003596, 'recall': 0.9657636417444605, 'f1-score': 0.9658754885849022, 'support': 10180} weighted_avg {'precision': 0.9669265008402023, 'recall': 0.9667976424361493, 'f1-score': 0.9668351598862488, 'support': 10180}
 
time = 6.50 secondes

Val loss 0.6529183594352552 accuracy 0.895667552947998 macro_avg {'precision': 0.9030228395397237, 'recall': 0.89772670021303, 'f1-score': 0.8972375579675027, 'support': 1131} weighted_avg {'precision': 0.9048165886347984, 'recall': 0.8956675508399646, 'f1-score': 0.897065411506179, 'support': 1131}
 
----------
Epoch 9/40
time = 255.53 secondes

Train loss 0.13662856976105794 accuracy 0.9730845093727112 macro_avg {'precision': 0.9727139129883786, 'recall': 0.9722014234562524, 'f1-score': 0.9723753798654101, 'support': 10180} weighted_avg {'precision': 0.9733212945359608, 'recall': 0.9730844793713163, 'f1-score': 0.9731206464600454, 'support': 10180}
 
time = 6.31 secondes

Val loss 0.6732724142895395 accuracy 0.9018567800521851 macro_avg {'precision': 0.9047545748686925, 'recall': 0.9017121580090525, 'f1-score': 0.9011853347821608, 'support': 1131} weighted_avg {'precision': 0.9055505815894599, 'recall': 0.9018567639257294, 'f1-score': 0.9018706313837621, 'support': 1131}
 
----------
Epoch 10/40
time = 252.46 secondes

Train loss 0.13843456778278374 accuracy 0.9704322814941406 macro_avg {'precision': 0.9698349841204432, 'recall': 0.9699565279248816, 'f1-score': 0.9698452334933906, 'support': 10180} weighted_avg {'precision': 0.9705204568024737, 'recall': 0.9704322200392927, 'f1-score': 0.9704262554351565, 'support': 10180}
 
time = 6.53 secondes

Val loss 0.7197302790304554 accuracy 0.8859416842460632 macro_avg {'precision': 0.8900575696317728, 'recall': 0.8861258858937815, 'f1-score': 0.884914652772183, 'support': 1131} weighted_avg {'precision': 0.8914674862747624, 'recall': 0.8859416445623343, 'f1-score': 0.8854500164473632, 'support': 1131}
 
----------
Epoch 11/40
time = 254.08 secondes

Train loss 0.13139081810948475 accuracy 0.9738703966140747 macro_avg {'precision': 0.9736684849165931, 'recall': 0.973443169210449, 'f1-score': 0.9734917485877299, 'support': 10180} weighted_avg {'precision': 0.9740390802309602, 'recall': 0.9738703339882122, 'f1-score': 0.9738889475039889, 'support': 10180}
 
time = 6.33 secondes

Val loss 0.8076934009080451 accuracy 0.8770999312400818 macro_avg {'precision': 0.8827841205681656, 'recall': 0.8825475808493584, 'f1-score': 0.8790945694528173, 'support': 1131} weighted_avg {'precision': 0.8883508247186942, 'recall': 0.8770999115826702, 'f1-score': 0.8790593401530211, 'support': 1131}
 
----------
Epoch 12/40
time = 255.58 secondes

Train loss 0.10938843878728427 accuracy 0.9778978824615479 macro_avg {'precision': 0.9774395518149864, 'recall': 0.9775138502804281, 'f1-score': 0.9774007344198911, 'support': 10180} weighted_avg {'precision': 0.978067530106983, 'recall': 0.9778978388998035, 'f1-score': 0.9779136623730593, 'support': 10180}
 
time = 5.95 secondes

Val loss 0.7774392747505515 accuracy 0.8912467360496521 macro_avg {'precision': 0.8946150483248185, 'recall': 0.8892649578245019, 'f1-score': 0.8900982843902188, 'support': 1131} weighted_avg {'precision': 0.8986783089433154, 'recall': 0.8912466843501327, 'f1-score': 0.8931143771141562, 'support': 1131}
 
----------
Epoch 13/40
time = 256.73 secondes

Train loss 0.11155108422688534 accuracy 0.9778978824615479 macro_avg {'precision': 0.9776704245064309, 'recall': 0.9775646881152541, 'f1-score': 0.9775582612885254, 'support': 10180} weighted_avg {'precision': 0.9780849699405078, 'recall': 0.9778978388998035, 'f1-score': 0.9779310017241668, 'support': 10180}
 
time = 6.74 secondes

Val loss 0.667198965942051 accuracy 0.9053934812545776 macro_avg {'precision': 0.9094459387771506, 'recall': 0.9082001027326541, 'f1-score': 0.9070601493723103, 'support': 1131} weighted_avg {'precision': 0.9110051288414909, 'recall': 0.905393457117595, 'f1-score': 0.9063979071049585, 'support': 1131}
 
----------
Epoch 14/40
time = 254.94 secondes

Train loss 0.09489547432345642 accuracy 0.9803536534309387 macro_avg {'precision': 0.9796051534259874, 'recall': 0.9794420547060902, 'f1-score': 0.9795049454138441, 'support': 10180} weighted_avg {'precision': 0.9803840715557735, 'recall': 0.9803536345776032, 'f1-score': 0.9803497244575784, 'support': 10180}
 
time = 6.26 secondes

Val loss 0.8236819178625961 accuracy 0.8885942101478577 macro_avg {'precision': 0.8924806633927205, 'recall': 0.891406145735823, 'f1-score': 0.8894266637439617, 'support': 1131} weighted_avg {'precision': 0.897012719702795, 'recall': 0.8885941644562334, 'f1-score': 0.8901763171461834, 'support': 1131}
 
----------
Epoch 15/40
time = 256.81 secondes

Train loss 0.09940407353899627 accuracy 0.9805501103401184 macro_avg {'precision': 0.9798024209897817, 'recall': 0.9798467931352688, 'f1-score': 0.9797707689675124, 'support': 10180} weighted_avg {'precision': 0.9807467881995214, 'recall': 0.9805500982318271, 'f1-score': 0.9805934896694872, 'support': 10180}
 
time = 6.34 secondes

Val loss 0.8028926655808365 accuracy 0.8974359035491943 macro_avg {'precision': 0.9010533487605932, 'recall': 0.9014841393647052, 'f1-score': 0.8992854851126639, 'support': 1131} weighted_avg {'precision': 0.9038555075659606, 'recall': 0.8974358974358975, 'f1-score': 0.8987085594220998, 'support': 1131}
 
----------
Epoch 16/40
time = 254.85 secondes

Train loss 0.0912077322089437 accuracy 0.9808448553085327 macro_avg {'precision': 0.9803215789343982, 'recall': 0.9799263363448478, 'f1-score': 0.9800924856272228, 'support': 10180} weighted_avg {'precision': 0.9808893508652115, 'recall': 0.980844793713163, 'f1-score': 0.9808398575471531, 'support': 10180}
 
time = 6.28 secondes

Val loss 0.7025544699945513 accuracy 0.9062776565551758 macro_avg {'precision': 0.9088509189218129, 'recall': 0.90860636062189, 'f1-score': 0.9075842467231221, 'support': 1131} weighted_avg {'precision': 0.9105005612810371, 'recall': 0.9062776304155614, 'f1-score': 0.9072373697116672, 'support': 1131}
 
----------
Epoch 17/40
time = 247.59 secondes

Train loss 0.08630306620627202 accuracy 0.9828094840049744 macro_avg {'precision': 0.9823508356693033, 'recall': 0.9824075504358625, 'f1-score': 0.9823259079640847, 'support': 10180} weighted_avg {'precision': 0.9829960924748676, 'recall': 0.9828094302554028, 'f1-score': 0.9828508482979531, 'support': 10180}
 
time = 6.54 secondes

Val loss 0.8942839723999831 accuracy 0.8930150270462036 macro_avg {'precision': 0.9007151679336574, 'recall': 0.8952769932440068, 'f1-score': 0.8948096792620515, 'support': 1131} weighted_avg {'precision': 0.904122434072862, 'recall': 0.8930150309460654, 'f1-score': 0.8954762901176235, 'support': 1131}
 
----------
Epoch 18/40
time = 261.69 secondes

Train loss 0.08818634122925711 accuracy 0.9833988547325134 macro_avg {'precision': 0.982777147363395, 'recall': 0.9830179235251956, 'f1-score': 0.982869021714319, 'support': 10180} weighted_avg {'precision': 0.983492867799917, 'recall': 0.9833988212180746, 'f1-score': 0.983419697984918, 'support': 10180}
 
time = 5.76 secondes

Val loss 0.8542920814804129 accuracy 0.8850575089454651 macro_avg {'precision': 0.8967613702687279, 'recall': 0.8787807723708673, 'f1-score': 0.8826495444774116, 'support': 1131} weighted_avg {'precision': 0.8937408205432551, 'recall': 0.8850574712643678, 'f1-score': 0.8851589664638703, 'support': 1131}
 
----------
Epoch 19/40
time = 219.32 secondes

Train loss 0.07127160499498292 accuracy 0.9854617118835449 macro_avg {'precision': 0.9854239651197053, 'recall': 0.9853421158014983, 'f1-score': 0.9853434517289734, 'support': 10180} weighted_avg {'precision': 0.9855478739108663, 'recall': 0.9854616895874263, 'f1-score': 0.9854634743599178, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.8503135254920803 accuracy 0.8930150270462036 macro_avg {'precision': 0.9010060471162811, 'recall': 0.8926668268211125, 'f1-score': 0.8943171214606396, 'support': 1131} weighted_avg {'precision': 0.8982501643476957, 'recall': 0.8930150309460654, 'f1-score': 0.8931534592656328, 'support': 1131}
 
----------
Epoch 20/40
time = 226.86 secondes

Train loss 0.09931909155223122 accuracy 0.9826130270957947 macro_avg {'precision': 0.9823445431365923, 'recall': 0.9819972351735679, 'f1-score': 0.9821437700956475, 'support': 10180} weighted_avg {'precision': 0.9826888762062062, 'recall': 0.9826129666011788, 'f1-score': 0.9826240857427412, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.7391820351917437 accuracy 0.9000884294509888 macro_avg {'precision': 0.9060432140246375, 'recall': 0.8990008936378062, 'f1-score': 0.9003013733227536, 'support': 1131} weighted_avg {'precision': 0.9045216992502138, 'recall': 0.9000884173297966, 'f1-score': 0.9001302055354593, 'support': 1131}
 
----------
Epoch 21/40
time = 228.27 secondes

Train loss 0.0897147113443981 accuracy 0.9846758842468262 macro_avg {'precision': 0.9846110814473107, 'recall': 0.9834878813718435, 'f1-score': 0.9839657088858423, 'support': 10180} weighted_avg {'precision': 0.9847917811216021, 'recall': 0.9846758349705305, 'f1-score': 0.9846580057183008, 'support': 10180}
 
time = 5.46 secondes

Val loss 0.8042383269752412 accuracy 0.8894783854484558 macro_avg {'precision': 0.895127049799442, 'recall': 0.8923985210025984, 'f1-score': 0.8902442519640319, 'support': 1131} weighted_avg {'precision': 0.8958600848880639, 'recall': 0.8894783377541998, 'f1-score': 0.8891899581834081, 'support': 1131}
 
----------
Epoch 22/40
time = 227.04 secondes

Train loss 0.07822807987250147 accuracy 0.9854617118835449 macro_avg {'precision': 0.9857727529825773, 'recall': 0.9853139418547737, 'f1-score': 0.9854954109342259, 'support': 10180} weighted_avg {'precision': 0.9855292056190923, 'recall': 0.9854616895874263, 'f1-score': 0.9854466993324194, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.7859687734334598 accuracy 0.8983200788497925 macro_avg {'precision': 0.9019515089778773, 'recall': 0.8996435899348313, 'f1-score': 0.8985948734199326, 'support': 1131} weighted_avg {'precision': 0.9044969228780887, 'recall': 0.8983200707338639, 'f1-score': 0.8992254021018883, 'support': 1131}
 
----------
Epoch 23/40
time = 221.76 secondes

Train loss 0.0651263971062697 accuracy 0.9878193140029907 macro_avg {'precision': 0.9879145125975753, 'recall': 0.9874858839952975, 'f1-score': 0.987674821933077, 'support': 10180} weighted_avg {'precision': 0.9878761265083196, 'recall': 0.9878192534381139, 'f1-score': 0.9878225188853894, 'support': 10180}
 
time = 4.98 secondes

Val loss 0.7304968578942017 accuracy 0.9098143577575684 macro_avg {'precision': 0.9101097806461986, 'recall': 0.9120402751219381, 'f1-score': 0.9096550260447234, 'support': 1131} weighted_avg {'precision': 0.9133610376667175, 'recall': 0.9098143236074271, 'f1-score': 0.9103307712507201, 'support': 1131}
 
----------
Epoch 24/40
time = 227.97 secondes

Train loss 0.05365153830771905 accuracy 0.9894892573356628 macro_avg {'precision': 0.9893995562683207, 'recall': 0.9891529803691327, 'f1-score': 0.9892616496631697, 'support': 10180} weighted_avg {'precision': 0.9895355757176935, 'recall': 0.9894891944990177, 'f1-score': 0.9894970053639878, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.8997549361294762 accuracy 0.8877100348472595 macro_avg {'precision': 0.8961223675132548, 'recall': 0.8898452396494816, 'f1-score': 0.8886728912675205, 'support': 1131} weighted_avg {'precision': 0.9002516888420047, 'recall': 0.887709991158267, 'f1-score': 0.8895546103737937, 'support': 1131}
 
----------
Epoch 25/40
time = 229.18 secondes

Train loss 0.06884385467567208 accuracy 0.9879175424575806 macro_avg {'precision': 0.9881007249012329, 'recall': 0.9878045755330607, 'f1-score': 0.9878976316469028, 'support': 10180} weighted_avg {'precision': 0.9880140330387167, 'recall': 0.9879174852652259, 'f1-score': 0.9879093544605301, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.7622452639785274 accuracy 0.9053934812545776 macro_avg {'precision': 0.9109506100923348, 'recall': 0.9058808934392519, 'f1-score': 0.906222597785673, 'support': 1131} weighted_avg {'precision': 0.9101332914490035, 'recall': 0.905393457117595, 'f1-score': 0.905636935019289, 'support': 1131}
 
----------
Epoch 26/40
time = 224.89 secondes

Train loss 0.04669598382689676 accuracy 0.9904715418815613 macro_avg {'precision': 0.9903606855965357, 'recall': 0.9902953065448491, 'f1-score': 0.9903074043285445, 'support': 10180} weighted_avg {'precision': 0.9905219908011307, 'recall': 0.9904715127701376, 'f1-score': 0.9904757264623179, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.838899769730502 accuracy 0.9009726047515869 macro_avg {'precision': 0.9027889577509572, 'recall': 0.9028489171424686, 'f1-score': 0.9019102911825364, 'support': 1131} weighted_avg {'precision': 0.9035816511704642, 'recall': 0.900972590627763, 'f1-score': 0.9012977390872786, 'support': 1131}
 
----------
Epoch 27/40
time = 220.54 secondes

Train loss 0.048253639850815176 accuracy 0.9902750849723816 macro_avg {'precision': 0.9902211509506772, 'recall': 0.9900475391253266, 'f1-score': 0.9901176830602261, 'support': 10180} weighted_avg {'precision': 0.9903245104191528, 'recall': 0.9902750491159136, 'f1-score': 0.9902824583912908, 'support': 10180}
 
time = 4.92 secondes

Val loss 0.8188495534418059 accuracy 0.8974359035491943 macro_avg {'precision': 0.8996167370438538, 'recall': 0.8988796802695515, 'f1-score': 0.8973164966260102, 'support': 1131} weighted_avg {'precision': 0.9016978949735063, 'recall': 0.8974358974358975, 'f1-score': 0.8976279268177039, 'support': 1131}
 
----------
Epoch 28/40
time = 227.75 secondes

Train loss 0.04910119991081256 accuracy 0.9907662272453308 macro_avg {'precision': 0.9910642110167578, 'recall': 0.9903740859527878, 'f1-score': 0.9906845273192022, 'support': 10180} weighted_avg {'precision': 0.990842298550745, 'recall': 0.9907662082514734, 'f1-score': 0.9907742664398819, 'support': 10180}
 
time = 4.93 secondes

Val loss 0.7500910023962103 accuracy 0.9098143577575684 macro_avg {'precision': 0.9113203372109595, 'recall': 0.9107852622534358, 'f1-score': 0.9101329277309456, 'support': 1131} weighted_avg {'precision': 0.912815214249484, 'recall': 0.9098143236074271, 'f1-score': 0.9103385518353394, 'support': 1131}
 
----------
Epoch 29/40
time = 228.79 secondes

Train loss 0.04040519720335712 accuracy 0.9912574291229248 macro_avg {'precision': 0.9913642692202187, 'recall': 0.9912058417485937, 'f1-score': 0.9912695481930717, 'support': 10180} weighted_avg {'precision': 0.9913131003787603, 'recall': 0.9912573673870334, 'f1-score': 0.9912696984381495, 'support': 10180}
 
time = 4.97 secondes

Val loss 0.8330000724029009 accuracy 0.8992042541503906 macro_avg {'precision': 0.905430585822752, 'recall': 0.9003626740644843, 'f1-score': 0.9005021599755644, 'support': 1131} weighted_avg {'precision': 0.9079238415305372, 'recall': 0.8992042440318302, 'f1-score': 0.9011278410616644, 'support': 1131}
 
----------
Epoch 30/40
time = 226.27 secondes

Train loss 0.04569226762631581 accuracy 0.9903733134269714 macro_avg {'precision': 0.990108543940225, 'recall': 0.9901225401566196, 'f1-score': 0.9900947883886037, 'support': 10180} weighted_avg {'precision': 0.9904593268686983, 'recall': 0.9903732809430256, 'f1-score': 0.9903952546066227, 'support': 10180}
 
time = 5.47 secondes

Val loss 0.8477973503333845 accuracy 0.9036251306533813 macro_avg {'precision': 0.9047870878840808, 'recall': 0.9066408901404245, 'f1-score': 0.9035884451897065, 'support': 1131} weighted_avg {'precision': 0.9089181026891537, 'recall': 0.9036251105216623, 'f1-score': 0.904236218416003, 'support': 1131}
 
----------
Epoch 31/40
time = 224.57 secondes

Train loss 0.03334398646325991 accuracy 0.9929273724555969 macro_avg {'precision': 0.9929487490478703, 'recall': 0.9929615002299419, 'f1-score': 0.9929364254261343, 'support': 10180} weighted_avg {'precision': 0.9929861983202067, 'recall': 0.9929273084479371, 'f1-score': 0.9929375948500194, 'support': 10180}
 
time = 5.11 secondes

Val loss 0.9186351692723561 accuracy 0.8983200788497925 macro_avg {'precision': 0.9063450962680604, 'recall': 0.9018444497598278, 'f1-score': 0.9011079699872366, 'support': 1131} weighted_avg {'precision': 0.9065729024089156, 'recall': 0.8983200707338639, 'f1-score': 0.8993492933011918, 'support': 1131}
 
----------
Epoch 32/40
time = 221.67 secondes

Train loss 0.03458768821879712 accuracy 0.9933202862739563 macro_avg {'precision': 0.9932799816985195, 'recall': 0.993297238708751, 'f1-score': 0.9932778319690531, 'support': 10180} weighted_avg {'precision': 0.9933726437594124, 'recall': 0.9933202357563851, 'f1-score': 0.9933353766791319, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.8209230712272471 accuracy 0.9000884294509888 macro_avg {'precision': 0.9089025711191251, 'recall': 0.9026393712431442, 'f1-score': 0.9027905386655046, 'support': 1131} weighted_avg {'precision': 0.9089723588301702, 'recall': 0.9000884173297966, 'f1-score': 0.901483429862932, 'support': 1131}
 
----------
Epoch 33/40
time = 227.43 secondes

Train loss 0.02862371812961596 accuracy 0.9942043423652649 macro_avg {'precision': 0.9944816104608714, 'recall': 0.9941365309684848, 'f1-score': 0.9942761594640492, 'support': 10180} weighted_avg {'precision': 0.9943133566710758, 'recall': 0.9942043222003929, 'f1-score': 0.994225352916464, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.8116327364196608 accuracy 0.9080460071563721 macro_avg {'precision': 0.9142588168705018, 'recall': 0.9096550472959442, 'f1-score': 0.9097076622140007, 'support': 1131} weighted_avg {'precision': 0.9148225489906766, 'recall': 0.9080459770114943, 'f1-score': 0.9090973226349927, 'support': 1131}
 
----------
Epoch 34/40
time = 226.97 secondes

Train loss 0.02521852284292463 accuracy 0.9942043423652649 macro_avg {'precision': 0.9943079446592116, 'recall': 0.9941753379652468, 'f1-score': 0.9942305908738488, 'support': 10180} weighted_avg {'precision': 0.9942500748773712, 'recall': 0.9942043222003929, 'f1-score': 0.9942159465315707, 'support': 10180}
 
time = 5.45 secondes

Val loss 0.7971520368156862 accuracy 0.9080460071563721 macro_avg {'precision': 0.9113842177442665, 'recall': 0.9105245797907674, 'f1-score': 0.90960286907002, 'support': 1131} weighted_avg {'precision': 0.9119610047778623, 'recall': 0.9080459770114943, 'f1-score': 0.9086009590226772, 'support': 1131}
 
----------
Epoch 35/40
time = 225.43 secondes

Train loss 0.018866839215725708 accuracy 0.9951866865158081 macro_avg {'precision': 0.9951232633621314, 'recall': 0.9950397810292939, 'f1-score': 0.9950685264673617, 'support': 10180} weighted_avg {'precision': 0.9952266717737993, 'recall': 0.9951866404715127, 'f1-score': 0.9951928399244035, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.7989277140160327 accuracy 0.9142352342605591 macro_avg {'precision': 0.9173509888765119, 'recall': 0.9160262924155391, 'f1-score': 0.9158541140914297, 'support': 1131} weighted_avg {'precision': 0.9168808775656581, 'recall': 0.9142351900972591, 'f1-score': 0.9146711219526091, 'support': 1131}
 
----------
Epoch 36/40
time = 221.23 secondes

Train loss 0.02088680397727426 accuracy 0.9951866865158081 macro_avg {'precision': 0.9954519481130799, 'recall': 0.9952458143698883, 'f1-score': 0.9953321567888818, 'support': 10180} weighted_avg {'precision': 0.9952480568007868, 'recall': 0.9951866404715127, 'f1-score': 0.9952000124525677, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.8447996072806656 accuracy 0.9027409553527832 macro_avg {'precision': 0.9062595226531425, 'recall': 0.9058737963831083, 'f1-score': 0.9050719168720965, 'support': 1131} weighted_avg {'precision': 0.9062865891098957, 'recall': 0.9027409372236959, 'f1-score': 0.9034694407363894, 'support': 1131}
 
----------
Epoch 37/40
time = 228.47 secondes

Train loss 0.019834817239222976 accuracy 0.9956778287887573 macro_avg {'precision': 0.9959121865827083, 'recall': 0.9956057905005278, 'f1-score': 0.9957418564129732, 'support': 10180} weighted_avg {'precision': 0.9957291621589945, 'recall': 0.9956777996070727, 'f1-score': 0.9956858700314872, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.8075849248731354 accuracy 0.9045093059539795 macro_avg {'precision': 0.907721606761308, 'recall': 0.907030334788311, 'f1-score': 0.9062837744646124, 'support': 1131} weighted_avg {'precision': 0.9084663001140592, 'recall': 0.9045092838196287, 'f1-score': 0.9053500296492316, 'support': 1131}
 
----------
Epoch 38/40
time = 228.05 secondes

Train loss 0.015428862647397804 accuracy 0.9960707426071167 macro_avg {'precision': 0.9963027291292768, 'recall': 0.9961585284586338, 'f1-score': 0.9962075505356163, 'support': 10180} weighted_avg {'precision': 0.9961577953783604, 'recall': 0.9960707269155207, 'f1-score': 0.996089595254898, 'support': 10180}
 
time = 4.94 secondes

Val loss 0.7738343446110865 accuracy 0.9098143577575684 macro_avg {'precision': 0.910970303896218, 'recall': 0.9094090055389298, 'f1-score': 0.90954912726053, 'support': 1131} weighted_avg {'precision': 0.911138252714741, 'recall': 0.9098143236074271, 'f1-score': 0.9098512444765454, 'support': 1131}
 
----------
Epoch 39/40
time = 219.33 secondes

Train loss 0.011648876173695304 accuracy 0.9961689710617065 macro_avg {'precision': 0.996380501976392, 'recall': 0.9961833137639962, 'f1-score': 0.9962734888983846, 'support': 10180} weighted_avg {'precision': 0.9962084242911655, 'recall': 0.9961689587426326, 'f1-score': 0.996180001730347, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.7964345079902377 accuracy 0.9160035848617554 macro_avg {'precision': 0.9186972026975244, 'recall': 0.9173179568329168, 'f1-score': 0.9168545876906293, 'support': 1131} weighted_avg {'precision': 0.9186686632276271, 'recall': 0.9160035366931919, 'f1-score': 0.9161764901430488, 'support': 1131}
 
----------
Epoch 40/40
time = 229.04 secondes

Train loss 0.007985838912719949 accuracy 0.9972495436668396 macro_avg {'precision': 0.9974308593084042, 'recall': 0.9973055308223422, 'f1-score': 0.9973463125709001, 'support': 10180} weighted_avg {'precision': 0.9973269233538239, 'recall': 0.9972495088408644, 'f1-score': 0.9972655025716816, 'support': 10180}
 
time = 4.96 secondes

Val loss 0.7818349600403681 accuracy 0.9160035848617554 macro_avg {'precision': 0.9185465000697303, 'recall': 0.9164383274432799, 'f1-score': 0.9164584660744604, 'support': 1131} weighted_avg {'precision': 0.9187100585206788, 'recall': 0.9160035366931919, 'f1-score': 0.9163272445950416, 'support': 1131}
 
----------
best_accuracy 0.9160035848617554 best_epoch 39 macro_avg {'precision': 0.9186972026975244, 'recall': 0.9173179568329168, 'f1-score': 0.9168545876906293, 'support': 1131} weighted_avg {'precision': 0.9186686632276271, 'recall': 0.9160035366931919, 'f1-score': 0.9161764901430488, 'support': 1131}

average train time 239.19847306013108

average val time 5.636779999732971
 
time = 4.94 secondes

test_accuracy 0.13758300244808197 macro_avg {'precision': 0.9186972026975244, 'recall': 0.9173179568329168, 'f1-score': 0.9168545876906293, 'support': 1131} weighted_avg {'precision': 0.9186686632276271, 'recall': 0.9160035366931919, 'f1-score': 0.9161764901430488, 'support': 1131}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: NVIDIA A100 80GB PCIe
##########
20newsgroups_BERT_tail_1
----------
Epoch 1/40
time = 226.54 secondes

Train loss 1.3489243150038097 accuracy 0.6430255770683289 macro_avg {'precision': 0.6421288794942742, 'recall': 0.627617718201417, 'f1-score': 0.6178057284980474, 'support': 10180} weighted_avg {'precision': 0.6509498122980395, 'recall': 0.6430255402750491, 'f1-score': 0.6319110890174966, 'support': 10180}
 
time = 5.19 secondes

Val loss 0.7758146591589484 accuracy 0.7745358347892761 macro_avg {'precision': 0.75000800233174, 'recall': 0.7602371843579887, 'f1-score': 0.7471252833912481, 'support': 1131} weighted_avg {'precision': 0.7606594079774615, 'recall': 0.7745358090185677, 'f1-score': 0.7608244911943329, 'support': 1131}
 
----------
Epoch 2/40
time = 222.46 secondes

Train loss 0.521368660874887 accuracy 0.8458743095397949 macro_avg {'precision': 0.8385298594102263, 'recall': 0.833408622153309, 'f1-score': 0.8303423101433663, 'support': 10180} weighted_avg {'precision': 0.8452379922203637, 'recall': 0.8458742632612967, 'f1-score': 0.8414598171832356, 'support': 10180}
 
time = 4.99 secondes

Val loss 0.5787412159039941 accuracy 0.8470380306243896 macro_avg {'precision': 0.8483921330110376, 'recall': 0.8412065545377521, 'f1-score': 0.8398355723705461, 'support': 1131} weighted_avg {'precision': 0.8554539937089262, 'recall': 0.8470380194518126, 'f1-score': 0.8462296612554556, 'support': 1131}
 
----------
Epoch 3/40
time = 221.71 secondes

Train loss 0.33317013077870644 accuracy 0.9026522636413574 macro_avg {'precision': 0.8952105299670295, 'recall': 0.8942552386056724, 'f1-score': 0.894223643688594, 'support': 10180} weighted_avg {'precision': 0.9017756803812275, 'recall': 0.9026522593320235, 'f1-score': 0.9017819106640604, 'support': 10180}
 
time = 4.95 secondes

Val loss 0.6776986181945868 accuracy 0.8213970065116882 macro_avg {'precision': 0.8365735688714768, 'recall': 0.8265020282609102, 'f1-score': 0.8186348264921259, 'support': 1131} weighted_avg {'precision': 0.8477642113309525, 'recall': 0.8213969938107869, 'f1-score': 0.8200390696817704, 'support': 1131}
 
----------
Epoch 4/40
time = 218.42 secondes

Train loss 0.255845244715116 accuracy 0.9268172979354858 macro_avg {'precision': 0.9208046528829978, 'recall': 0.9205095520887074, 'f1-score': 0.9205667624318961, 'support': 10180} weighted_avg {'precision': 0.9271619772355103, 'recall': 0.9268172888015717, 'f1-score': 0.9268979039378227, 'support': 10180}
 
time = 5.09 secondes

Val loss 0.6254286199388369 accuracy 0.8673740029335022 macro_avg {'precision': 0.8792591765729364, 'recall': 0.8637533153584229, 'f1-score': 0.8658131216191155, 'support': 1131} weighted_avg {'precision': 0.8825757846965222, 'recall': 0.8673740053050398, 'f1-score': 0.8695200148646032, 'support': 1131}
 
----------
Epoch 5/40
time = 314.75 secondes

Train loss 0.20657903508369457 accuracy 0.9433202743530273 macro_avg {'precision': 0.9392027986868456, 'recall': 0.9385458103494498, 'f1-score': 0.9387896667919573, 'support': 10180} weighted_avg {'precision': 0.9430749110385411, 'recall': 0.943320235756385, 'f1-score': 0.9431283476541924, 'support': 10180}
 
time = 7.04 secondes

Val loss 0.672461210638547 accuracy 0.8541114330291748 macro_avg {'precision': 0.8716026144745486, 'recall': 0.8444999840672773, 'f1-score': 0.8490088933331771, 'support': 1131} weighted_avg {'precision': 0.8753179829366801, 'recall': 0.8541114058355438, 'f1-score': 0.8568509750752331, 'support': 1131}
 
----------
Epoch 6/40
time = 347.03 secondes

Train loss 0.17028248679803704 accuracy 0.9548134207725525 macro_avg {'precision': 0.9522288088086311, 'recall': 0.9517152591918678, 'f1-score': 0.9519229210603992, 'support': 10180} weighted_avg {'precision': 0.9548098169774213, 'recall': 0.9548133595284872, 'f1-score': 0.9547642817073733, 'support': 10180}
 
time = 6.63 secondes

Val loss 0.6883938733560466 accuracy 0.8717948794364929 macro_avg {'precision': 0.878424179663788, 'recall': 0.8647061282609595, 'f1-score': 0.8641684931536682, 'support': 1131} weighted_avg {'precision': 0.8804767603934021, 'recall': 0.8717948717948718, 'f1-score': 0.870273454863472, 'support': 1131}
 
----------
Epoch 7/40
time = 343.87 secondes

Train loss 0.16186950531295827 accuracy 0.9609037637710571 macro_avg {'precision': 0.9589101317928843, 'recall': 0.9592509368761217, 'f1-score': 0.9590157995436174, 'support': 10180} weighted_avg {'precision': 0.9611629442939975, 'recall': 0.9609037328094302, 'f1-score': 0.9609778402334871, 'support': 10180}
 
time = 6.92 secondes

Val loss 0.7452749495892982 accuracy 0.8744474053382874 macro_avg {'precision': 0.8870712567436998, 'recall': 0.8708877699045511, 'f1-score': 0.870058914382289, 'support': 1131} weighted_avg {'precision': 0.887131825930881, 'recall': 0.874447391688771, 'f1-score': 0.8732031753903244, 'support': 1131}
 
----------
Epoch 8/40
time = 342.54 secondes

Train loss 0.14609922794159136 accuracy 0.9669941663742065 macro_avg {'precision': 0.965498855336565, 'recall': 0.9651873170664246, 'f1-score': 0.9653090036195202, 'support': 10180} weighted_avg {'precision': 0.9670849877664451, 'recall': 0.9669941060903733, 'f1-score': 0.9670052567446684, 'support': 10180}
 
time = 6.57 secondes

Val loss 0.7225840872868529 accuracy 0.8850575089454651 macro_avg {'precision': 0.8889468285730352, 'recall': 0.883290609237523, 'f1-score': 0.8841883676728163, 'support': 1131} weighted_avg {'precision': 0.8896215248192892, 'recall': 0.8850574712643678, 'f1-score': 0.8854260937751661, 'support': 1131}
 
----------
Epoch 9/40
time = 347.23 secondes

Train loss 0.127357701207343 accuracy 0.9706287384033203 macro_avg {'precision': 0.9693280663840769, 'recall': 0.9689698939772136, 'f1-score': 0.9691219976184067, 'support': 10180} weighted_avg {'precision': 0.9707056290739128, 'recall': 0.9706286836935167, 'f1-score': 0.9706409126181181, 'support': 10180}
 
time = 6.99 secondes

Val loss 0.8895643739346218 accuracy 0.8709107041358948 macro_avg {'precision': 0.875048930939213, 'recall': 0.8701097851238876, 'f1-score': 0.8689064108419371, 'support': 1131} weighted_avg {'precision': 0.8768840070323547, 'recall': 0.8709106984969054, 'f1-score': 0.8705144074429776, 'support': 1131}
 
----------
Epoch 10/40
time = 338.06 secondes

Train loss 0.13171681130604287 accuracy 0.9712181091308594 macro_avg {'precision': 0.9695308821620265, 'recall': 0.9695057654292751, 'f1-score': 0.9694699342234332, 'support': 10180} weighted_avg {'precision': 0.9712888962004141, 'recall': 0.9712180746561886, 'f1-score': 0.9712047308802267, 'support': 10180}
 
time = 6.50 secondes

Val loss 0.9008327582080415 accuracy 0.8682581782341003 macro_avg {'precision': 0.8777686010529651, 'recall': 0.8675006510766492, 'f1-score': 0.8684263234655301, 'support': 1131} weighted_avg {'precision': 0.8796988120918926, 'recall': 0.8682581786030061, 'f1-score': 0.8697964434491892, 'support': 1131}
 
----------
Epoch 11/40
time = 346.10 secondes

Train loss 0.14095627630899407 accuracy 0.9704322814941406 macro_avg {'precision': 0.9697194637314457, 'recall': 0.9691610341797071, 'f1-score': 0.9693789172296354, 'support': 10180} weighted_avg {'precision': 0.9705989528786433, 'recall': 0.9704322200392927, 'f1-score': 0.9704540010398871, 'support': 10180}
 
time = 6.64 secondes

Val loss 1.0943122183007161 accuracy 0.8479222059249878 macro_avg {'precision': 0.8609070938752913, 'recall': 0.8445379342824794, 'f1-score': 0.8439568283937138, 'support': 1131} weighted_avg {'precision': 0.8673195651383739, 'recall': 0.847922192749779, 'f1-score': 0.8496813027368092, 'support': 1131}
 
----------
Epoch 12/40
time = 344.24 secondes

Train loss 0.10980969944565296 accuracy 0.9769155383110046 macro_avg {'precision': 0.9758003116921978, 'recall': 0.9750787929950515, 'f1-score': 0.9753798646741132, 'support': 10180} weighted_avg {'precision': 0.9769894327766383, 'recall': 0.9769155206286837, 'f1-score': 0.9768972719478504, 'support': 10180}
 
time = 6.32 secondes

Val loss 1.0196869289686963 accuracy 0.8488063812255859 macro_avg {'precision': 0.883451388958591, 'recall': 0.8319673666350363, 'f1-score': 0.8267229579123818, 'support': 1131} weighted_avg {'precision': 0.8820107856920495, 'recall': 0.8488063660477454, 'f1-score': 0.8427118006217226, 'support': 1131}
 
----------
Epoch 13/40
time = 350.49 secondes

Train loss 0.1128355105254204 accuracy 0.9773085117340088 macro_avg {'precision': 0.9767954944489647, 'recall': 0.9756822156304313, 'f1-score': 0.9761394215462909, 'support': 10180} weighted_avg {'precision': 0.9773298020469027, 'recall': 0.9773084479371317, 'f1-score': 0.9772386564950272, 'support': 10180}
 
time = 7.07 secondes

Val loss 0.8960464345549503 accuracy 0.8770999312400818 macro_avg {'precision': 0.8894786579019118, 'recall': 0.8763326275085029, 'f1-score': 0.8784585679051634, 'support': 1131} weighted_avg {'precision': 0.8898884062470683, 'recall': 0.8770999115826702, 'f1-score': 0.8789525042824008, 'support': 1131}
 
----------
Epoch 14/40
time = 337.27 secondes

Train loss 0.10030724507144939 accuracy 0.9798625111579895 macro_avg {'precision': 0.9791817066232756, 'recall': 0.9793281664785841, 'f1-score': 0.979218995032598, 'support': 10180} weighted_avg {'precision': 0.9799695932203086, 'recall': 0.9798624754420432, 'f1-score': 0.9798797325624639, 'support': 10180}
 
time = 4.96 secondes

Val loss 1.125699181934617 accuracy 0.8629531860351562 macro_avg {'precision': 0.8780962420504835, 'recall': 0.8642917652792729, 'f1-score': 0.8641117295572002, 'support': 1131} weighted_avg {'precision': 0.8799734849164909, 'recall': 0.8629531388152077, 'f1-score': 0.8648294550014092, 'support': 1131}
 
----------
Epoch 15/40
time = 354.74 secondes

Train loss 0.11156589062200944 accuracy 0.97956782579422 macro_avg {'precision': 0.9795319670838589, 'recall': 0.9790922798101986, 'f1-score': 0.9792713301731559, 'support': 10180} weighted_avg {'precision': 0.9796431421932243, 'recall': 0.9795677799607073, 'f1-score': 0.9795672968623881, 'support': 10180}
 
time = 6.82 secondes

Val loss 0.81810059694247 accuracy 0.8885942101478577 macro_avg {'precision': 0.8908429711693246, 'recall': 0.8900717827476585, 'f1-score': 0.8893721345769612, 'support': 1131} weighted_avg {'precision': 0.8930165866631322, 'recall': 0.8885941644562334, 'f1-score': 0.8896367475421361, 'support': 1131}
 
----------
Epoch 16/40
time = 331.61 secondes

Train loss 0.09246844603550135 accuracy 0.9805501103401184 macro_avg {'precision': 0.979494382709808, 'recall': 0.9796676349250607, 'f1-score': 0.9795413756838259, 'support': 10180} weighted_avg {'precision': 0.980649905375814, 'recall': 0.9805500982318271, 'f1-score': 0.9805641489460691, 'support': 10180}
 
time = 6.26 secondes

Val loss 0.9968213571076081 accuracy 0.8735632300376892 macro_avg {'precision': 0.8825128757582948, 'recall': 0.8682393284729182, 'f1-score': 0.872330319722634, 'support': 1131} weighted_avg {'precision': 0.8782025926929167, 'recall': 0.8735632183908046, 'f1-score': 0.873183065405861, 'support': 1131}
 
----------
Epoch 17/40
time = 363.60 secondes

Train loss 0.09139926127953824 accuracy 0.9825147986412048 macro_avg {'precision': 0.9825634708924117, 'recall': 0.9819122690739563, 'f1-score': 0.9821866367454017, 'support': 10180} weighted_avg {'precision': 0.9825751952251324, 'recall': 0.9825147347740668, 'f1-score': 0.9824962575304943, 'support': 10180}
 
time = 7.27 secondes

Val loss 0.9749284598342648 accuracy 0.8824049830436707 macro_avg {'precision': 0.8894411560592751, 'recall': 0.8823937404856419, 'f1-score': 0.8836107777059322, 'support': 1131} weighted_avg {'precision': 0.888099896738047, 'recall': 0.8824049513704686, 'f1-score': 0.8829504098207234, 'support': 1131}
 
----------
Epoch 18/40
time = 324.30 secondes

Train loss 0.08686089571830842 accuracy 0.9827112555503845 macro_avg {'precision': 0.9819152627215549, 'recall': 0.9821416728448987, 'f1-score': 0.9820004040409083, 'support': 10180} weighted_avg {'precision': 0.9827981628575785, 'recall': 0.9827111984282908, 'f1-score': 0.9827271615041061, 'support': 10180}
 
time = 6.96 secondes

Val loss 1.0246888230895808 accuracy 0.8682581782341003 macro_avg {'precision': 0.8796054314920718, 'recall': 0.8615121442793752, 'f1-score': 0.8656352130807099, 'support': 1131} weighted_avg {'precision': 0.8754061378068387, 'recall': 0.8682581786030061, 'f1-score': 0.867751566797979, 'support': 1131}
 
----------
Epoch 19/40
time = 362.00 secondes

Train loss 0.07095039073642204 accuracy 0.9850687980651855 macro_avg {'precision': 0.9849483549875346, 'recall': 0.9844255471296979, 'f1-score': 0.9846673895830357, 'support': 10180} weighted_avg {'precision': 0.9851177200917575, 'recall': 0.9850687622789784, 'f1-score': 0.9850752768426996, 'support': 10180}
 
time = 6.91 secondes

Val loss 0.932970997787853 accuracy 0.8850575089454651 macro_avg {'precision': 0.8900006524080639, 'recall': 0.8856032437631555, 'f1-score': 0.8860990150664115, 'support': 1131} weighted_avg {'precision': 0.889187897909242, 'recall': 0.8850574712643678, 'f1-score': 0.8855394251248927, 'support': 1131}
 
----------
Epoch 20/40
time = 326.47 secondes

Train loss 0.08086720255064742 accuracy 0.9840864539146423 macro_avg {'precision': 0.984225440315796, 'recall': 0.98412945120714, 'f1-score': 0.9841606624702999, 'support': 10180} weighted_avg {'precision': 0.9841535944108427, 'recall': 0.9840864440078585, 'f1-score': 0.9841027851524513, 'support': 10180}
 
time = 7.00 secondes

Val loss 0.8382557904065757 accuracy 0.8868258595466614 macro_avg {'precision': 0.8937517136811677, 'recall': 0.8864329898759735, 'f1-score': 0.8889319860648108, 'support': 1131} weighted_avg {'precision': 0.8909117475779937, 'recall': 0.8868258178603006, 'f1-score': 0.8876983903234494, 'support': 1131}
 
----------
Epoch 21/40
time = 360.89 secondes

Train loss 0.06781369714104125 accuracy 0.9856581687927246 macro_avg {'precision': 0.984882660550306, 'recall': 0.9849018074520508, 'f1-score': 0.9848754009762463, 'support': 10180} weighted_avg {'precision': 0.9856840769055824, 'recall': 0.9856581532416503, 'f1-score': 0.9856552798096522, 'support': 10180}
 
time = 6.83 secondes

Val loss 0.9620458739951112 accuracy 0.8859416842460632 macro_avg {'precision': 0.8867773504875327, 'recall': 0.8876109730846732, 'f1-score': 0.8855784410607942, 'support': 1131} weighted_avg {'precision': 0.8912661499655908, 'recall': 0.8859416445623343, 'f1-score': 0.8871171983485193, 'support': 1131}
 
----------
Epoch 22/40
time = 325.61 secondes

Train loss 0.06566729069553055 accuracy 0.9868369698524475 macro_avg {'precision': 0.9864934286855072, 'recall': 0.9866543366610081, 'f1-score': 0.986556726314355, 'support': 10180} weighted_avg {'precision': 0.9869121948662557, 'recall': 0.9868369351669941, 'f1-score': 0.986857794928184, 'support': 10180}
 
time = 6.96 secondes

Val loss 0.9912984138118959 accuracy 0.8806366324424744 macro_avg {'precision': 0.8854272215162717, 'recall': 0.8801815650527247, 'f1-score': 0.8807829856767464, 'support': 1131} weighted_avg {'precision': 0.8847451266936038, 'recall': 0.8806366047745358, 'f1-score': 0.8807622955990803, 'support': 1131}
 
----------
Epoch 23/40
time = 364.47 secondes

Train loss 0.07242751247508143 accuracy 0.986542284488678 macro_avg {'precision': 0.9863646840677897, 'recall': 0.9865145720307883, 'f1-score': 0.9864053564130575, 'support': 10180} weighted_avg {'precision': 0.9866622110660136, 'recall': 0.9865422396856581, 'f1-score': 0.9865687222719681, 'support': 10180}
 
time = 6.43 secondes

Val loss 0.9652780971344738 accuracy 0.8859416842460632 macro_avg {'precision': 0.8917685696217206, 'recall': 0.8872248046355897, 'f1-score': 0.8866282462366802, 'support': 1131} weighted_avg {'precision': 0.8914695411922474, 'recall': 0.8859416445623343, 'f1-score': 0.8860249671710755, 'support': 1131}
 
----------
Epoch 24/40
time = 327.49 secondes

Train loss 0.054639035644905395 accuracy 0.9888015985488892 macro_avg {'precision': 0.9887712437095244, 'recall': 0.9882845994856163, 'f1-score': 0.9884972629714017, 'support': 10180} weighted_avg {'precision': 0.9888922767354094, 'recall': 0.9888015717092338, 'f1-score': 0.9888173555742619, 'support': 10180}
 
time = 7.05 secondes

Val loss 0.8393578696829959 accuracy 0.8974359035491943 macro_avg {'precision': 0.9023467855831852, 'recall': 0.8967685802892129, 'f1-score': 0.8982957864187675, 'support': 1131} weighted_avg {'precision': 0.9010553049962307, 'recall': 0.8974358974358975, 'f1-score': 0.8980198609313952, 'support': 1131}
 
----------
Epoch 25/40
time = 359.06 secondes

Train loss 0.051449816830027376 accuracy 0.9894892573356628 macro_avg {'precision': 0.989058708389335, 'recall': 0.9892457895371984, 'f1-score': 0.989129213326469, 'support': 10180} weighted_avg {'precision': 0.9895576767130893, 'recall': 0.9894891944990177, 'f1-score': 0.9895003300976426, 'support': 10180}
 
time = 6.58 secondes

Val loss 0.8805852823965145 accuracy 0.8894783854484558 macro_avg {'precision': 0.8912707669471679, 'recall': 0.8901857226041727, 'f1-score': 0.8900789217994978, 'support': 1131} weighted_avg {'precision': 0.8918943106089748, 'recall': 0.8894783377541998, 'f1-score': 0.8900207211252017, 'support': 1131}
 
----------
Epoch 26/40
time = 326.26 secondes

Train loss 0.05084770732452626 accuracy 0.9892927408218384 macro_avg {'precision': 0.9890248571982931, 'recall': 0.9890405044818149, 'f1-score': 0.9890206076678458, 'support': 10180} weighted_avg {'precision': 0.9893460783752409, 'recall': 0.9892927308447937, 'f1-score': 0.9893070732703951, 'support': 10180}
 
time = 6.58 secondes

Val loss 0.9315305006036307 accuracy 0.8965517282485962 macro_avg {'precision': 0.9015298198202115, 'recall': 0.896769279738235, 'f1-score': 0.8973888811980236, 'support': 1131} weighted_avg {'precision': 0.902968738639918, 'recall': 0.896551724137931, 'f1-score': 0.898132012489229, 'support': 1131}
 
----------
Epoch 27/40
time = 360.75 secondes

Train loss 0.04783191157995181 accuracy 0.9904715418815613 macro_avg {'precision': 0.9905982652115032, 'recall': 0.9902803853682383, 'f1-score': 0.9904275346971444, 'support': 10180} weighted_avg {'precision': 0.9905062993100263, 'recall': 0.9904715127701376, 'f1-score': 0.9904784120618537, 'support': 10180}
 
time = 6.37 secondes

Val loss 0.9155300832203401 accuracy 0.8974359035491943 macro_avg {'precision': 0.9015185003674647, 'recall': 0.8992477884753848, 'f1-score': 0.8987314937505471, 'support': 1131} weighted_avg {'precision': 0.9030542815295869, 'recall': 0.8974358974358975, 'f1-score': 0.8985162753516055, 'support': 1131}
 
----------
Epoch 28/40
time = 329.64 secondes

Train loss 0.048003880169663275 accuracy 0.9905697703361511 macro_avg {'precision': 0.9905387971666533, 'recall': 0.9904264888677654, 'f1-score': 0.9904750497856932, 'support': 10180} weighted_avg {'precision': 0.9906082819241728, 'recall': 0.9905697445972496, 'f1-score': 0.9905815981636139, 'support': 10180}
 
time = 7.15 secondes

Val loss 1.0196348088079183 accuracy 0.8806366324424744 macro_avg {'precision': 0.8907818022729508, 'recall': 0.8821114059544012, 'f1-score': 0.8825834224312649, 'support': 1131} weighted_avg {'precision': 0.8916232984679389, 'recall': 0.8806366047745358, 'f1-score': 0.882560635605037, 'support': 1131}
 
----------
Epoch 29/40
time = 365.06 secondes

Train loss 0.03892400893020529 accuracy 0.9921414852142334 macro_avg {'precision': 0.9922916881281709, 'recall': 0.9921623635719241, 'f1-score': 0.9922133486455014, 'support': 10180} weighted_avg {'precision': 0.9921769288306508, 'recall': 0.9921414538310412, 'f1-score': 0.9921448602438852, 'support': 10180}
 
time = 6.70 secondes

Val loss 0.9976281443624417 accuracy 0.8938992023468018 macro_avg {'precision': 0.8936042161374985, 'recall': 0.8961109086596594, 'f1-score': 0.8934220445438396, 'support': 1131} weighted_avg {'precision': 0.8982923028560362, 'recall': 0.8938992042440318, 'f1-score': 0.8946164165963522, 'support': 1131}
 
----------
Epoch 30/40
time = 329.77 secondes

Train loss 0.04481049274154872 accuracy 0.9913556575775146 macro_avg {'precision': 0.9912208567695012, 'recall': 0.9911016406377151, 'f1-score': 0.9911273800319245, 'support': 10180} weighted_avg {'precision': 0.9914529965233629, 'recall': 0.9913555992141454, 'f1-score': 0.9913694277705041, 'support': 10180}
 
time = 7.10 secondes

Val loss 1.0369079370235776 accuracy 0.8850575089454651 macro_avg {'precision': 0.8877768848975581, 'recall': 0.8851425027648021, 'f1-score': 0.8835113958714901, 'support': 1131} weighted_avg {'precision': 0.8890580015219332, 'recall': 0.8850574712643678, 'f1-score': 0.8842922021214711, 'support': 1131}
 
----------
Epoch 31/40
time = 355.72 secondes

Train loss 0.0439269173084697 accuracy 0.9913556575775146 macro_avg {'precision': 0.9914921280643686, 'recall': 0.9913212798113058, 'f1-score': 0.9913882156414697, 'support': 10180} weighted_avg {'precision': 0.9914130186708191, 'recall': 0.9913555992141454, 'f1-score': 0.9913649084729894, 'support': 10180}
 
time = 6.64 secondes

Val loss 0.9926831215075061 accuracy 0.8938992023468018 macro_avg {'precision': 0.8957560889330874, 'recall': 0.8958615255140682, 'f1-score': 0.8944325265577374, 'support': 1131} weighted_avg {'precision': 0.8994458509502559, 'recall': 0.8938992042440318, 'f1-score': 0.89522616572068, 'support': 1131}
 
----------
Epoch 32/40
time = 336.89 secondes

Train loss 0.03770167604874747 accuracy 0.9932220578193665 macro_avg {'precision': 0.9934937621799278, 'recall': 0.9932784437010291, 'f1-score': 0.9933550334594783, 'support': 10180} weighted_avg {'precision': 0.9933264826210664, 'recall': 0.9932220039292731, 'f1-score': 0.9932411255585822, 'support': 10180}
 
time = 7.05 secondes

Val loss 1.0063975043302773 accuracy 0.8894783854484558 macro_avg {'precision': 0.8885513320453174, 'recall': 0.8909806807443597, 'f1-score': 0.8878206009604852, 'support': 1131} weighted_avg {'precision': 0.8939063296705356, 'recall': 0.8894783377541998, 'f1-score': 0.88997106718462, 'support': 1131}
 
----------
Epoch 33/40
time = 352.21 secondes

Train loss 0.03907526146649673 accuracy 0.9927308559417725 macro_avg {'precision': 0.992978808862024, 'recall': 0.9928554928470881, 'f1-score': 0.9928995954528359, 'support': 10180} weighted_avg {'precision': 0.9927789501276382, 'recall': 0.9927308447937132, 'f1-score': 0.9927365893382609, 'support': 10180}
 
time = 6.56 secondes

Val loss 0.9359816704767786 accuracy 0.895667552947998 macro_avg {'precision': 0.898793766775429, 'recall': 0.8956846207952054, 'f1-score': 0.8959888733572343, 'support': 1131} weighted_avg {'precision': 0.8998084909841604, 'recall': 0.8956675508399646, 'f1-score': 0.8963926012696638, 'support': 1131}
 
----------
Epoch 34/40
time = 339.11 secondes

Train loss 0.0365170104427738 accuracy 0.9921414852142334 macro_avg {'precision': 0.992020480105498, 'recall': 0.9911174842144235, 'f1-score': 0.991507823157846, 'support': 10180} weighted_avg {'precision': 0.9922295456101419, 'recall': 0.9921414538310412, 'f1-score': 0.9921356106544496, 'support': 10180}
 
time = 6.88 secondes

Val loss 0.8513703137880287 accuracy 0.9018567800521851 macro_avg {'precision': 0.9037673402710775, 'recall': 0.9020755303882959, 'f1-score': 0.902156519351345, 'support': 1131} weighted_avg {'precision': 0.9031024797913364, 'recall': 0.9018567639257294, 'f1-score': 0.9016870953449694, 'support': 1131}
 
----------
Epoch 35/40
time = 353.47 secondes

Train loss 0.03144030351301424 accuracy 0.9934185147285461 macro_avg {'precision': 0.9936730753477582, 'recall': 0.9934571194609456, 'f1-score': 0.9935489646639892, 'support': 10180} weighted_avg {'precision': 0.993478535596071, 'recall': 0.9934184675834971, 'f1-score': 0.9934313262401807, 'support': 10180}
 
time = 6.91 secondes

Val loss 0.8801687135954279 accuracy 0.895667552947998 macro_avg {'precision': 0.8998614673653387, 'recall': 0.8971986915577685, 'f1-score': 0.8977737648634356, 'support': 1131} weighted_avg {'precision': 0.8986606127283984, 'recall': 0.8956675508399646, 'f1-score': 0.8963693659828741, 'support': 1131}
 
----------
Epoch 36/40
time = 339.20 secondes

Train loss 0.02605929943200573 accuracy 0.9943025708198547 macro_avg {'precision': 0.9945424375026398, 'recall': 0.9944098799144531, 'f1-score': 0.9944448282256154, 'support': 10180} weighted_avg {'precision': 0.9943938556127029, 'recall': 0.9943025540275049, 'f1-score': 0.9943150072537363, 'support': 10180}
 
time = 6.86 secondes

Val loss 0.8958294073535411 accuracy 0.8983200788497925 macro_avg {'precision': 0.9036989335302525, 'recall': 0.8974063747881257, 'f1-score': 0.8997038877968052, 'support': 1131} weighted_avg {'precision': 0.9012548333185598, 'recall': 0.8983200707338639, 'f1-score': 0.898969868018695, 'support': 1131}
 
----------
Epoch 37/40
time = 349.47 secondes

Train loss 0.020250570979497957 accuracy 0.9951866865158081 macro_avg {'precision': 0.9953975441842268, 'recall': 0.9949213973348744, 'f1-score': 0.9951246487256279, 'support': 10180} weighted_avg {'precision': 0.9952860728722879, 'recall': 0.9951866404715127, 'f1-score': 0.9952018095533273, 'support': 10180}
 
time = 6.92 secondes

Val loss 0.8724849622370091 accuracy 0.9018567800521851 macro_avg {'precision': 0.9050115127957591, 'recall': 0.9029612056044847, 'f1-score': 0.9030105914536064, 'support': 1131} weighted_avg {'precision': 0.9059081073959423, 'recall': 0.9018567639257294, 'f1-score': 0.9029382908275405, 'support': 1131}
 
----------
Epoch 38/40
time = 343.02 secondes

Train loss 0.017445575089328525 accuracy 0.9954813718795776 macro_avg {'precision': 0.9955781846129639, 'recall': 0.9953513052480412, 'f1-score': 0.995435398341346, 'support': 10180} weighted_avg {'precision': 0.9955753141523896, 'recall': 0.9954813359528487, 'f1-score': 0.9954978078590829, 'support': 10180}
 
time = 6.97 secondes

Val loss 0.8989506793590613 accuracy 0.9009726047515869 macro_avg {'precision': 0.9032019628831842, 'recall': 0.8989056670335513, 'f1-score': 0.9000123790594088, 'support': 1131} weighted_avg {'precision': 0.9029421378729603, 'recall': 0.900972590627763, 'f1-score': 0.9009987880498853, 'support': 1131}
 
----------
Epoch 39/40
time = 345.28 secondes

Train loss 0.013429555245030134 accuracy 0.9965619444847107 macro_avg {'precision': 0.9968008268091776, 'recall': 0.9965327553867176, 'f1-score': 0.9966416945206007, 'support': 10180} weighted_avg {'precision': 0.9966519402893844, 'recall': 0.9965618860510805, 'f1-score': 0.9965802581818691, 'support': 10180}
 
time = 6.71 secondes

Val loss 0.880260494883157 accuracy 0.9071618318557739 macro_avg {'precision': 0.9089641183456081, 'recall': 0.9086935886453464, 'f1-score': 0.9081493606424085, 'support': 1131} weighted_avg {'precision': 0.9093624733250951, 'recall': 0.9071618037135278, 'f1-score': 0.9075562602616847, 'support': 1131}
 
----------
Epoch 40/40
time = 351.61 secondes

Train loss 0.010300231988652715 accuracy 0.9970530867576599 macro_avg {'precision': 0.9972378377550093, 'recall': 0.9970943139628062, 'f1-score': 0.9971416744681514, 'support': 10180} weighted_avg {'precision': 0.9971426037126098, 'recall': 0.9970530451866405, 'f1-score': 0.9970717348676846, 'support': 10180}
 
time = 6.94 secondes

Val loss 0.8896317839662002 accuracy 0.9045093059539795 macro_avg {'precision': 0.9070907306995484, 'recall': 0.9046310711055456, 'f1-score': 0.9049953401304529, 'support': 1131} weighted_avg {'precision': 0.9061192316344815, 'recall': 0.9045092838196287, 'f1-score': 0.9045327046828983, 'support': 1131}
 
----------
best_accuracy 0.9071618318557739 best_epoch 39 macro_avg {'precision': 0.9089641183456081, 'recall': 0.9086935886453464, 'f1-score': 0.9081493606424085, 'support': 1131} weighted_avg {'precision': 0.9093624733250951, 'recall': 0.9071618037135278, 'f1-score': 0.9075562602616847, 'support': 1131}

average train time 331.96163523197174

average val time 6.581237918138504
 
time = 7.10 secondes

test_accuracy 0.13625498116016388 macro_avg {'precision': 0.9089641183456081, 'recall': 0.9086935886453464, 'f1-score': 0.9081493606424085, 'support': 1131} weighted_avg {'precision': 0.9093624733250951, 'recall': 0.9071618037135278, 'f1-score': 0.9075562602616847, 'support': 1131}

----------
datasets imported
11.  at the beginning of the events relevant to the application, k. had a daughter, p., and a son, m., born in 1986 and 1988 respectively. p.s father is x and m.s father is v. from march to may 1989 k. was voluntarily hospitalised for about three months, having been diagnosed as suffering from schizophrenia. from august to november 1989 and from december 1989 to march 1990, she was again hospitalised for periods of about three months on account of this illness. in 1991 she was hospitalised for less than a week, diagnosed as suffering from an atypical and undefinable psychosis. it appears that social welfare and health authorities have been in contact with the family since 1989. 12.  the applicants initially cohabited from the summer of 1991 to july 1993. in 1991 both p. and m. were living with them. from 1991 to 1993 k. and x were involved in a custody and access dispute concerning p. in may 1992 a residence order was made transferring custody of p. to x. 13.  k. was again hospitalised from 22 april to 7 may 1992, from 13 may to 10 june 1992, and from 11 to 17 january 1993, on account of psychoses. she was in compulsory care between 15 may and 10 june 1992. according to a medical report dated 15 may 1992, k. was paranoid and psychotic. 14.  on 19 march 1993, according to the social welfare authorities records, a discussion took place between a social worker and k.s mother. k.s mother said that her daughters health condition was really bad and that k. had destroyed a childhood picture of hers, a wedding photo of the mother, broken a glass and pierced the eyes of all appearing in the photos. k.s mother had said that she was tired of the situation, as she did not get any support from the mental health authorities. she added that she was worried and afraid that again something must happen before k. is admitted to care.
on 24 march 1993 k. was placed under observation with a view to determining whether she should be placed in compulsory psychiatric care, having initially been diagnosed as suffering from psychosis. the conditions for compulsory care were not considered to be met but she remained in voluntary care until 5 may 1993. 15.  allegedly, x did not allow k., p. and m. to meet. on 11 may 1993, when k. was again pregnant, her access to p. was further limited by an order of the district court of r. basing itself on a doctors opinion, the court held that the childs mental development would be endangered if the meetings between p. and k. continued without supervision as had been ordered in 1992. 16.  according to the records of the social welfare authorities, m. showed signs of behavioural problems. on 30 march 1992 a psychologist reported how m. had played with two dolls saying  in very vulgar terms  that they were performing sexual acts. on 17 february 1993 k. was said to have broken a mirror in the presence of m. who had kept repeating: mummy broke the mirror ...
notes of the social authorities of 24 and 30 march 1993 among others state that games which m. played and pictures he drew were of a destructive nature. according to the notes taken on 30 march, he had lately, while the children were singing together at the day-care nursery, shown immense hatred, threatening to kill everybody. the occasions when k. fetched him were described as unpleasant scenes, m. shouting and hitting his mother who did not react. it was noted, however, that he no longer played doll games with sexual connotations. 17.  according to the records of the social welfare authorities, a discussion between k., her mother, t. and a number of social and mental-health care officials took place on 31 march 1993, during which it was mentioned that the authorities might have to intervene in m.s upbringing, from the child-protection point of view, in a more drastic way than had been the case so far. it appeared that in connection with k.s recent hospitalisation t. had forcibly taken her from a restaurant, which had made k. furious, with the consequence that she had thrown things around; for example, the microwave oven had ended up on the floor. t. had said that k. was unable to control herself. 18.  on the following day the child welfare support group, consisting of various social and health authorities, agreed that the aim should be to place m. in a childrens home for three months as an assistance measure of open care under section 14 of the 1983 child welfare act (lastensuojelulaki, barnskyddslag 683/1983  the 1983 act), during which period psychological examinations of the child would be carried out. 19.  on 3 may 1993 a social welfare official decided on behalf of the social welfare board (perusturvalautakunta, grundtrygghetsnmnden) of s. to place m. in a childrens home for a period of three months. this was to be regarded as a short-term support measure pursuant to the 1983 act. the applicants had been consulted, together with k.s mother and sister, on 8 april 1993, in order to find an open-care measure which would be practicable. according to the records of that meeting, no such practical measure had been proposed by any of the participants. the applicants had then been heard again on 21 april 1993 and had not objected to the placing of m. in a childrens home. 20.  in an opinion of 12 may 1993, requested by the social welfare board, doctors m.l. and k.r. considered that k. was not at that time able to care for m., but that her mental state would not necessarily permanently prevent her from caring for him. doctors m.l. and k.r. worked at the hospital of h., where k. had been cared for since 1991 during the periods indicated above. 21.  on 7 june 1993 it was reported by the social welfare authorities that, when k. and t. had come to the childrens home where m. was staying, the boy had undergone a total change in his behaviour, characterised by anger, hatred, swearing, etc. t. had said that he was really tired of the situation and that in his view k. was in need of hospitalisation. when a visit to the health centre had been suggested to her, she had become very angry.
according to a statement of 22 june 1993 by the childrens home, k. and t. had come to the home on 17 june 1993. while t. had been playing with m., other children had come to tell the staff that k. had asked a 3-year-old girl what her name was. as the girl did not reply, k. had raised her voice and shaken the girl, not letting her go until an older girl had given the childs name. the other children had been frightened by k.s behaviour. 22.  on 11 june 1993 the social welfare official who had decided on 3 may 1993 to place m. in a childrens home informed the university hospital of t. and the local hospital of s. in writing that she was very worried about the health of k. and the baby she was carrying. she requested the hospitals to contact her as soon as k. arrived at the hospital and, more particularly, at the time of the babys delivery. she also expressed the wish that health-care professionals should pay special attention to the relationship between the mother and the new-born baby from the very beginning. 23.  on 18 june 1993 k. was taken to a district hospital, where she gave birth to j. on the same day. according to the hospital records, the mother stayed calm during the delivery. after the delivery a written decision concerning an emergency care order was served on the hospital. the child was taken to the childrens ward. the mothers behaviour in the ward was later found to be somewhat restless but not completely disorderly. the hospital records indicate that she understood the situation and wanted to leave hospital the following day. medication to prevent the secretion of milk was prescribed. it seems that k. left the hospital on 19 june 1993, that is, the following morning, without any post-natal examination. she went to her mothers home, where she started pushing an empty pram around the place. 24.  j. was immediately placed in emergency care, pursuant to section 18 of the 1983 act. after the birth of their child, k. and t. were informed of this decision by two social workers at the hospital of h. the social director, who had made the decision on behalf of the social welfare board, noted that k.s mental state had been unstable during the last stages of her pregnancy. he considered that the babys health would be endangered since k. had found out about the plans to place the baby in public care. lastly, he considered that the babys father, t., could not guarantee its development and safety. in addition the social director referred to the familys long-standing difficulties, namely, k.s serious illness and occasionally uncontrolled emotional reactions which could be traumatic for the children, t.s inability to care for both j. and k., k.s reluctance to accept guidance, the impossibility of putting the whole responsibility for j.s development on t., and the impossibility of providing open-care support measures to the necessary extent. the applicants were not heard prior to the decision. on 24 june 1993 the applicants were notified in writing of the decision to take the new-born baby into public care. the notification was also faxed to k. 25.  on 21 june 1993 the social director also placed m. in emergency care, citing principally the same reasons as in his decision of 18 june 1993 concerning j. 26.  the applicants did not appeal against the emergency care orders. 27.  on 21 june 1993 the social welfare board took note of the emergency care orders and prohibited all unsupervised access between k. on the one hand, and j. and m. on the other. the number of supervised visits, however, was not restricted. the board decided to continue preparations for taking m. and j. into care. 28.  a meeting was held by social workers at the family centre on 21 june 1993, before the arrival of the baby from the hospital and in the absence of the applicants. it is mentioned in the report that there was a plan to prohibit the mothers visits for a month on the ground that her reactions could not be predicted as she had, for example, broken things at home. after this initial period she would be allowed to visit the baby without restriction, but accompanied by her personal nurse. however, this plan was not implemented. the following entry appears in the register for 24 june: the mother may come with her personal nurse if she wants. other visitors not allowed for the time being. 29.  k. was asked to come with t. to the social welfare office on 22 june 1993 at 11.30 a.m. in order to be informed of the decision of 21 june 1993 by the social director concerning m. on 24 june 1993 k. and v. (m.s biological father) were notified in writing of the decision of 21 june 1993. the notification was also faxed to k. 30.  on 22 june 1993 k. was hospitalised voluntarily at the hospital of h. on account of psychosis, having obtained a referral from a doctor at a health care centre. she was treated there until 30 june 1993. 31.  on 23 june 1993 j. was placed in the family centre. t. visited her the same day. 32.  at the beginning of july 1993 t. left the applicants home, having been told by the social welfare officials that he had to break off his relationship with k. if he wanted to keep j. the applicants nevertheless continued their relationship. 33.  on 15 july 1993 the social welfare board gave its decisions taking j. and m. into normal public care, giving reasons similar to those mentioned in the emergency care orders (see paragraph 24 above), and prolonged the access restriction until 15 september 1993. k. was allowed to see the children only in the company of her personal nurse. the board essentially considered that k.s state of health remained unstable; that she was subject to aggressive and uncontrolled emotional moods; and that public care proceedings were a severe mental ordeal for a patient. as regards j., the board therefore believed that her personal security could be jeopardised if access were to take place without supervision. as regards m., the board feared that k.s visits to the childrens home could no longer be supervised by its staff, which would not be in his interest. before the decisions of 15 july 1993 the applicants had been heard and had expressed their objection to the care decisions envisaged. 34.  on 15 july 1993 k. visited both her children, accompanied by her personal nurse. the register indicates that it was a difficult situation. 35.  on 19 july 1993 t. moved to the family unit of the family centre with j. 36.  on 20 july 1993 k. was again hospitalised in voluntary care at the open ward of the hospital of h., suffering from psychosis. she left hospital the following day, however. on 26 july 1993 she was placed under observation with a view to determining whether she should be placed in compulsory psychiatric care. on 30 july 1993 she was committed to compulsory psychiatric care. according to the file, her relatives had earlier been worried about her and had contacted the hospital in order to get her into hospital care. they reported that k. had disappeared from her home, where she had behaved in an unsettled and aggressive manner. her hospitalisation lasted until 27 october 1993, that is, three months. 37.  during the period between 18 june and 31 august 1993 k. visited her children at their respective childrens homes. during the visits she was accompanied by her personal nurse from the hospital, who was in contact with the social welfare authorities and arranged the visits having regard to k.s state of mental health. according to the centres register, she visited j. twice during this period. 38.  according to a statement made by a social worker on 4 august 1993, t. had taken good care of j., first at the hospital until 23 june 1993 and later on at the family centre. it was agreed that j. would stay at the family centre and that t. would visit her every other day. j. would visit her father for the first time from 13 to 15 august 1993, during which time t. would organise her christening. the intention was that the baby could move in with her father later on. 39.  after t.s paternity had been established on 13 july 1993, t. and k. were granted joint custody of j. on 4 august 1993. 40.  t.s travel expenses to the centre were paid for by the social welfare authorities. from the centres records it can be deduced that t. succeeded in creating a relationship with the baby and learned to take good care of her. the home leaves were spent with t. first at his mothers house and later in his new home. 41.  on 12 august 1993 the social welfare board referred both public care orders to the county administrative court (lninoikeus, lnsrtten) for confirmation, as the applicants had opposed them. in support of its referrals, the board submitted a statement by a social welfare official dated 25 august 1993, according to which t. would not be able to care both for m. and the new-born j. alone, since k. was living in the same home and had been psychotic for the last four years. t. had been in contact with j. at the childrens home three to four times a week. while staying in a flat attached to a municipal childrens home, he had cared for j. for two whole weeks and had subsequently cared for her three days a week in his new home. the board had therefore begun investigating whether it would be possible to entrust him with the responsibility for j. with the help of support measures taken by the board. 42.  on 9 september 1993 the county administrative court confirmed the care order concerning j., considering that k. had been mentally ill; that the applicants had had conflicts as a result of which t. had moved away from their home at the beginning of july 1993; that because of k.s illness and the familys other problems the applicants had been unable to provide j. with adequate care; that the care support provided to the family had not sufficiently improved the familys situation and that the measures could not be expected to satisfy j.s care needs. no hearing was held. 43.  on 11 november 1993 the county administrative court confirmed the care order concerning m., repeating the reasons put forward in its decision of 9 september concerning j. no hearing was held. 44.  in an appeal to the supreme administrative court (korkein hallinto-oikeus, hgsta frvaltningsdomstolen) against the confirmation of the public care order concerning m., the applicants were represented by the public legal adviser (yleinen oikeusavustaja, allmnna rttsbitrdet) of s. the supreme administrative court dismissed the appeal on 23 september 1994. 45.  on the same date the supreme administrative court extended the time allowed for an appeal by k. against the confirmation of the care order made in respect of j. 46.  on 18 october 1994 k. appealed against the care order in respect of j. as confirmed by the county administrative court on 9 september 1993. on 21 august 1995 the supreme administrative court granted k. cost-free proceedings as from 1 march 1994, appointed ms suomela as her representative and upheld the county administrative courts decision of 9 september 1993. 47.  by a decision of 21 january 1994 the social welfare board placed j. in a foster home in k., a town some 120 km away from the applicants home. m. joined her on 7 february 1994. the foster parents had no children of their own. social welfare officials told the applicants and the foster parents that j.s and m.s placement would last for years. the applicants had proposed that the childrens public care be implemented in the homes of relatives. 48.  in the meantime, on 15 august 1993, j. was christened in the presence of k., t. and m. 49.  a consultation was held at the childrens home, on 18 august 1993, in the presence of t. according to the records, k.s mental health was very unstable and her psychiatric treatment was expected to have to be continued for four to five years. t., however, had expressed his hopes that k. and he could, together, take care of j. in the future. it was agreed that j. would stay at the childrens home and would visit t. every week from thursday until saturday, beginning on 28 august 1993. t. would visit j. on other days, according to an arrangement to be agreed with the childrens home. 50.  on 14 september 1993 the social welfare board prolonged the access restriction until 15 december 1993. 51.  the following notes of a social welfare official appear among those in the case records of the social welfare board:
14 september 1993:
 2.  ... in addition, the importance of future access between j. and t. has now been questioned, since j.s placement in [public foster care] is under preparation. it will be difficult for t. to give up j. ...
13 october 1993: 52.  on 27 october 1993 k. was discharged from the hospital of h. 53.  on 2 february 1994 the social welfare board drew up a plan concerning the implementation of the public care. the applicants alternative plan was allegedly ignored. for instance, the children could not meet their maternal grandmother at her home. 54.  after the adoption of the care plan on 2 february 1994, the applicants requested a relaxation of the access restriction. for example, t. had been permitted to see j. only once a month. 55.  on 21 march 1994 the applicants requested, inter alia, that the social welfare board should draw up a public care plan aiming at the reunification of the family. 56.  on 3 may 1994 the social welfare authorities organised a meeting in order to revise the care plan of 2 february 1994. the applicants and their representative did not attend the meeting. 57.  on 17 may 1994 the social director restricted both applicants access to the children to one monthly visit at the foster home, to take place under supervision and last three hours. the social director considered that the grounds for public care still existed. in his view, although the applicants were dissatisfied with the visits set out in the care plan, affording the children an unlimited right to see their parents would create an obstacle to their successful placement. the applicants appealed. 58.  on 28 september 1994 the county administrative court held an oral hearing concerning the access restriction imposed on 17 may 1994. it took evidence from two psychiatrists, who had interviewed k. one of them, dr t.i.-e., did not know k. personally but commented on a diagnosis concerning her mental state by indicating that k. had a tendency to react in a psychotic manner to conflict situations. dr k.p. stated that k.s state of health did not prevent her from caring for her children. consequently, if her illness had been the reason for the access restriction, that reason no longer existed. 59.  in a written expert opinion, requested by the social welfare board and submitted to the county administrative court, dr e.v., a child psychiatrist, expressed the opinion that the children should be permanently cared for by the foster parents and that the applicants visits should, for the time being, be discontinued so as to protect the children and the foster parents. according to the applicants, dr e.v. had not met them or the children, nor had he consulted the other psychiatrists before making his proposal. 60.  on 11 october 1994 the county administrative court upheld the access restriction issued on 17 may 1994. it noted that neither of the witnesses who had been heard orally had been willing to state any opinion as regards the childrens development. it reasoned, inter alia, as follows:
... [by allowing] access to take place once a month and [by allowing contact through correspondence] it will be ensured that the children will retain knowledge about their biological parents. if the grounds for public care later cease to exist, a reunification of the family will thus be possible. ... 61.  the county administrative court dismissed the applicants request for exemption from costs, since the relevant legislation did not cover disputes concerning access restrictions. at the courts hearing, the applicants were nevertheless assisted by ms suomela. 62.  on 26 may 1994 the applicants requested that the social welfare board discontinue the public care of m. and j. 63.  on 18 september 1994 the social director allegedly told the applicants that any further children born to them would also be placed in public care. according to the government, the social director only told them, when expressly asked, that it was possible that any further children born would be taken into public care. 64.  in an opinion of 22 september 1994 submitted at the social welfare boards request, dr k.p., a psychiatrist, commented on the possibility of revoking the public care orders. she concluded that k.s mental state would not prevent her from having custody of the children. according to dr k.p., k.s efforts to have public care discontinued and access restrictions relaxed showed that she possessed psychological resources. she noted, inter alia, that t. was k.s closest support in the care and upbringing of the children. in addition, k.s mother, at the time her guardian ad litem, was ready to help in caring for them. dr k.p., however, added that she could not, as a psychiatrist for adults, take any stand as regards the interests of the children. dr k.p.s opinion was also based on a report submitted by dr k.po., a psychologist, who had come to the same conclusion as regards k.s ability to have custody of her children. 65.  the public legal adviser advised against requesting revocation of the care orders. 66.  k. was hospitalised from 15 to 24 february and from 11 april to 29 may 1995, apparently on account of psychosis. 67.  on 14 march 1995 the social welfare board rejected the applicants request of 26 may 1994 that the care order be revoked, stating as follows:
at the moment the health of the childrens mother, k., is better and the family situation has changed in other respects in comparison with the situation in 1993 when the decisions to take the children into care were made.
...
according to dr k.p., a psychiatrist, k. still has a lot of instability in her emotional life as well as fragility, brought about by the last five years experiences and the diagnosis of mental illness for which she needs  and will need for a long time to come  therapeutic support and treatment. a regular medication is also needed in order to guarantee her continued well-being and to make it possible for her to manage in open care and to have custody of her children. dr k.p., however, did not give her more precise opinion as to k.s ability to take care of and bring up her children even though dr k.p. was explicitly asked to give such an opinion. 68.  the applicants appealed on 5 april 1995, requesting that they be granted exemption from costs and afforded free legal representation. they also requested an oral hearing. 69.  on 7 april 1995 a further child, r., was born to the applicants. having given birth, k. left the hospital for a while on the same evening with the new-born baby wrapped in a blanket, walking barefoot in the cold weather until the hospital staff realised what had happened and intervened. 70.  on 13 april 1995 k. was committed to compulsory psychiatric care and treated at the hospital of h. until 29 may 1995, while r. was being cared for by t. according to a psychiatrists observation of 10 april 1995, k. must have been suffering from paranoid schizophrenia for some time. 71.  on 15 june 1995 the county administrative court granted the applicants exemption from costs and appointed ms suomela as their representative in the case concerning their appeal against the social welfare boards decision of 14 march 1995. it decided not to hold a hearing in respect of the applicants request for a revocation of the care orders and provided the parties with an opportunity to supplement their written observations. 72.  on 28 september 1995 the county administrative court rejected the applicants appeals of 5 april 1995 without holding an oral hearing. the court noted, inter alia, that according to the medical certificates, k.s state of health had improved but her emotional life was still unstable. she therefore continued to be in need of psychotherapy and medication. in addition, a further child had been born to the applicants and k. had again been treated at the hospital of h. these two factors had caused an additional strain militating against a revocation of the care orders. 73.  on 17 november 1994 social welfare officials revised the public care plan, proposing that the children meet the applicants once a month on neutral premises at the family advice centre of k., where the foster parents were living. the applicants objected to this proposal, considering that it would have entailed a further restriction of their access to the children. instead, they requested two meetings a month, one of which was to be at their place of residence. on 22 december 1994 they asked for a separate written decision concerning their access request, so that they could appeal against it. 74.  in a letter of 22 december 1994 the social director informed the applicants that there were no longer any grounds for the access restriction. meetings between the applicants and the children were nevertheless only authorised for three hours once a month on premises chosen by the social welfare board. they were also informed that the meetings would be supervised. 75.  in his decision of 11 january 1995 the social director confirmed that there were no longer any grounds for the access restriction. on 31 january and 28 february 1995 the social welfare board confirmed the decision of 11 january 1995. the applicants appealed. 76.  as regards the applicants appeal against the social welfare boards decisions of 31 january and 28 february 1995, the county administrative court considered, on 15 june 1995, that the revised care plan drawn up on 17 november 1994 had already entailed an access restriction which had later been renewed by further decisions, without the applicants having been properly heard, in respect of their access request. the matter was referred back to the social welfare board for further consideration. 77.  in the light of the county administrative courts decision the acting social director, on 28 june 1995, formally restricted the applicants access to the children to one meeting a month up to 31 may 1996. the meetings were to take place in the foster home. in addition, the foster parents were to visit the applicants with the children every six months. the director considered, inter alia, that it was important that the children settle themselves in the foster family environment in which they would grow up. closer contacts with their parents would mean change and insecurity as well as the creation of a new crisis in their development. the process of settling which had started well would be jeopardised. for the childrens progress it was therefore necessary that their situation remain stable and secure. the directors decision was confirmed by the social welfare board on 22 august 1995. the applicants appealed. 78.  on 3 november 1995 the county administrative court rejected the applicants appeal against the access restriction confirmed on 22 august 1995. 79.  on 25 may 1996 social welfare officials revised the public care plan, proposing that the children meet the applicants once a month on the premises of a school at the childrens place of residence. as the applicants were not present when the proposal was made, the care plan was again revised on 9 october 1996 in so far as the access restriction was concerned. the applicants then proposed that the children meet them without supervision once a month. the public care plan was, however, revised as proposed by the social welfare officials. 80.  on 17 june 1996 the social director restricted both applicants access to the children, until 30 november 1997, to one monthly visit on the premises of a school at the childrens place of residence, where access was to take place under supervision for three hours. one of the foster parents was also ordered to be present at the time of the access. the social directors decision was confirmed by the social welfare board on 20 august 1996. the applicants appealed against the decision to the county administrative court, requesting an oral hearing. the court obtained a statement from a child psychiatrist, dr j.p., who was also recommended by the applicants representative to the social welfare board. dr j.p.s statement included the following observations:
the right of access of m. and j. to the persons close to them must primarily be examined in the light of their psychological growth and development and their health. this requires an examination of the quality, permanence and durability of their human relationships, because psychological growth and development take place in interaction with human relationships. in my opinion, the human relationships are to be examined from the childrens point of view. ...
... in conclusion, i note that before m. was placed in the childrens home ... the mother had been in psychiatric hospital for treatment eight times, making a total of thirteen months. thus, m. had lived with his mother for forty-five months, namely, three years and nine months. the longest that they spent together was two years and one month. ... t. has, as stepfather, helped to look after m. for at most ten months. ... the foster parents have so far looked after m. for three years and three months without interruption. ... in practice, m. has not had any kind of relationship with his biological father ... 
in the light of the above, i note that the human relationships in m.s early childhood have, owing to the circumstances, been non-continuous, short-term and changing. the most stable and continuous relationships have been with his foster parents ... therefore, these relationships are the most relevant and important ones for m.s psychological growth and development.
... j. was born in june 1993. she was taken into public care immediately after she was born. at first, she stayed in the district hospital for a short time, and later at a reception home for small children. t., as the biological father of j., looked after her for two weeks in june and august 1993. j. was placed in the foster family ... in january 1994, when she was some seven months old. so far, j. has stayed with her foster family for some three years and three months without interruption. j. is now a little over 3 years and 10 months old.
in the light of the above, i note that, due to the circumstances, j. has not had any significant and important relationships other than those with her foster parents. j.s relationship with her foster parents is of primary importance for her psychological growth and development. ...
... from the childrens point of view, especially, but naturally also from that of the foster parents, the foster family is a family to which the principles concerning family life enshrined in the united nations convention on the rights of the child and in the european convention on human rights can be applied in the same way as to biological families. this point of view is especially important when, due to the circumstances, the biological family has not lived together.
in the light of the above, i note that the arrangements for helping and supporting the foster parents of m. and j. are in the best interests of the children. the arrangement will, in the first place, ensure the important, continuous and safe human relationships of m. and j. with their foster parents ...
it is also important for m. and j.s psychological growth and development that, in the safe and stable conditions provided by the foster family, they are able to form and maintain a good internalised picture of their biological parents ... from whom they have been separated because of the circumstances.
in my opinion, this can be done by complying with the decision of the social welfare board of s. of 20 august 1996 concerning the right of access. at present, an unrestricted right of access or a right of access of the extent suggested by the applicants is not in the interests of the children, because k. and t. are not capable of meeting the emotional needs of m. and j. ... such arrangements concerning the right of access would clearly endanger the health and development of m. and j. in my opinion, the question of an unrestricted right of access should be evaluated when the children have attained the age of 12. 81.  in a statement of 10 september 1996 dr k.p. stated that in her opinion k.s psychiatric state did not preclude k.s having custody of her daughter r. 82.  on 2 april 1997 the care plan was again revised by the social welfare authorities. the applicants had been informed of the time of the meeting concerning the revision of this care plan on home visits on 15 january and 10 march 1997. their representative had also been informed of the meeting by a letter sent on 10 february 1997. the applicants did not attend the meeting, and neither did their representative. the applicants were thus not explicitly heard in this connection but, as they had expressed their opinion on other occasions, the authorities recorded their point of view in the plan. 83.  on 12 june 1997 the county administrative court rejected the applicants appeal against the social welfare boards decision of 20 august 1996 to restrict the applicants access right (see paragraph 80 above). it refused the applicants request for an oral hearing. 84.  although the applicants had stated only in their reply that the appeal was also made on r.s behalf, the county administrative court found in its decision that it was in part made in her name. the court stated that a person to whom a decision was directed, or upon whose right, duty or interest it had a direct effect, had the right of appeal. the court considered that the boards decision, which concerned r.s siblings and parents right of access, was not such a decision. 85.  on 28 november 1997 the social director restricted the applicants, and consequently their youngest child r.s, access to j. and m. to one monthly visit of three hours on the premises of a school at the childrens place of residence until the end of 1998. the applicants did not appeal. 86.  the care plan was again revised on 1 december 1998. 87.  according to a statement made on 3 july 1998 by dr k.m. (formerly dr k.p.), k. had not been hospitalised since may 1995 and her health had been stable since the beginning of 1995. there had been no problems concerning the care of r. (who had lived with her parents all the time and had not been taken into care). it was recommended by dr k.m. that the social welfare authorities should reduce or discontinue control visits to the applicants home in order to give k. the possibility of settling down to normal life without constant supervision by the authorities. 88.  the restriction orders were extended by the social director on 11 december 1998, until the end of 2000. the visits were to take place under supervision on the premises of a school at the childrens place of residence. however, one of the visits was to take place at the applicants home in the presence of the foster parents. the social director considered, inter alia, that the reunification of the family was not in sight as the foster family was now the childrens de facto home; that the applicants access to the children once a month and through correspondence was enough to maintain the childrens awareness of their biological parents; and that closer contacts with the applicants would endanger the childrens development, bring change and insecurity and create a new crisis in their development. the applicants appealed against this decision to the social welfare board which, on 2 february 1999, rejected the appeal and upheld the social directors decisions. in its reasoning, the board quoted both the county administrative court and dr j.p. 89.  according to the reports drawn up by the supervisor who attended the meetings of the children and the applicants during the period from 25 may 1996 to 10 january 1999, the adults got on quite well together during the meetings. j. often played games with m. when r. was smaller, j. played by herself, but later it seemed that the girls, j. and r., spent more time together. on the other hand, it seemed that the first applicant made very little contact with j. and m. according to the supervisors description, especially in the earlier reports, the first applicant seemed to have concentrated on r. 90.  m. visited k. and t. at their home for the weekend of 21 to 23 july 2000 without supervision. 91.  the applicants appealed against the social welfare boards decision of 2 february 1999, concerning the right of access, to the administrative court (formerly the county administrative court). an oral hearing, at which m. was also heard, was held on 3 october 2000. in its decision of 13 october 2000 the administrative court upheld the social welfare boards decision. 92.  the social authorities reviewed the care plan on 23 november 2000, having consulted the applicants, among others. it was decided that the children would remain in the foster home. according to the care plan, m. and j. are allowed to meet k. and t and others close to them, as from 1 january 2001 until 31 december 2001, without supervision once a month alternately at the applicants home and the foster parents home. the meetings at the applicants home will take place from saturday 11 a.m. until sunday 4 p.m., and the meetings at the foster parents home on sundays, from 11 a.m. until 5 p.m. the children are also allowed to meet their other relatives freely during those meetings. in addition to the above, the children will also spend a day and a night with the applicants each christmas, and two weeks each summer during their school holidays. 93.  j. and m.s foster mother died in may 2001.
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (19). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (18) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (15). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (17). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (26) found smaller than n_clusters (27). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (23) found smaller than n_clusters (24). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (25) found smaller than n_clusters (26). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (22) found smaller than n_clusters (23). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
