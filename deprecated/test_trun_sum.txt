[nltk_data] Downloading package punkt to /vol/fob-
[nltk_data]     vol3/nebenf20/wubingti/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
516 516
datasets imported
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
Hyperpartisan_BERT_head_tail_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 2; 22.17 GiB total capacity; 8.98 GiB already allocated; 39.69 MiB free; 9.02 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_BERT_tail_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 2; 22.17 GiB total capacity; 8.79 GiB already allocated; 107.69 MiB free; 8.96 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_BERT_head_none_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 2; 22.17 GiB total capacity; 8.80 GiB already allocated; 107.69 MiB free; 8.96 GiB reserved in total by PyTorch)
Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]Downloading: 100%|██████████| 571/571 [00:00<00:00, 392kB/s]
Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]Downloading:   1%|          | 7.28M/1.25G [00:00<00:17, 76.3MB/s]Downloading:   1%|          | 15.9M/1.25G [00:00<00:15, 84.4MB/s]Downloading:   2%|▏         | 24.8M/1.25G [00:00<00:14, 88.4MB/s]Downloading:   3%|▎         | 33.5M/1.25G [00:00<00:14, 89.8MB/s]Downloading:   3%|▎         | 42.2M/1.25G [00:00<00:14, 90.3MB/s]Downloading:   4%|▍         | 51.0M/1.25G [00:00<00:14, 90.9MB/s]Downloading:   5%|▍         | 59.8M/1.25G [00:00<00:14, 91.5MB/s]Downloading:   5%|▌         | 68.7M/1.25G [00:00<00:13, 92.0MB/s]Downloading:   6%|▌         | 77.5M/1.25G [00:00<00:13, 91.5MB/s]Downloading:   7%|▋         | 86.3M/1.25G [00:01<00:13, 91.6MB/s]Downloading:   7%|▋         | 95.0M/1.25G [00:01<00:13, 91.7MB/s]Downloading:   8%|▊         | 104M/1.25G [00:01<00:13, 92.0MB/s] Downloading:   9%|▉         | 113M/1.25G [00:01<00:13, 92.1MB/s]Downloading:   9%|▉         | 121M/1.25G [00:01<00:13, 92.1MB/s]Downloading:  10%|█         | 130M/1.25G [00:01<00:13, 92.6MB/s]Downloading:  11%|█         | 139M/1.25G [00:01<00:12, 92.5MB/s]Downloading:  12%|█▏        | 148M/1.25G [00:01<00:12, 92.3MB/s]Downloading:  12%|█▏        | 157M/1.25G [00:01<00:12, 92.0MB/s]Downloading:  13%|█▎        | 166M/1.25G [00:01<00:12, 91.6MB/s]Downloading:  14%|█▎        | 174M/1.25G [00:02<00:12, 91.6MB/s]Downloading:  14%|█▍        | 183M/1.25G [00:02<00:12, 91.8MB/s]Downloading:  15%|█▍        | 192M/1.25G [00:02<00:12, 92.1MB/s]Downloading:  16%|█▌        | 201M/1.25G [00:02<00:12, 91.9MB/s]Downloading:  16%|█▋        | 210M/1.25G [00:02<00:12, 92.1MB/s]Downloading:  17%|█▋        | 218M/1.25G [00:02<00:12, 92.3MB/s]Downloading:  18%|█▊        | 227M/1.25G [00:02<00:11, 92.7MB/s]Downloading:  18%|█▊        | 236M/1.25G [00:02<00:11, 92.6MB/s]Downloading:  19%|█▉        | 245M/1.25G [00:02<00:11, 92.8MB/s]Downloading:  20%|█▉        | 254M/1.25G [00:02<00:11, 92.7MB/s]Downloading:  20%|██        | 263M/1.25G [00:03<00:11, 92.4MB/s]Downloading:  21%|██        | 272M/1.25G [00:03<00:11, 92.5MB/s]Downloading:  22%|██▏       | 281M/1.25G [00:03<00:11, 92.7MB/s]Downloading:  23%|██▎       | 289M/1.25G [00:03<00:11, 92.7MB/s]Downloading:  23%|██▎       | 298M/1.25G [00:03<00:11, 92.7MB/s]Downloading:  24%|██▍       | 307M/1.25G [00:03<00:15, 67.5MB/s]Downloading:  25%|██▍       | 316M/1.25G [00:03<00:13, 73.5MB/s]Downloading:  25%|██▌       | 325M/1.25G [00:03<00:12, 78.2MB/s]Downloading:  26%|██▌       | 334M/1.25G [00:03<00:12, 82.0MB/s]Downloading:  27%|██▋       | 342M/1.25G [00:04<00:11, 84.6MB/s]Downloading:  27%|██▋       | 351M/1.25G [00:04<00:11, 86.7MB/s]Downloading:  28%|██▊       | 360M/1.25G [00:04<00:10, 88.5MB/s]Downloading:  29%|██▊       | 369M/1.25G [00:04<00:10, 89.4MB/s]Downloading:  29%|██▉       | 378M/1.25G [00:04<00:10, 90.6MB/s]Downloading:  30%|███       | 386M/1.25G [00:04<00:10, 90.6MB/s]Downloading:  31%|███       | 395M/1.25G [00:04<00:10, 91.2MB/s]Downloading:  31%|███▏      | 404M/1.25G [00:04<00:10, 91.8MB/s]Downloading:  32%|███▏      | 413M/1.25G [00:04<00:09, 92.0MB/s]Downloading:  33%|███▎      | 422M/1.25G [00:04<00:09, 92.4MB/s]Downloading:  34%|███▎      | 431M/1.25G [00:05<00:09, 92.1MB/s]Downloading:  34%|███▍      | 439M/1.25G [00:05<00:09, 92.1MB/s]Downloading:  35%|███▍      | 448M/1.25G [00:05<00:09, 92.0MB/s]Downloading:  36%|███▌      | 457M/1.25G [00:05<00:09, 92.2MB/s]Downloading:  36%|███▋      | 466M/1.25G [00:05<00:09, 92.0MB/s]Downloading:  37%|███▋      | 475M/1.25G [00:05<00:09, 92.3MB/s]Downloading:  38%|███▊      | 483M/1.25G [00:05<00:09, 92.3MB/s]Downloading:  38%|███▊      | 492M/1.25G [00:05<00:08, 92.4MB/s]Downloading:  39%|███▉      | 501M/1.25G [00:05<00:08, 92.4MB/s]Downloading:  40%|███▉      | 510M/1.25G [00:05<00:08, 92.3MB/s]Downloading:  40%|████      | 519M/1.25G [00:06<00:08, 92.3MB/s]Downloading:  41%|████      | 528M/1.25G [00:06<00:08, 92.4MB/s]Downloading:  42%|████▏     | 536M/1.25G [00:06<00:08, 92.4MB/s]Downloading:  43%|████▎     | 545M/1.25G [00:06<00:08, 92.5MB/s]Downloading:  43%|████▎     | 554M/1.25G [00:06<00:08, 92.5MB/s]Downloading:  44%|████▍     | 563M/1.25G [00:06<00:08, 92.7MB/s]Downloading:  45%|████▍     | 572M/1.25G [00:06<00:08, 92.8MB/s]Downloading:  45%|████▌     | 581M/1.25G [00:06<00:07, 92.7MB/s]Downloading:  46%|████▌     | 590M/1.25G [00:06<00:07, 93.0MB/s]Downloading:  47%|████▋     | 598M/1.25G [00:06<00:07, 92.9MB/s]Downloading:  47%|████▋     | 607M/1.25G [00:07<00:07, 93.0MB/s]Downloading:  48%|████▊     | 616M/1.25G [00:07<00:07, 93.3MB/s]Downloading:  49%|████▊     | 625M/1.25G [00:07<00:07, 93.2MB/s]Downloading:  49%|████▉     | 634M/1.25G [00:07<00:07, 93.1MB/s]Downloading:  50%|█████     | 643M/1.25G [00:07<00:07, 93.3MB/s]Downloading:  51%|█████     | 652M/1.25G [00:07<00:07, 93.4MB/s]Downloading:  52%|█████▏    | 661M/1.25G [00:07<00:06, 93.5MB/s]Downloading:  52%|█████▏    | 670M/1.25G [00:07<00:06, 93.3MB/s]Downloading:  53%|█████▎    | 679M/1.25G [00:07<00:06, 93.1MB/s]Downloading:  54%|█████▎    | 688M/1.25G [00:07<00:06, 93.0MB/s]Downloading:  54%|█████▍    | 697M/1.25G [00:08<00:06, 93.1MB/s]Downloading:  55%|█████▍    | 705M/1.25G [00:08<00:06, 93.1MB/s]Downloading:  56%|█████▌    | 714M/1.25G [00:08<00:06, 93.0MB/s]Downloading:  56%|█████▋    | 723M/1.25G [00:08<00:06, 93.0MB/s]Downloading:  57%|█████▋    | 732M/1.25G [00:08<00:06, 92.7MB/s]Downloading:  58%|█████▊    | 741M/1.25G [00:08<00:06, 92.2MB/s]Downloading:  58%|█████▊    | 750M/1.25G [00:08<00:06, 91.7MB/s]Downloading:  59%|█████▉    | 758M/1.25G [00:08<00:06, 91.4MB/s]Downloading:  60%|█████▉    | 767M/1.25G [00:08<00:05, 91.8MB/s]Downloading:  61%|██████    | 776M/1.25G [00:08<00:05, 91.7MB/s]Downloading:  61%|██████    | 785M/1.25G [00:09<00:05, 91.6MB/s]Downloading:  62%|██████▏   | 794M/1.25G [00:09<00:05, 91.6MB/s]Downloading:  63%|██████▎   | 802M/1.25G [00:09<00:05, 91.3MB/s]Downloading:  63%|██████▎   | 811M/1.25G [00:09<00:05, 90.3MB/s]Downloading:  64%|██████▍   | 820M/1.25G [00:09<00:05, 90.2MB/s]Downloading:  65%|██████▍   | 828M/1.25G [00:09<00:05, 84.7MB/s]Downloading:  65%|██████▌   | 836M/1.25G [00:09<00:05, 85.0MB/s]Downloading:  66%|██████▌   | 845M/1.25G [00:09<00:05, 85.7MB/s]Downloading:  67%|██████▋   | 853M/1.25G [00:09<00:05, 86.0MB/s]Downloading:  67%|██████▋   | 861M/1.25G [00:09<00:05, 86.5MB/s]Downloading:  68%|██████▊   | 870M/1.25G [00:10<00:04, 86.9MB/s]Downloading:  68%|██████▊   | 878M/1.25G [00:10<00:04, 86.8MB/s]Downloading:  69%|██████▉   | 886M/1.25G [00:10<00:04, 86.9MB/s]Downloading:  70%|██████▉   | 895M/1.25G [00:10<00:04, 86.0MB/s]Downloading:  70%|███████   | 903M/1.25G [00:10<00:04, 85.4MB/s]Downloading:  71%|███████   | 911M/1.25G [00:10<00:04, 86.0MB/s]Downloading:  72%|███████▏  | 919M/1.25G [00:10<00:04, 86.3MB/s]Downloading:  72%|███████▏  | 928M/1.25G [00:10<00:04, 86.7MB/s]Downloading:  73%|███████▎  | 936M/1.25G [00:10<00:04, 87.0MB/s]Downloading:  74%|███████▎  | 945M/1.25G [00:11<00:06, 54.5MB/s]Downloading:  74%|███████▍  | 952M/1.25G [00:11<00:05, 60.6MB/s]Downloading:  75%|███████▍  | 961M/1.25G [00:11<00:04, 68.4MB/s]Downloading:  76%|███████▌  | 970M/1.25G [00:11<00:04, 73.5MB/s]Downloading:  76%|███████▋  | 978M/1.25G [00:11<00:04, 77.8MB/s]Downloading:  77%|███████▋  | 987M/1.25G [00:11<00:03, 81.5MB/s]Downloading:  78%|███████▊  | 996M/1.25G [00:11<00:03, 83.3MB/s]Downloading:  78%|███████▊  | 0.98G/1.25G [00:11<00:03, 85.6MB/s]Downloading:  79%|███████▉  | 0.99G/1.25G [00:11<00:03, 87.3MB/s]Downloading:  80%|███████▉  | 1.00G/1.25G [00:12<00:03, 89.2MB/s]Downloading:  80%|████████  | 1.01G/1.25G [00:12<00:02, 90.4MB/s]Downloading:  81%|████████  | 1.02G/1.25G [00:12<00:02, 90.6MB/s]Downloading:  82%|████████▏ | 1.02G/1.25G [00:12<00:02, 90.7MB/s]Downloading:  82%|████████▏ | 1.03G/1.25G [00:12<00:02, 90.5MB/s]Downloading:  83%|████████▎ | 1.04G/1.25G [00:12<00:02, 90.3MB/s]Downloading:  84%|████████▍ | 1.05G/1.25G [00:12<00:02, 90.4MB/s]Downloading:  84%|████████▍ | 1.06G/1.25G [00:12<00:02, 90.1MB/s]Downloading:  85%|████████▌ | 1.07G/1.25G [00:12<00:02, 90.7MB/s]Downloading:  86%|████████▌ | 1.07G/1.25G [00:12<00:02, 90.5MB/s]Downloading:  86%|████████▋ | 1.08G/1.25G [00:13<00:01, 91.3MB/s]Downloading:  87%|████████▋ | 1.09G/1.25G [00:13<00:01, 92.0MB/s]Downloading:  88%|████████▊ | 1.10G/1.25G [00:13<00:01, 92.7MB/s]Downloading:  89%|████████▊ | 1.11G/1.25G [00:13<00:01, 93.0MB/s]Downloading:  89%|████████▉ | 1.12G/1.25G [00:13<00:01, 92.2MB/s]Downloading:  90%|████████▉ | 1.13G/1.25G [00:13<00:01, 91.2MB/s]Downloading:  91%|█████████ | 1.14G/1.25G [00:13<00:01, 90.6MB/s]Downloading:  91%|█████████▏| 1.14G/1.25G [00:13<00:01, 90.2MB/s]Downloading:  92%|█████████▏| 1.15G/1.25G [00:13<00:01, 90.9MB/s]Downloading:  93%|█████████▎| 1.16G/1.25G [00:13<00:01, 91.2MB/s]Downloading:  93%|█████████▎| 1.17G/1.25G [00:14<00:00, 91.8MB/s]Downloading:  94%|█████████▍| 1.18G/1.25G [00:14<00:00, 92.4MB/s]Downloading:  95%|█████████▍| 1.19G/1.25G [00:14<00:00, 93.0MB/s]Downloading:  95%|█████████▌| 1.20G/1.25G [00:14<00:00, 93.2MB/s]Downloading:  96%|█████████▌| 1.20G/1.25G [00:14<00:00, 93.0MB/s]Downloading:  97%|█████████▋| 1.21G/1.25G [00:14<00:00, 92.3MB/s]Downloading:  98%|█████████▊| 1.22G/1.25G [00:14<00:00, 92.3MB/s]Downloading:  98%|█████████▊| 1.23G/1.25G [00:14<00:00, 92.2MB/s]Downloading:  99%|█████████▉| 1.24G/1.25G [00:14<00:00, 92.5MB/s]Downloading: 100%|█████████▉| 1.25G/1.25G [00:15<00:00, 92.5MB/s]Downloading: 100%|██████████| 1.25G/1.25G [00:15<00:00, 89.3MB/s]
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]Downloading:  16%|█▌        | 36.0k/226k [00:00<00:00, 369kB/s]Downloading:  35%|███▍      | 78.0k/226k [00:00<00:00, 257kB/s]Downloading:  88%|████████▊ | 199k/226k [00:00<00:00, 598kB/s] Downloading: 100%|██████████| 226k/226k [00:00<00:00, 577kB/s]
Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 21.1kB/s]
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (10) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (19) found smaller than n_clusters (20). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (27) found smaller than n_clusters (28). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (24) found smaller than n_clusters (25). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (28) found smaller than n_clusters (29). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (21) found smaller than n_clusters (22). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (20) found smaller than n_clusters (21). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.
  model = self._get_model(k).fit(self.features)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
##########
Hyperpartisan_BERT_head_bert_summarizer_1
----------
Epoch 1/40
time = 27.07 secondes

/usr/lib64/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Train loss 0.6457894534775706 accuracy 0.6375969052314758 macro_avg {'precision': 0.3187984496124031, 'recall': 0.5, 'f1-score': 0.38934911242603554, 'support': 516} weighted_avg {'precision': 0.40652980590108767, 'recall': 0.6375968992248062, 'f1-score': 0.4964955735975415, 'support': 516}
 
time = 1.04 secondes

Val loss 0.6398251354694366 accuracy 0.59375 macro_avg {'precision': 0.296875, 'recall': 0.5, 'f1-score': 0.37254901960784315, 'support': 64} weighted_avg {'precision': 0.3525390625, 'recall': 0.59375, 'f1-score': 0.4424019607843137, 'support': 64}
 
----------
Epoch 2/40
time = 26.20 secondes

Train loss 0.5224695025068341 accuracy 0.7538759708404541 macro_avg {'precision': 0.779242714530604, 'recall': 0.6800464866797783, 'f1-score': 0.6894674242172951, 'support': 516} weighted_avg {'precision': 0.768260769815828, 'recall': 0.7538759689922481, 'f1-score': 0.7283866299962029, 'support': 516}
 
time = 0.99 secondes

Val loss 0.6491810083389282 accuracy 0.71875 macro_avg {'precision': 0.7628205128205128, 'recall': 0.6659919028340081, 'f1-score': 0.6631578947368421, 'support': 64} weighted_avg {'precision': 0.749599358974359, 'recall': 0.71875, 'f1-score': 0.6888157894736842, 'support': 64}
 
----------
Epoch 3/40
time = 26.49 secondes

Train loss 0.4287033397139925 accuracy 0.8236433863639832 macro_avg {'precision': 0.8090620134924753, 'recall': 0.8097703297953611, 'f1-score': 0.8094124809741248, 'support': 516} weighted_avg {'precision': 0.823851716529288, 'recall': 0.8236434108527132, 'f1-score': 0.8237443394334124, 'support': 516}
 
time = 0.99 secondes

Val loss 0.6222874224185944 accuracy 0.765625 macro_avg {'precision': 0.8006802721088435, 'recall': 0.7236842105263157, 'f1-score': 0.7308662741799832, 'support': 64} weighted_avg {'precision': 0.7883078231292517, 'recall': 0.765625, 'f1-score': 0.7490012615643398, 'support': 64}
 
----------
Epoch 4/40
time = 26.18 secondes

Train loss 0.3742446978435372 accuracy 0.8527131676673889 macro_avg {'precision': 0.850529262531712, 'recall': 0.8256424426637192, 'f1-score': 0.8352490421455938, 'support': 516} weighted_avg {'precision': 0.8521277988117447, 'recall': 0.8527131782945736, 'f1-score': 0.8500103953191362, 'support': 516}
 
time = 0.98 secondes

Val loss 0.705215334892273 accuracy 0.71875 macro_avg {'precision': 0.7198067632850241, 'recall': 0.6842105263157895, 'f1-score': 0.6883116883116883, 'support': 64} weighted_avg {'precision': 0.7193538647342995, 'recall': 0.71875, 'f1-score': 0.7065746753246753, 'support': 64}
 
----------
Epoch 5/40
time = 26.22 secondes

Train loss 0.3325760332923947 accuracy 0.8798449635505676 macro_avg {'precision': 0.8694723557108879, 'recall': 0.8711538774116997, 'f1-score': 0.8702970939283861, 'support': 516} weighted_avg {'precision': 0.880145616473047, 'recall': 0.8798449612403101, 'f1-score': 0.8799813593447661, 'support': 516}
 
time = 0.98 secondes

Val loss 0.7077734619379044 accuracy 0.75 macro_avg {'precision': 0.75, 'recall': 0.7226720647773279, 'f1-score': 0.7285259809119831, 'support': 64} weighted_avg {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.7428419936373276, 'support': 64}
 
----------
Epoch 6/40
time = 26.25 secondes

Train loss 0.28399407248379605 accuracy 0.9127907156944275 macro_avg {'precision': 0.9040316358024691, 'recall': 0.9085301431984787, 'f1-score': 0.9061768900992779, 'support': 516} weighted_avg {'precision': 0.913454262967748, 'recall': 0.9127906976744186, 'f1-score': 0.9130320775129275, 'support': 516}
 
time = 0.99 secondes

Val loss 0.8856154233217239 accuracy 0.796875 macro_avg {'precision': 0.8442176870748299, 'recall': 0.7560728744939271, 'f1-score': 0.7667507709559854, 'support': 64} weighted_avg {'precision': 0.8275085034013605, 'recall': 0.796875, 'f1-score': 0.7824677600224279, 'support': 64}
 
----------
Epoch 7/40
time = 26.38 secondes

Train loss 0.3306890811319604 accuracy 0.8992248177528381 macro_avg {'precision': 0.8897784937575512, 'recall': 0.8932756855159859, 'f1-score': 0.8914615795349741, 'support': 516} weighted_avg {'precision': 0.8997887651534309, 'recall': 0.8992248062015504, 'f1-score': 0.8994498272643497, 'support': 516}
 
time = 0.98 secondes

Val loss 0.9759181886911392 accuracy 0.75 macro_avg {'precision': 0.8141025641025641, 'recall': 0.6983805668016194, 'f1-score': 0.7005847953216375, 'support': 64} weighted_avg {'precision': 0.7948717948717949, 'recall': 0.75, 'f1-score': 0.7233918128654971, 'support': 64}
 
----------
Epoch 8/40
time = 26.41 secondes

Train loss 0.2745155419477008 accuracy 0.9127907156944275 macro_avg {'precision': 0.9127906976744186, 'recall': 0.8969897436730978, 'f1-score': 0.9038935130190764, 'support': 516} weighted_avg {'precision': 0.9127906976744186, 'recall': 0.9127906976744186, 'f1-score': 0.9119406481850547, 'support': 516}
 
time = 0.99 secondes

Val loss 0.7314314842224121 accuracy 0.796875 macro_avg {'precision': 0.8241551939924906, 'recall': 0.7621457489878543, 'f1-score': 0.7723666210670315, 'support': 64} weighted_avg {'precision': 0.8132431163954943, 'recall': 0.796875, 'f1-score': 0.7863714090287277, 'support': 64}
 
----------
Epoch 9/40
time = 26.35 secondes

Train loss 0.1831354063399362 accuracy 0.9496123790740967 macro_avg {'precision': 0.9446143391097519, 'recall': 0.9466378427579929, 'f1-score': 0.945608458744162, 'support': 516} weighted_avg {'precision': 0.9497572745208048, 'recall': 0.9496124031007752, 'f1-score': 0.9496696023058697, 'support': 516}
 
time = 0.98 secondes

Val loss 1.0087169855833054 accuracy 0.78125 macro_avg {'precision': 0.8653846153846154, 'recall': 0.7307692307692308, 'f1-score': 0.7380116959064327, 'support': 64} weighted_avg {'precision': 0.8401442307692307, 'recall': 0.78125, 'f1-score': 0.7579678362573098, 'support': 64}
 
----------
Epoch 10/40
time = 26.29 secondes

Train loss 0.24329305276500457 accuracy 0.9379844665527344 macro_avg {'precision': 0.9337797011513024, 'recall': 0.9317491019618679, 'f1-score': 0.9327468230694037, 'support': 516} weighted_avg {'precision': 0.9378692962617645, 'recall': 0.937984496124031, 'f1-score': 0.937911750664939, 'support': 516}
 
time = 0.98 secondes

Val loss 0.8112938404083252 accuracy 0.71875 macro_avg {'precision': 0.774891774891775, 'recall': 0.757085020242915, 'f1-score': 0.7176470588235294, 'support': 64} weighted_avg {'precision': 0.8085768398268398, 'recall': 0.71875, 'f1-score': 0.7143382352941177, 'support': 64}
 
----------
Epoch 11/40
time = 26.22 secondes

Train loss 0.2503752887926318 accuracy 0.9263566136360168 macro_avg {'precision': 0.9171427207485321, 'recall': 0.9260926807860475, 'f1-score': 0.9211998456790123, 'support': 516} weighted_avg {'precision': 0.9278670593765909, 'recall': 0.9263565891472868, 'f1-score': 0.9267472515312469, 'support': 516}
 
time = 0.99 secondes

Val loss 1.010076880455017 accuracy 0.78125 macro_avg {'precision': 0.8342857142857143, 'recall': 0.736842105263158, 'f1-score': 0.7454545454545455, 'support': 64} weighted_avg {'precision': 0.8166071428571429, 'recall': 0.78125, 'f1-score': 0.7633522727272728, 'support': 64}
 
----------
Epoch 12/40
time = 26.49 secondes

Train loss 0.26420672175784904 accuracy 0.9341084957122803 macro_avg {'precision': 0.9325298808083152, 'recall': 0.9240934284739041, 'f1-score': 0.9280263870427805, 'support': 516} weighted_avg {'precision': 0.9339486642129509, 'recall': 0.9341085271317829, 'f1-score': 0.9337841463270362, 'support': 516}
 
time = 0.99 secondes

Val loss 1.043968215584755 accuracy 0.765625 macro_avg {'precision': 0.7841051314142677, 'recall': 0.7297570850202428, 'f1-score': 0.73734610123119, 'support': 64} weighted_avg {'precision': 0.7767130788485607, 'recall': 0.765625, 'f1-score': 0.7535054719562242, 'support': 64}
 
----------
Epoch 13/40
time = 26.37 secondes

Train loss 0.13605080417950044 accuracy 0.9651162624359131 macro_avg {'precision': 0.9596239914018512, 'recall': 0.9657201371844676, 'f1-score': 0.9625121084920891, 'support': 516} weighted_avg {'precision': 0.9656232594698828, 'recall': 0.9651162790697675, 'f1-score': 0.9652311689481944, 'support': 516}
 
time = 0.98 secondes

Val loss 1.0527806133031845 accuracy 0.765625 macro_avg {'precision': 0.7841051314142677, 'recall': 0.7297570850202428, 'f1-score': 0.73734610123119, 'support': 64} weighted_avg {'precision': 0.7767130788485607, 'recall': 0.765625, 'f1-score': 0.7535054719562242, 'support': 64}
 
----------
Epoch 14/40
time = 26.60 secondes

Train loss 0.22856184251775796 accuracy 0.9379844665527344 macro_avg {'precision': 0.9277703709512979, 'recall': 0.9432895014872487, 'f1-score': 0.9341836734693877, 'support': 516} weighted_avg {'precision': 0.9414534442959026, 'recall': 0.937984496124031, 'f1-score': 0.9385362284448664, 'support': 516}
 
time = 0.99 secondes

Val loss 1.1894433200359344 accuracy 0.765625 macro_avg {'precision': 0.7841051314142677, 'recall': 0.7297570850202428, 'f1-score': 0.73734610123119, 'support': 64} weighted_avg {'precision': 0.7767130788485607, 'recall': 0.765625, 'f1-score': 0.7535054719562242, 'support': 64}
 
----------
Epoch 15/40
time = 26.35 secondes

Train loss 0.5170859252243782 accuracy 0.8779069781303406 macro_avg {'precision': 0.8649764150943396, 'recall': 0.8823285600507127, 'f1-score': 0.871289598403592, 'support': 516} weighted_avg {'precision': 0.8849344559017113, 'recall': 0.877906976744186, 'f1-score': 0.8793209464750824, 'support': 516}
 
time = 0.99 secondes

Val loss 0.6951270028948784 accuracy 0.765625 macro_avg {'precision': 0.776847290640394, 'recall': 0.784412955465587, 'f1-score': 0.7651088818204062, 'support': 64} weighted_avg {'precision': 0.7992918719211823, 'recall': 0.765625, 'f1-score': 0.7671733545387815, 'support': 64}
 
----------
Epoch 16/40
time = 26.39 secondes

Train loss 0.10527133364512613 accuracy 0.961240291595459 macro_avg {'precision': 0.9637518124093796, 'recall': 0.9522942639338134, 'f1-score': 0.957557412647233, 'support': 516} weighted_avg {'precision': 0.9615503720937983, 'recall': 0.9612403100775194, 'f1-score': 0.9609980141939479, 'support': 516}
 
time = 0.99 secondes

Val loss 0.965007022023201 accuracy 0.8125 macro_avg {'precision': 0.8138528138528138, 'recall': 0.7935222672064777, 'f1-score': 0.8, 'support': 64} weighted_avg {'precision': 0.8130411255411256, 'recall': 0.8125, 'f1-score': 0.8093750000000002, 'support': 64}
 
----------
Epoch 17/40
time = 26.37 secondes

Train loss 0.049641674917163044 accuracy 0.9883720874786377 macro_avg {'precision': 0.989760252055334, 'recall': 0.985111259203875, 'f1-score': 0.9873601698375112, 'support': 516} weighted_avg {'precision': 0.9884461281716332, 'recall': 0.9883720930232558, 'f1-score': 0.9883443691003586, 'support': 516}
 
time = 0.99 secondes

Val loss 1.142213836312294 accuracy 0.8125 macro_avg {'precision': 0.8138528138528138, 'recall': 0.7935222672064777, 'f1-score': 0.8, 'support': 64} weighted_avg {'precision': 0.8130411255411256, 'recall': 0.8125, 'f1-score': 0.8093750000000002, 'support': 64}
 
----------
Epoch 18/40
time = 26.28 secondes

Train loss 0.08922254096603754 accuracy 0.9806201457977295 macro_avg {'precision': 0.9761786361667656, 'recall': 0.9824943517058661, 'f1-score': 0.9791733936067162, 'support': 516} weighted_avg {'precision': 0.9810301413961745, 'recall': 0.9806201550387597, 'f1-score': 0.9806839827489969, 'support': 516}
 
time = 0.99 secondes

Val loss 0.8917828947305679 accuracy 0.78125 macro_avg {'precision': 0.8342857142857143, 'recall': 0.736842105263158, 'f1-score': 0.7454545454545455, 'support': 64} weighted_avg {'precision': 0.8166071428571429, 'recall': 0.78125, 'f1-score': 0.7633522727272728, 'support': 64}
 
----------
Epoch 19/40
time = 26.38 secondes

Train loss 0.12295447608498349 accuracy 0.9728682041168213 macro_avg {'precision': 0.9740249031087655, 'recall': 0.9671830047299383, 'f1-score': 0.9704360921948665, 'support': 516} weighted_avg {'precision': 0.9729583484351338, 'recall': 0.9728682170542635, 'f1-score': 0.9727696173978015, 'support': 516}
 
time = 0.99 secondes

Val loss 1.4610062539577484 accuracy 0.75 macro_avg {'precision': 0.7885714285714285, 'recall': 0.7044534412955465, 'f1-score': 0.709090909090909, 'support': 64} weighted_avg {'precision': 0.7757142857142857, 'recall': 0.75, 'f1-score': 0.7295454545454545, 'support': 64}
 
----------
Epoch 20/40
time = 26.28 secondes

Train loss 0.29669937171803956 accuracy 0.9224806427955627 macro_avg {'precision': 0.9196989539644911, 'recall': 0.9115127675828552, 'f1-score': 0.9153251612268005, 'support': 516} weighted_avg {'precision': 0.9221989324395402, 'recall': 0.9224806201550387, 'f1-score': 0.9220989956788661, 'support': 516}
 
time = 0.98 secondes

Val loss 1.2914364337921143 accuracy 0.75 macro_avg {'precision': 0.7773279352226721, 'recall': 0.7773279352226721, 'f1-score': 0.7499999999999999, 'support': 64} weighted_avg {'precision': 0.8046558704453441, 'recall': 0.75, 'f1-score': 0.7499999999999999, 'support': 64}
 
----------
Epoch 21/40
time = 26.34 secondes

Train loss 0.18977428716783779 accuracy 0.9534883499145508 macro_avg {'precision': 0.9593185863208746, 'recall': 0.9404450368155, 'f1-score': 0.9486762926247037, 'support': 516} weighted_avg {'precision': 0.9545605953992947, 'recall': 0.9534883720930233, 'f1-score': 0.953001072906358, 'support': 516}
 
time = 1.01 secondes

Val loss 1.1700627654790878 accuracy 0.78125 macro_avg {'precision': 0.7882352941176471, 'recall': 0.7975708502024291, 'f1-score': 0.780392156862745, 'support': 64} weighted_avg {'precision': 0.8091911764705884, 'recall': 0.78125, 'f1-score': 0.7829656862745098, 'support': 64}
 
----------
Epoch 22/40
time = 26.39 secondes

Train loss 0.3440814427396452 accuracy 0.9321705102920532 macro_avg {'precision': 0.9320855614973262, 'recall': 0.9202655917299221, 'f1-score': 0.9256321881678587, 'support': 516} weighted_avg {'precision': 0.9321591427268582, 'recall': 0.9321705426356589, 'f1-score': 0.9317004648634641, 'support': 516}
 
time = 0.98 secondes

Val loss 2.0264738500118256 accuracy 0.703125 macro_avg {'precision': 0.7808080808080808, 'recall': 0.6406882591093117, 'f1-score': 0.6264208909370199, 'support': 64} weighted_avg {'precision': 0.7605429292929293, 'recall': 0.703125, 'f1-score': 0.6581605222734255, 'support': 64}
 
----------
Epoch 23/40
time = 26.40 secondes

Train loss 0.1281908968019045 accuracy 0.9651162624359131 macro_avg {'precision': 0.9570244018005212, 'recall': 0.9703362969946199, 'f1-score': 0.9628289684318372, 'support': 516} weighted_avg {'precision': 0.9671037576973015, 'recall': 0.9651162790697675, 'f1-score': 0.965366453670791, 'support': 516}
 
time = 1.00 secondes

Val loss 1.2969551384449005 accuracy 0.796875 macro_avg {'precision': 0.8099415204678362, 'recall': 0.7682186234817814, 'f1-score': 0.7772423025435075, 'support': 64} weighted_avg {'precision': 0.8039108187134503, 'recall': 0.796875, 'f1-score': 0.7896419009370819, 'support': 64}
 
----------
Epoch 24/40
time = 26.38 secondes

Train loss 0.09760372802636332 accuracy 0.9767441749572754 macro_avg {'precision': 0.9824046920821115, 'recall': 0.9679144385026738, 'f1-score': 0.9744701904840438, 'support': 516} weighted_avg {'precision': 0.9775625724612972, 'recall': 0.9767441860465116, 'f1-score': 0.9765669915870987, 'support': 516}
 
time = 0.99 secondes

Val loss 1.8128123730421066 accuracy 0.71875 macro_avg {'precision': 0.7583333333333333, 'recall': 0.7510121457489879, 'f1-score': 0.718475073313783, 'support': 64} weighted_avg {'precision': 0.7880208333333333, 'recall': 0.71875, 'f1-score': 0.716825513196481, 'support': 64}
 
----------
Epoch 25/40
time = 26.35 secondes

Train loss 0.14428197981077107 accuracy 0.9651162624359131 macro_avg {'precision': 0.9575796965902066, 'recall': 0.9691822570420818, 'f1-score': 0.9627520492789425, 'support': 516} weighted_avg {'precision': 0.9666491433028983, 'recall': 0.9651162790697675, 'f1-score': 0.9653345156658436, 'support': 516}
 
time = 0.99 secondes

Val loss 1.7705034017562866 accuracy 0.734375 macro_avg {'precision': 0.8036020583190394, 'recall': 0.6791497975708503, 'f1-score': 0.6768636768636769, 'support': 64} weighted_avg {'precision': 0.7838228987993139, 'recall': 0.734375, 'f1-score': 0.7024242649242649, 'support': 64}
 
----------
Epoch 26/40
time = 26.27 secondes

Train loss 0.07034589485424063 accuracy 0.9825581312179565 macro_avg {'precision': 0.9770408163265306, 'recall': 0.9863221884498481, 'f1-score': 0.9813169085196345, 'support': 516} weighted_avg {'precision': 0.9833590412909349, 'recall': 0.9825581395348837, 'f1-score': 0.9826421326111036, 'support': 516}
 
time = 0.99 secondes

Val loss 1.5536284446716309 accuracy 0.765625 macro_avg {'precision': 0.7598091198303287, 'recall': 0.7479757085020242, 'f1-score': 0.7520020666494445, 'support': 64} weighted_avg {'precision': 0.7636863732767762, 'recall': 0.765625, 'f1-score': 0.7629004133298889, 'support': 64}
 
----------
Epoch 27/40
time = 26.22 secondes

Train loss 0.05073227907732045 accuracy 0.9864341020584106 macro_avg {'precision': 0.9882707113246035, 'recall': 0.9824374624124311, 'f1-score': 0.9852358704582521, 'support': 516} weighted_avg {'precision': 0.9865549376585444, 'recall': 0.9864341085271318, 'f1-score': 0.9863933521302312, 'support': 516}
 
time = 0.99 secondes

Val loss 1.167267732322216 accuracy 0.8125 macro_avg {'precision': 0.8227272727272728, 'recall': 0.7874493927125505, 'f1-score': 0.7963944856839873, 'support': 64} weighted_avg {'precision': 0.8176136363636364, 'recall': 0.8125, 'f1-score': 0.8071314952279958, 'support': 64}
 
----------
Epoch 28/40
time = 26.34 secondes

Train loss 0.033015962047184905 accuracy 0.9903100728988647 macro_avg {'precision': 0.9869791666666667, 'recall': 0.9924012158054711, 'f1-score': 0.9895752100110309, 'support': 516} weighted_avg {'precision': 0.990562419250646, 'recall': 0.9903100775193798, 'f1-score': 0.9903368975014364, 'support': 516}
 
time = 0.99 secondes

Val loss 1.9586085975170135 accuracy 0.75 macro_avg {'precision': 0.7708333333333333, 'recall': 0.7105263157894737, 'f1-score': 0.7165005537098561, 'support': 64} weighted_avg {'precision': 0.7630208333333333, 'recall': 0.75, 'f1-score': 0.7347729789590256, 'support': 64}
 
----------
Epoch 29/40
time = 26.24 secondes

Train loss 0.08243167874576157 accuracy 0.9864341020584106 macro_avg {'precision': 0.9895833333333333, 'recall': 0.981283422459893, 'f1-score': 0.9852000573641189, 'support': 516} weighted_avg {'precision': 0.9867167312661498, 'recall': 0.9864341085271318, 'f1-score': 0.986376132969138, 'support': 516}
 
time = 0.99 secondes

Val loss 1.292262703180313 accuracy 0.828125 macro_avg {'precision': 0.8313782991202345, 'recall': 0.8431174089068827, 'f1-score': 0.827069516089413, 'support': 64} weighted_avg {'precision': 0.8508980938416422, 'recall': 0.828125, 'f1-score': 0.829602677474822, 'support': 64}
 
----------
Epoch 30/40
time = 26.31 secondes

Train loss 0.12812966800375955 accuracy 0.9786821603775024 macro_avg {'precision': 0.9729272959183674, 'recall': 0.9821286348194984, 'f1-score': 0.9771651104128867, 'support': 516} weighted_avg {'precision': 0.9795175555687392, 'recall': 0.9786821705426356, 'f1-score': 0.9787848287469044, 'support': 516}
 
time = 0.99 secondes

Val loss 2.0444490909576416 accuracy 0.75 macro_avg {'precision': 0.7708333333333333, 'recall': 0.7105263157894737, 'f1-score': 0.7165005537098561, 'support': 64} weighted_avg {'precision': 0.7630208333333333, 'recall': 0.75, 'f1-score': 0.7347729789590256, 'support': 64}
 
----------
Epoch 31/40
time = 26.25 secondes

Train loss 0.06659212173183209 accuracy 0.9903100728988647 macro_avg {'precision': 0.9869791666666667, 'recall': 0.9924012158054711, 'f1-score': 0.9895752100110309, 'support': 516} weighted_avg {'precision': 0.990562419250646, 'recall': 0.9903100775193798, 'f1-score': 0.9903368975014364, 'support': 516}
 
time = 0.98 secondes

Val loss 1.8287920504808426 accuracy 0.765625 macro_avg {'precision': 0.7872872872872874, 'recall': 0.7904858299595142, 'f1-score': 0.7655677655677655, 'support': 64} weighted_avg {'precision': 0.813282032032032, 'recall': 0.765625, 'f1-score': 0.7662545787545787, 'support': 64}
 
----------
Epoch 32/40
time = 26.37 secondes

Train loss 0.048697915158382704 accuracy 0.9922480583190918 macro_avg {'precision': 0.9895287958115183, 'recall': 0.993920972644377, 'f1-score': 0.9916508907334596, 'support': 516} weighted_avg {'precision': 0.992410406266488, 'recall': 0.9922480620155039, 'f1-score': 0.9922653713280268, 'support': 516}
 
time = 0.99 secondes

Val loss 2.2126616537570953 accuracy 0.734375 macro_avg {'precision': 0.8036020583190394, 'recall': 0.6791497975708503, 'f1-score': 0.6768636768636769, 'support': 64} weighted_avg {'precision': 0.7838228987993139, 'recall': 0.734375, 'f1-score': 0.7024242649242649, 'support': 64}
 
----------
Epoch 33/40
time = 26.32 secondes

Train loss 0.0012022984676260614 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 1.01 secondes

Val loss 1.0506259202957153 accuracy 0.84375 macro_avg {'precision': 0.8392156862745098, 'recall': 0.8502024291497976, 'f1-score': 0.8412698412698413, 'support': 64} weighted_avg {'precision': 0.8528186274509804, 'recall': 0.84375, 'f1-score': 0.8449900793650793, 'support': 64}
 
----------
Epoch 34/40
time = 26.57 secondes

Train loss 0.01685853152197193 accuracy 0.9941860437393188 macro_avg {'precision': 0.9921052631578947, 'recall': 0.9954407294832827, 'f1-score': 0.9937311438232733, 'support': 516} weighted_avg {'precision': 0.9942778457772338, 'recall': 0.9941860465116279, 'f1-score': 0.9941958645552614, 'support': 516}
 
time = 0.99 secondes

Val loss 1.8527673184871674 accuracy 0.796875 macro_avg {'precision': 0.8099415204678362, 'recall': 0.7682186234817814, 'f1-score': 0.7772423025435075, 'support': 64} weighted_avg {'precision': 0.8039108187134503, 'recall': 0.796875, 'f1-score': 0.7896419009370819, 'support': 64}
 
----------
Epoch 35/40
time = 26.47 secondes

Train loss 0.10502460090025455 accuracy 0.9844961166381836 macro_avg {'precision': 0.9881305637982196, 'recall': 0.9786096256684492, 'f1-score': 0.98306503224536, 'support': 516} weighted_avg {'precision': 0.9848641685643963, 'recall': 0.9844961240310077, 'f1-score': 0.9844197991357732, 'support': 516}
 
time = 0.99 secondes

Val loss 2.2279078662395477 accuracy 0.75 macro_avg {'precision': 0.7708333333333333, 'recall': 0.7105263157894737, 'f1-score': 0.7165005537098561, 'support': 64} weighted_avg {'precision': 0.7630208333333333, 'recall': 0.75, 'f1-score': 0.7347729789590256, 'support': 64}
 
----------
Epoch 36/40
time = 26.24 secondes

Train loss 0.01103571493877098 accuracy 0.998062014579773 macro_avg {'precision': 0.9973404255319149, 'recall': 0.9984802431610942, 'f1-score': 0.9979056316590563, 'support': 516} weighted_avg {'precision': 0.9980723239320469, 'recall': 0.998062015503876, 'f1-score': 0.9980631246091585, 'support': 516}
 
time = 0.99 secondes

Val loss 1.7251185178756714 accuracy 0.796875 macro_avg {'precision': 0.7892892892892893, 'recall': 0.7925101214574899, 'f1-score': 0.790691823899371, 'support': 64} weighted_avg {'precision': 0.7983921421421422, 'recall': 0.796875, 'f1-score': 0.7974371069182389, 'support': 64}
 
----------
Epoch 37/40
time = 26.36 secondes

Train loss 0.0001888646057164861 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.98 secondes

Val loss 2.0661932229995728 accuracy 0.765625 macro_avg {'precision': 0.7841051314142677, 'recall': 0.7297570850202428, 'f1-score': 0.73734610123119, 'support': 64} weighted_avg {'precision': 0.7767130788485607, 'recall': 0.765625, 'f1-score': 0.7535054719562242, 'support': 64}
 
----------
Epoch 38/40
time = 26.30 secondes

Train loss 0.024561048492394162 accuracy 0.9941860437393188 macro_avg {'precision': 0.9954819277108433, 'recall': 0.9919786096256684, 'f1-score': 0.9936875843592369, 'support': 516} weighted_avg {'precision': 0.9942385822359204, 'recall': 0.9941860465116279, 'f1-score': 0.9941757335015786, 'support': 516}
 
time = 0.99 secondes

Val loss 2.0857282876968384 accuracy 0.75 macro_avg {'precision': 0.7885714285714285, 'recall': 0.7044534412955465, 'f1-score': 0.709090909090909, 'support': 64} weighted_avg {'precision': 0.7757142857142857, 'recall': 0.75, 'f1-score': 0.7295454545454545, 'support': 64}
 
----------
Epoch 39/40
time = 26.41 secondes

Train loss 0.0077381870774622784 accuracy 0.998062014579773 macro_avg {'precision': 0.9984848484848485, 'recall': 0.9973262032085561, 'f1-score': 0.997900792084847, 'support': 516} weighted_avg {'precision': 0.9980678881841673, 'recall': 0.998062015503876, 'f1-score': 0.9980608880673791, 'support': 516}
 
time = 0.99 secondes

Val loss 2.118749976158142 accuracy 0.765625 macro_avg {'precision': 0.7841051314142677, 'recall': 0.7297570850202428, 'f1-score': 0.73734610123119, 'support': 64} weighted_avg {'precision': 0.7767130788485607, 'recall': 0.765625, 'f1-score': 0.7535054719562242, 'support': 64}
 
----------
Epoch 40/40
time = 26.29 secondes

Train loss 0.00013664773128270596 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 1.00 secondes

Val loss 2.127288281917572 accuracy 0.765625 macro_avg {'precision': 0.7841051314142677, 'recall': 0.7297570850202428, 'f1-score': 0.73734610123119, 'support': 64} weighted_avg {'precision': 0.7767130788485607, 'recall': 0.765625, 'f1-score': 0.7535054719562242, 'support': 64}
 
----------
best_accuracy 0.84375 best_epoch 33 macro_avg {'precision': 0.8392156862745098, 'recall': 0.8502024291497976, 'f1-score': 0.8412698412698413, 'support': 64} weighted_avg {'precision': 0.8528186274509804, 'recall': 0.84375, 'f1-score': 0.8449900793650793, 'support': 64}

average train time 26.358742129802703

average val time 0.9901659250259399
 
time = 1.08 secondes

test_accuracy 0.9538461565971375 macro_avg {'precision': 0.9507722007722008, 'recall': 0.9551656920077972, 'f1-score': 0.9527272727272726, 'support': 65} weighted_avg {'precision': 0.9545292545292546, 'recall': 0.9538461538461539, 'f1-score': 0.9539580419580419, 'support': 65}

----------
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
Hyperpartisan_BERT_head_text_rank_1
----------
Epoch 1/40
time = 26.11 secondes

Train loss 0.6449768615491462 accuracy 0.6375969052314758 macro_avg {'precision': 0.3187984496124031, 'recall': 0.5, 'f1-score': 0.38934911242603554, 'support': 516} weighted_avg {'precision': 0.40652980590108767, 'recall': 0.6375968992248062, 'f1-score': 0.4964955735975415, 'support': 516}
 
time = 1.00 secondes

Val loss 0.6409585773944855 accuracy 0.59375 macro_avg {'precision': 0.296875, 'recall': 0.5, 'f1-score': 0.37254901960784315, 'support': 64} weighted_avg {'precision': 0.3525390625, 'recall': 0.59375, 'f1-score': 0.4424019607843137, 'support': 64}
 
----------
Epoch 2/40
time = 26.16 secondes

Train loss 0.5573652793060649 accuracy 0.6996123790740967 macro_avg {'precision': 0.8128472222222223, 'recall': 0.5878695772312794, 'f1-score': 0.5566690871196793, 'support': 516} weighted_avg {'precision': 0.7766324827734712, 'recall': 0.6996124031007752, 'f1-score': 0.6259453699501081, 'support': 516}
 
time = 0.98 secondes

Val loss 0.565097488462925 accuracy 0.765625 macro_avg {'precision': 0.8006802721088435, 'recall': 0.7236842105263157, 'f1-score': 0.7308662741799832, 'support': 64} weighted_avg {'precision': 0.7883078231292517, 'recall': 0.765625, 'f1-score': 0.7490012615643398, 'support': 64}
 
----------
Epoch 3/40
time = 26.12 secondes

Train loss 0.40865102274851367 accuracy 0.8217054009437561 macro_avg {'precision': 0.8062921984922646, 'recall': 0.8163288526242218, 'f1-score': 0.810400191708603, 'support': 516} weighted_avg {'precision': 0.8261892744625744, 'recall': 0.8217054263565892, 'f1-score': 0.8231410117087146, 'support': 516}
 
time = 0.99 secondes

Val loss 0.4893980622291565 accuracy 0.734375 macro_avg {'precision': 0.744055068836045, 'recall': 0.6973684210526316, 'f1-score': 0.7023255813953488, 'support': 64} weighted_avg {'precision': 0.740183041301627, 'recall': 0.734375, 'f1-score': 0.7206395348837209, 'support': 64}
 
----------
Epoch 4/40
time = 26.16 secondes

Train loss 0.29383619490898016 accuracy 0.893410861492157 macro_avg {'precision': 0.8856731272917758, 'recall': 0.8829462152365782, 'f1-score': 0.8842723799193415, 'support': 516} weighted_avg {'precision': 0.8930971611420404, 'recall': 0.8934108527131783, 'f1-score': 0.8932217808622713, 'support': 516}
 
time = 0.99 secondes

Val loss 0.4050730764865875 accuracy 0.84375 macro_avg {'precision': 0.8373015873015872, 'recall': 0.8441295546558705, 'f1-score': 0.8398398398398399, 'support': 64} weighted_avg {'precision': 0.8469742063492063, 'recall': 0.84375, 'f1-score': 0.8445320320320321, 'support': 64}
 
----------
Epoch 5/40
time = 26.14 secondes

Train loss 0.2340428763153878 accuracy 0.9166666865348816 macro_avg {'precision': 0.912797619047619, 'recall': 0.9057994571135999, 'f1-score': 0.9090860666653009, 'support': 516} weighted_avg {'precision': 0.9163194444444446, 'recall': 0.9166666666666666, 'f1-score': 0.9163105310961327, 'support': 516}
 
time = 0.99 secondes

Val loss 0.8126554638147354 accuracy 0.78125 macro_avg {'precision': 0.8125, 'recall': 0.742914979757085, 'f1-score': 0.751937984496124, 'support': 64} weighted_avg {'precision': 0.80078125, 'recall': 0.78125, 'f1-score': 0.7679263565891474, 'support': 64}
 
----------
Epoch 6/40
time = 25.99 secondes

Train loss 0.30249680522264855 accuracy 0.895348846912384 macro_avg {'precision': 0.8836768538261075, 'recall': 0.8948523316483267, 'f1-score': 0.8884869052955113, 'support': 516} weighted_avg {'precision': 0.8982156401455256, 'recall': 0.8953488372093024, 'f1-score': 0.8960993610123732, 'support': 516}
 
time = 0.99 secondes

Val loss 0.6918926201760769 accuracy 0.78125 macro_avg {'precision': 0.776470588235294, 'recall': 0.785425101214575, 'f1-score': 0.7777777777777777, 'support': 64} weighted_avg {'precision': 0.7908088235294117, 'recall': 0.78125, 'f1-score': 0.782986111111111, 'support': 64}
 
----------
Epoch 7/40
time = 26.12 secondes

Train loss 0.15743065001725248 accuracy 0.9573643207550049 macro_avg {'precision': 0.9529634483762924, 'recall': 0.9550249500186923, 'f1-score': 0.953976388168137, 'support': 516} weighted_avg {'precision': 0.9574919031927781, 'recall': 0.9573643410852714, 'f1-score': 0.957412740412659, 'support': 516}
 
time = 0.99 secondes

Val loss 0.5073987152427435 accuracy 0.84375 macro_avg {'precision': 0.8373015873015872, 'recall': 0.8441295546558705, 'f1-score': 0.8398398398398399, 'support': 64} weighted_avg {'precision': 0.8469742063492063, 'recall': 0.84375, 'f1-score': 0.8445320320320321, 'support': 64}
 
----------
Epoch 8/40
time = 25.96 secondes

Train loss 0.1474672927323616 accuracy 0.9573643207550049 macro_avg {'precision': 0.9581917344959634, 'recall': 0.9492547502560018, 'f1-score': 0.9534288386747403, 'support': 516} weighted_avg {'precision': 0.9574481277597718, 'recall': 0.9573643410852714, 'f1-score': 0.9571544476233763, 'support': 516}
 
time = 0.99 secondes

Val loss 0.4361864887177944 accuracy 0.859375 macro_avg {'precision': 0.8709856035437431, 'recall': 0.8390688259109311, 'f1-score': 0.8486997635933806, 'support': 64} weighted_avg {'precision': 0.8646525470653379, 'recall': 0.859375, 'f1-score': 0.8562352245862884, 'support': 64}
 
----------
Epoch 9/40
time = 26.16 secondes

Train loss 0.1464367859827524 accuracy 0.963178277015686 macro_avg {'precision': 0.9639880952380953, 'recall': 0.9561221006777954, 'f1-score': 0.9598287271311794, 'support': 516} weighted_avg {'precision': 0.9632509689922482, 'recall': 0.9631782945736435, 'f1-score': 0.9630209323448028, 'support': 516}
 
time = 0.98 secondes

Val loss 0.770469181239605 accuracy 0.84375 macro_avg {'precision': 0.8380566801619433, 'recall': 0.8380566801619433, 'f1-score': 0.8380566801619433, 'support': 64} weighted_avg {'precision': 0.84375, 'recall': 0.84375, 'f1-score': 0.84375, 'support': 64}
 
----------
Epoch 10/40
time = 26.00 secondes

Train loss 0.36284910247548285 accuracy 0.9224806427955627 macro_avg {'precision': 0.9147160692710431, 'recall': 0.9184370072980836, 'f1-score': 0.9165089073345956, 'support': 516} weighted_avg {'precision': 0.9229441754316953, 'recall': 0.9224806201550387, 'f1-score': 0.9226537132802691, 'support': 516}
 
time = 0.99 secondes

Val loss 0.7389279752969742 accuracy 0.828125 macro_avg {'precision': 0.823076923076923, 'recall': 0.8188259109311742, 'f1-score': 0.8207282913165266, 'support': 64} weighted_avg {'precision': 0.8274038461538462, 'recall': 0.828125, 'f1-score': 0.8275560224089636, 'support': 64}
 
----------
Epoch 11/40
time = 26.15 secondes

Train loss 0.20429191657934676 accuracy 0.9515503644943237 macro_avg {'precision': 0.9480449657869012, 'recall': 0.9470035596443607, 'f1-score': 0.9475198021211764, 'support': 516} weighted_avg {'precision': 0.9515017011828714, 'recall': 0.9515503875968992, 'f1-score': 0.9515222016844816, 'support': 516}
 
time = 0.99 secondes

Val loss 1.1490284502506256 accuracy 0.78125 macro_avg {'precision': 0.8125, 'recall': 0.742914979757085, 'f1-score': 0.751937984496124, 'support': 64} weighted_avg {'precision': 0.80078125, 'recall': 0.78125, 'f1-score': 0.7679263565891474, 'support': 64}
 
----------
Epoch 12/40
time = 26.12 secondes

Train loss 0.1791614042030591 accuracy 0.9593023061752319 macro_avg {'precision': 0.9530479605558047, 'recall': 0.9600068267152122, 'f1-score': 0.9563119126238253, 'support': 516} weighted_avg {'precision': 0.9599863967560693, 'recall': 0.9593023255813954, 'f1-score': 0.9594573840310471, 'support': 516}
 
time = 0.99 secondes

Val loss 1.4133387506008148 accuracy 0.765625 macro_avg {'precision': 0.8242835595776772, 'recall': 0.7176113360323887, 'f1-score': 0.7234226447709595, 'support': 64} weighted_avg {'precision': 0.8057598039215685, 'recall': 0.765625, 'f1-score': 0.743679775280899, 'support': 64}
 
----------
Epoch 13/40
time = 26.03 secondes

Train loss 0.42757514567876403 accuracy 0.9127907156944275 macro_avg {'precision': 0.9181679600886918, 'recall': 0.8923735838629456, 'f1-score': 0.9028577883203435, 'support': 516} weighted_avg {'precision': 0.9141064108183365, 'recall': 0.9127906976744186, 'f1-score': 0.9114061103099113, 'support': 516}
 
time = 0.98 secondes

Val loss 0.7391950897872448 accuracy 0.859375 macro_avg {'precision': 0.8558974358974358, 'recall': 0.8512145748987854, 'f1-score': 0.8533231474407945, 'support': 64} weighted_avg {'precision': 0.8588782051282051, 'recall': 0.859375, 'f1-score': 0.858909472880061, 'support': 64}
 
----------
Epoch 14/40
time = 26.06 secondes

Train loss 0.08281957736648055 accuracy 0.9806201457977295 macro_avg {'precision': 0.9812927681780141, 'recall': 0.9767241519431757, 'f1-score': 0.978933616395852, 'support': 516} weighted_avg {'precision': 0.98065602773952, 'recall': 0.9806201550387597, 'f1-score': 0.9805739485005979, 'support': 516}
 
time = 0.98 secondes

Val loss 1.036994844675064 accuracy 0.828125 macro_avg {'precision': 0.8355481727574751, 'recall': 0.8066801619433198, 'f1-score': 0.8150774888363541, 'support': 64} weighted_avg {'precision': 0.831499169435216, 'recall': 0.828125, 'f1-score': 0.8242874967165746, 'support': 64}
 
----------
Epoch 15/40
time = 26.18 secondes

Train loss 0.2884641893730132 accuracy 0.9437984228134155 macro_avg {'precision': 0.9556915672734881, 'recall': 0.9247679729532046, 'f1-score': 0.9372228538346332, 'support': 516} weighted_avg {'precision': 0.9468956156699774, 'recall': 0.9437984496124031, 'f1-score': 0.9428140789869884, 'support': 516}
 
time = 1.02 secondes

Val loss 1.2363505586981773 accuracy 0.765625 macro_avg {'precision': 0.7629521016617791, 'recall': 0.7722672064777327, 'f1-score': 0.7627872498146775, 'support': 64} weighted_avg {'precision': 0.7789894916911047, 'recall': 0.765625, 'f1-score': 0.7676519644180875, 'support': 64}
 
----------
Epoch 16/40
time = 26.26 secondes

Train loss 0.11406150602058253 accuracy 0.9748061895370483 macro_avg {'precision': 0.9732649071358749, 'recall': 0.9721648814264584, 'f1-score': 0.9727102971030117, 'support': 516} weighted_avg {'precision': 0.9747847946835194, 'recall': 0.9748062015503876, 'f1-score': 0.9747915448759306, 'support': 516}
 
time = 0.99 secondes

Val loss 1.2369011342525482 accuracy 0.78125 macro_avg {'precision': 0.776470588235294, 'recall': 0.785425101214575, 'f1-score': 0.7777777777777777, 'support': 64} weighted_avg {'precision': 0.7908088235294117, 'recall': 0.78125, 'f1-score': 0.782986111111111, 'support': 64}
 
----------
Epoch 17/40
time = 26.09 secondes

Train loss 0.42448465693671483 accuracy 0.9224806427955627 macro_avg {'precision': 0.925668877785925, 'recall': 0.9057425678201648, 'f1-score': 0.914234425902533, 'support': 516} weighted_avg {'precision': 0.9231254363051058, 'recall': 0.9224806201550387, 'f1-score': 0.9215529233016319, 'support': 516}
 
time = 0.98 secondes

Val loss 1.8960042893886566 accuracy 0.703125 macro_avg {'precision': 0.7808080808080808, 'recall': 0.6406882591093117, 'f1-score': 0.6264208909370199, 'support': 64} weighted_avg {'precision': 0.7605429292929293, 'recall': 0.703125, 'f1-score': 0.6581605222734255, 'support': 64}
 
----------
Epoch 18/40
time = 26.14 secondes

Train loss 0.07137340891781743 accuracy 0.9864341020584106 macro_avg {'precision': 0.9870350969093766, 'recall': 0.9835915023649693, 'f1-score': 0.9852710301715525, 'support': 516} weighted_avg {'precision': 0.9864584729210066, 'recall': 0.9864341085271318, 'f1-score': 0.9864100448370163, 'support': 516}
 
time = 0.98 secondes

Val loss 0.758607029914856 accuracy 0.859375 macro_avg {'precision': 0.8616118769883351, 'recall': 0.8451417004048583, 'f1-score': 0.8512012399896668, 'support': 64} weighted_avg {'precision': 0.8601206256627784, 'recall': 0.859375, 'f1-score': 0.8577402479979334, 'support': 64}
 
----------
Epoch 19/40
time = 26.08 secondes

Train loss 0.10430321354193217 accuracy 0.9806201457977295 macro_avg {'precision': 0.9790322318482518, 'recall': 0.9790322318482518, 'f1-score': 0.9790322318482518, 'support': 516} weighted_avg {'precision': 0.9806201550387597, 'recall': 0.9806201550387597, 'f1-score': 0.9806201550387597, 'support': 516}
 
time = 0.98 secondes

Val loss 1.547379195690155 accuracy 0.78125 macro_avg {'precision': 0.8342857142857143, 'recall': 0.736842105263158, 'f1-score': 0.7454545454545455, 'support': 64} weighted_avg {'precision': 0.8166071428571429, 'recall': 0.78125, 'f1-score': 0.7633522727272728, 'support': 64}
 
----------
Epoch 20/40
time = 26.03 secondes

Train loss 0.13062894934873012 accuracy 0.9748061895370483 macro_avg {'precision': 0.9681246426529446, 'recall': 0.9790891211416868, 'f1-score': 0.9730705152652603, 'support': 516} weighted_avg {'precision': 0.9760311540149189, 'recall': 0.9748062015503876, 'f1-score': 0.9749519462002839, 'support': 516}
 
time = 0.98 secondes

Val loss 2.012249380350113 accuracy 0.703125 macro_avg {'precision': 0.7808080808080808, 'recall': 0.6406882591093117, 'f1-score': 0.6264208909370199, 'support': 64} weighted_avg {'precision': 0.7605429292929293, 'recall': 0.703125, 'f1-score': 0.6581605222734255, 'support': 64}
 
----------
Epoch 21/40
time = 26.04 secondes

Train loss 0.11252281056702927 accuracy 0.9709302186965942 macro_avg {'precision': 0.9701414353064431, 'recall': 0.9668172878435708, 'f1-score': 0.968437921796184, 'support': 516} weighted_avg {'precision': 0.9708982542911787, 'recall': 0.9709302325581395, 'f1-score': 0.9708786675078922, 'support': 516}
 
time = 0.98 secondes

Val loss 1.6095744967460632 accuracy 0.703125 macro_avg {'precision': 0.7348717948717949, 'recall': 0.7317813765182186, 'f1-score': 0.703052503052503, 'support': 64} weighted_avg {'precision': 0.7620833333333333, 'recall': 0.703125, 'f1-score': 0.7021825396825396, 'support': 64}
 
----------
Epoch 22/40
time = 26.01 secondes

Train loss 0.18810042956928638 accuracy 0.963178277015686 macro_avg {'precision': 0.9652449970081777, 'recall': 0.9549680607252573, 'f1-score': 0.9597297241790064, 'support': 516} weighted_avg {'precision': 0.9634107985975287, 'recall': 0.9631782945736435, 'f1-score': 0.9629727506428373, 'support': 516}
 
time = 0.98 secondes

Val loss 1.5868148505687714 accuracy 0.765625 macro_avg {'precision': 0.8006802721088435, 'recall': 0.7236842105263157, 'f1-score': 0.7308662741799832, 'support': 64} weighted_avg {'precision': 0.7883078231292517, 'recall': 0.765625, 'f1-score': 0.7490012615643398, 'support': 64}
 
----------
Epoch 23/40
time = 26.00 secondes

Train loss 0.025348196998520783 accuracy 0.9961240291595459 macro_avg {'precision': 0.9958064463696503, 'recall': 0.9958064463696503, 'f1-score': 0.9958064463696503, 'support': 516} weighted_avg {'precision': 0.9961240310077519, 'recall': 0.9961240310077519, 'f1-score': 0.9961240310077519, 'support': 516}
 
time = 1.01 secondes

Val loss 2.1262461841106415 accuracy 0.6875 macro_avg {'precision': 0.6971428571428572, 'recall': 0.6396761133603239, 'f1-score': 0.6363636363636364, 'support': 64} weighted_avg {'precision': 0.6939285714285715, 'recall': 0.6875, 'f1-score': 0.6619318181818181, 'support': 64}
 
----------
Epoch 24/40
time = 25.97 secondes

Train loss 0.10603645810271811 accuracy 0.9806201457977295 macro_avg {'precision': 0.9812927681780141, 'recall': 0.9767241519431757, 'f1-score': 0.978933616395852, 'support': 516} weighted_avg {'precision': 0.98065602773952, 'recall': 0.9806201550387597, 'f1-score': 0.9805739485005979, 'support': 516}
 
time = 1.03 secondes

Val loss 1.6031638383865356 accuracy 0.75 macro_avg {'precision': 0.75, 'recall': 0.7226720647773279, 'f1-score': 0.7285259809119831, 'support': 64} weighted_avg {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.7428419936373276, 'support': 64}
 
----------
Epoch 25/40
time = 25.89 secondes

Train loss 0.04513287368480991 accuracy 0.9864341020584106 macro_avg {'precision': 0.9895833333333333, 'recall': 0.981283422459893, 'f1-score': 0.9852000573641189, 'support': 516} weighted_avg {'precision': 0.9867167312661498, 'recall': 0.9864341085271318, 'f1-score': 0.986376132969138, 'support': 516}
 
time = 0.97 secondes

Val loss 1.5364571288228035 accuracy 0.75 macro_avg {'precision': 0.7568627450980392, 'recall': 0.7651821862348178, 'f1-score': 0.7490196078431374, 'support': 64} weighted_avg {'precision': 0.777450980392157, 'recall': 0.75, 'f1-score': 0.7519607843137257, 'support': 64}
 
----------
Epoch 26/40
time = 25.98 secondes

Train loss 0.1318096421600785 accuracy 0.9728682041168213 macro_avg {'precision': 0.9657593963508394, 'recall': 0.9775693643027811, 'f1-score': 0.9710293716613998, 'support': 516} weighted_avg {'precision': 0.9743140788922482, 'recall': 0.9728682170542635, 'f1-score': 0.9730379566289895, 'support': 516}
 
time = 0.99 secondes

Val loss 1.669739730656147 accuracy 0.796875 macro_avg {'precision': 0.8241551939924906, 'recall': 0.7621457489878543, 'f1-score': 0.7723666210670315, 'support': 64} weighted_avg {'precision': 0.8132431163954943, 'recall': 0.796875, 'f1-score': 0.7863714090287277, 'support': 64}
 
----------
Epoch 27/40
time = 25.97 secondes

Train loss 0.039961238729478224 accuracy 0.9883720874786377 macro_avg {'precision': 0.9844559585492227, 'recall': 0.9908814589665653, 'f1-score': 0.9875040361640297, 'support': 516} weighted_avg {'precision': 0.9887335823593204, 'recall': 0.9883720930232558, 'f1-score': 0.9884103896493981, 'support': 516}
 
time = 0.99 secondes

Val loss 1.244026091415435 accuracy 0.8125 macro_avg {'precision': 0.8125, 'recall': 0.8238866396761133, 'f1-score': 0.8108374384236454, 'support': 64} weighted_avg {'precision': 0.830078125, 'recall': 0.8125, 'f1-score': 0.8141625615763547, 'support': 64}
 
----------
Epoch 28/40
time = 25.99 secondes

Train loss 0.018082002405814517 accuracy 0.9922480583190918 macro_avg {'precision': 0.9927655752429166, 'recall': 0.9904588527867627, 'f1-score': 0.9915933528836756, 'support': 516} weighted_avg {'precision': 0.9922622404600905, 'recall': 0.9922480620155039, 'f1-score': 0.9922389688331174, 'support': 516}
 
time = 0.98 secondes

Val loss 1.530873188865371 accuracy 0.796875 macro_avg {'precision': 0.7902564102564102, 'recall': 0.7864372469635628, 'f1-score': 0.7881334351922588, 'support': 64} weighted_avg {'precision': 0.7959294871794872, 'recall': 0.796875, 'f1-score': 0.7962025719378661, 'support': 64}
 
----------
Epoch 29/40
time = 26.08 secondes

Train loss 0.0005243211436953226 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.98 secondes

Val loss 1.7606234848499298 accuracy 0.78125 macro_avg {'precision': 0.8125, 'recall': 0.742914979757085, 'f1-score': 0.751937984496124, 'support': 64} weighted_avg {'precision': 0.80078125, 'recall': 0.78125, 'f1-score': 0.7679263565891474, 'support': 64}
 
----------
Epoch 30/40
time = 25.91 secondes

Train loss 0.24966408159856324 accuracy 0.961240291595459 macro_avg {'precision': 0.9713467048710602, 'recall': 0.946524064171123, 'f1-score': 0.9570021498925053, 'support': 516} weighted_avg {'precision': 0.9634614957464295, 'recall': 0.9612403100775194, 'f1-score': 0.9607170804250486, 'support': 516}
 
time = 1.00 secondes

Val loss 1.5853553712368011 accuracy 0.78125 macro_avg {'precision': 0.78125, 'recall': 0.791497975708502, 'f1-score': 0.7793103448275862, 'support': 64} weighted_avg {'precision': 0.798828125, 'recall': 0.78125, 'f1-score': 0.7831896551724138, 'support': 64}
 
----------
Epoch 31/40
time = 26.03 secondes

Train loss 0.12933098640430465 accuracy 0.9767441749572754 macro_avg {'precision': 0.9698492462311558, 'recall': 0.9817629179331306, 'f1-score': 0.9751680328526284, 'support': 516} weighted_avg {'precision': 0.9781465466869229, 'recall': 0.9767441860465116, 'f1-score': 0.9768896771105624, 'support': 516}
 
time = 0.98 secondes

Val loss 1.5568786263465881 accuracy 0.796875 macro_avg {'precision': 0.7906403940886699, 'recall': 0.798582995951417, 'f1-score': 0.7927770859277709, 'support': 64} weighted_avg {'precision': 0.8031096059113301, 'recall': 0.796875, 'f1-score': 0.7982409713574097, 'support': 64}
 
----------
Epoch 32/40
time = 25.82 secondes

Train loss 0.10572692204401722 accuracy 0.9903100728988647 macro_avg {'precision': 0.9925149700598803, 'recall': 0.9866310160427807, 'f1-score': 0.9894541931844658, 'support': 516} weighted_avg {'precision': 0.9904551362391496, 'recall': 0.9903100775193798, 'f1-score': 0.990280965807308, 'support': 516}
 
time = 0.98 secondes

Val loss 2.3419799506664276 accuracy 0.75 macro_avg {'precision': 0.8518518518518519, 'recall': 0.6923076923076923, 'f1-score': 0.6908212560386473, 'support': 64} weighted_avg {'precision': 0.8240740740740741, 'recall': 0.75, 'f1-score': 0.716183574879227, 'support': 64}
 
----------
Epoch 33/40
time = 25.89 secondes

Train loss 0.01786110847641601 accuracy 0.998062014579773 macro_avg {'precision': 0.9984848484848485, 'recall': 0.9973262032085561, 'f1-score': 0.997900792084847, 'support': 516} weighted_avg {'precision': 0.9980678881841673, 'recall': 0.998062015503876, 'f1-score': 0.9980608880673791, 'support': 516}
 
time = 0.98 secondes

Val loss 1.2802133799996227 accuracy 0.828125 macro_avg {'precision': 0.822167487684729, 'recall': 0.8309716599190283, 'f1-score': 0.8246575342465753, 'support': 64} weighted_avg {'precision': 0.8340825123152709, 'recall': 0.828125, 'f1-score': 0.829280821917808, 'support': 64}
 
----------
Epoch 34/40
time = 25.90 secondes

Train loss 0.026897820414777732 accuracy 0.9961240291595459 macro_avg {'precision': 0.9947089947089947, 'recall': 0.9969604863221885, 'f1-score': 0.9958160352880125, 'support': 516} weighted_avg {'precision': 0.9961650465526435, 'recall': 0.9961240310077519, 'f1-score': 0.9961284309466054, 'support': 516}
 
time = 0.98 secondes

Val loss 1.022511397342896 accuracy 0.84375 macro_avg {'precision': 0.8380566801619433, 'recall': 0.8380566801619433, 'f1-score': 0.8380566801619433, 'support': 64} weighted_avg {'precision': 0.84375, 'recall': 0.84375, 'f1-score': 0.84375, 'support': 64}
 
----------
Epoch 35/40
time = 26.19 secondes

Train loss 0.0160260481180475 accuracy 0.9941860437393188 macro_avg {'precision': 0.9954819277108433, 'recall': 0.9919786096256684, 'f1-score': 0.9936875843592369, 'support': 516} weighted_avg {'precision': 0.9942385822359204, 'recall': 0.9941860465116279, 'f1-score': 0.9941757335015786, 'support': 516}
 
time = 0.98 secondes

Val loss 1.0397284732171101 accuracy 0.859375 macro_avg {'precision': 0.8847953216374269, 'recall': 0.832995951417004, 'f1-score': 0.8457831325301204, 'support': 64} weighted_avg {'precision': 0.873062865497076, 'recall': 0.859375, 'f1-score': 0.8543674698795181, 'support': 64}
 
----------
Epoch 36/40
time = 25.99 secondes

Train loss 0.00019323047926070902 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.99 secondes

Val loss 1.7249045819044113 accuracy 0.78125 macro_avg {'precision': 0.776470588235294, 'recall': 0.785425101214575, 'f1-score': 0.7777777777777777, 'support': 64} weighted_avg {'precision': 0.7908088235294117, 'recall': 0.78125, 'f1-score': 0.782986111111111, 'support': 64}
 
----------
Epoch 37/40
time = 26.02 secondes

Train loss 0.012514501826830603 accuracy 0.998062014579773 macro_avg {'precision': 0.9973404255319149, 'recall': 0.9984802431610942, 'f1-score': 0.9979056316590563, 'support': 516} weighted_avg {'precision': 0.9980723239320469, 'recall': 0.998062015503876, 'f1-score': 0.9980631246091585, 'support': 516}
 
time = 0.99 secondes

Val loss 1.9605919793248177 accuracy 0.765625 macro_avg {'precision': 0.7629521016617791, 'recall': 0.7722672064777327, 'f1-score': 0.7627872498146775, 'support': 64} weighted_avg {'precision': 0.7789894916911047, 'recall': 0.765625, 'f1-score': 0.7676519644180875, 'support': 64}
 
----------
Epoch 38/40
time = 26.07 secondes

Train loss 0.00012468473384312043 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.98 secondes

Val loss 1.3040162244578823 accuracy 0.828125 macro_avg {'precision': 0.8213213213213213, 'recall': 0.8248987854251012, 'f1-score': 0.8228930817610063, 'support': 64} weighted_avg {'precision': 0.8294857357357358, 'recall': 0.828125, 'f1-score': 0.8286006289308177, 'support': 64}
 
----------
Epoch 39/40
time = 25.89 secondes

Train loss 0.00010911855806074472 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.99 secondes

Val loss 1.0856217437903979 accuracy 0.859375 macro_avg {'precision': 0.8709856035437431, 'recall': 0.8390688259109311, 'f1-score': 0.8486997635933806, 'support': 64} weighted_avg {'precision': 0.8646525470653379, 'recall': 0.859375, 'f1-score': 0.8562352245862884, 'support': 64}
 
----------
Epoch 40/40
time = 25.88 secondes

Train loss 9.999064875018755e-05 accuracy 1.0 macro_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516} weighted_avg {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 516}
 
time = 0.98 secondes

Val loss 1.1511753885060898 accuracy 0.875 macro_avg {'precision': 0.8954545454545455, 'recall': 0.8522267206477733, 'f1-score': 0.8642629904559915, 'support': 64} weighted_avg {'precision': 0.8852272727272728, 'recall': 0.875, 'f1-score': 0.8714209968186639, 'support': 64}
 
----------
best_accuracy 0.875 best_epoch 40 macro_avg {'precision': 0.8954545454545455, 'recall': 0.8522267206477733, 'f1-score': 0.8642629904559915, 'support': 64} weighted_avg {'precision': 0.8852272727272728, 'recall': 0.875, 'f1-score': 0.8714209968186639, 'support': 64}

average train time 26.03961008787155

average val time 0.988084465265274
 
time = 1.04 secondes

test_accuracy 0.9692307710647583 macro_avg {'precision': 0.975, 'recall': 0.962962962962963, 'f1-score': 0.9679487179487178, 'support': 65} weighted_avg {'precision': 0.9707692307692308, 'recall': 0.9692307692307692, 'f1-score': 0.969033530571992, 'support': 65}

----------
datasets imported
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_head_tail_1
----------
Epoch 1/40
time = 517.61 secondes

Train loss 1.254808080346094 accuracy 0.6837556958198547 macro_avg {'precision': 0.6957057536153191, 'recall': 0.668616988007338, 'f1-score': 0.6622788186558621, 'support': 10182} weighted_avg {'precision': 0.705061271444689, 'recall': 0.6837556472205853, 'f1-score': 0.6765578934023104, 'support': 10182}
 
time = 18.17 secondes

Val loss 0.6202786466185476 accuracy 0.8286219239234924 macro_avg {'precision': 0.8010364633429312, 'recall': 0.8226349726404365, 'f1-score': 0.8065398545365744, 'support': 1132} weighted_avg {'precision': 0.8126220866659833, 'recall': 0.8286219081272085, 'f1-score': 0.8154373117761239, 'support': 1132}
 
----------
Epoch 2/40
time = 511.62 secondes

Train loss 0.43584281003199155 accuracy 0.8748772740364075 macro_avg {'precision': 0.8666589729542779, 'recall': 0.8643311966545764, 'f1-score': 0.8632018673508638, 'support': 10182} weighted_avg {'precision': 0.8734220016977908, 'recall': 0.8748772343351011, 'f1-score': 0.8724089837744172, 'support': 10182}
 
time = 17.93 secondes

Val loss 0.47189863152067424 accuracy 0.8692579865455627 macro_avg {'precision': 0.8715751155355477, 'recall': 0.8651676661241379, 'f1-score': 0.8611875321928825, 'support': 1132} weighted_avg {'precision': 0.8720532898321993, 'recall': 0.8692579505300353, 'f1-score': 0.8643093110240648, 'support': 1132}
 
----------
Epoch 3/40
time = 511.52 secondes

Train loss 0.27061287581400134 accuracy 0.922412097454071 macro_avg {'precision': 0.9183113402600884, 'recall': 0.916871552951798, 'f1-score': 0.9173137934746792, 'support': 10182} weighted_avg {'precision': 0.9223690649950802, 'recall': 0.9224120997839325, 'f1-score': 0.9221409459319915, 'support': 10182}
 
time = 18.08 secondes

Val loss 0.488228929963645 accuracy 0.8886925578117371 macro_avg {'precision': 0.8993837268517421, 'recall': 0.8952843745940532, 'f1-score': 0.8923161729697684, 'support': 1132} weighted_avg {'precision': 0.9007114420734538, 'recall': 0.8886925795053003, 'f1-score': 0.8891449845618572, 'support': 1132}
 
----------
Epoch 4/40
time = 511.69 secondes

Train loss 0.2116524964955517 accuracy 0.9441170692443848 macro_avg {'precision': 0.941329017726531, 'recall': 0.940964065717184, 'f1-score': 0.9410841492295736, 'support': 10182} weighted_avg {'precision': 0.9442669216943869, 'recall': 0.9441170693380475, 'f1-score': 0.9441329991627894, 'support': 10182}
 
time = 17.80 secondes

Val loss 0.5722832293446425 accuracy 0.8833922147750854 macro_avg {'precision': 0.8947296888406303, 'recall': 0.8881359255391315, 'f1-score': 0.8853016079065268, 'support': 1132} weighted_avg {'precision': 0.8942858985279495, 'recall': 0.8833922261484098, 'f1-score': 0.8818410989633787, 'support': 1132}
 
----------
Epoch 5/40
time = 530.44 secondes

Train loss 0.16607188862701658 accuracy 0.9580633044242859 macro_avg {'precision': 0.9563216428880977, 'recall': 0.9561934182444078, 'f1-score': 0.9562195387963136, 'support': 10182} weighted_avg {'precision': 0.9581218272366105, 'recall': 0.9580632488705558, 'f1-score': 0.9580549198023585, 'support': 10182}
 
time = 21.02 secondes

Val loss 0.4953984243723131 accuracy 0.9107773900032043 macro_avg {'precision': 0.9139044205410597, 'recall': 0.9119556357493115, 'f1-score': 0.9112615249390468, 'support': 1132} weighted_avg {'precision': 0.9138235578530565, 'recall': 0.9107773851590106, 'f1-score': 0.9107503575604122, 'support': 1132}
 
----------
Epoch 6/40
time = 538.91 secondes

Train loss 0.15206336906281032 accuracy 0.9642506837844849 macro_avg {'precision': 0.9630253349893427, 'recall': 0.9628816230160728, 'f1-score': 0.9628806493398502, 'support': 10182} weighted_avg {'precision': 0.9642778251875087, 'recall': 0.9642506383814575, 'f1-score': 0.9641927760397945, 'support': 10182}
 
time = 20.74 secondes

Val loss 0.5535795988734733 accuracy 0.9001767039299011 macro_avg {'precision': 0.9077287808435732, 'recall': 0.8998202274555028, 'f1-score': 0.9009055384732919, 'support': 1132} weighted_avg {'precision': 0.9064977907349818, 'recall': 0.9001766784452296, 'f1-score': 0.9003306031893584, 'support': 1132}
 
----------
Epoch 7/40
time = 524.63 secondes

Train loss 0.15794787694836399 accuracy 0.9658220410346985 macro_avg {'precision': 0.9646898153278978, 'recall': 0.9647123360643102, 'f1-score': 0.9646570713409842, 'support': 10182} weighted_avg {'precision': 0.9659758556322514, 'recall': 0.9658220388921627, 'f1-score': 0.9658580004347876, 'support': 10182}
 
time = 18.57 secondes

Val loss 0.5856309695977678 accuracy 0.9081271886825562 macro_avg {'precision': 0.9109965891919931, 'recall': 0.9084331268239968, 'f1-score': 0.9072337516958257, 'support': 1132} weighted_avg {'precision': 0.9118717759541134, 'recall': 0.9081272084805654, 'f1-score': 0.9076556199432794, 'support': 1132}
 
----------
Epoch 8/40
time = 514.74 secondes

Train loss 0.13123482961450328 accuracy 0.9716166257858276 macro_avg {'precision': 0.9707800076339048, 'recall': 0.9708565586517606, 'f1-score': 0.9707894544520899, 'support': 10182} weighted_avg {'precision': 0.9716168846781044, 'recall': 0.9716165782753879, 'f1-score': 0.9715885312502149, 'support': 10182}
 
time = 18.30 secondes

Val loss 0.6540838029960327 accuracy 0.8913427591323853 macro_avg {'precision': 0.8979549623377334, 'recall': 0.890717997812805, 'f1-score': 0.8913750269272981, 'support': 1132} weighted_avg {'precision': 0.8973795309871702, 'recall': 0.8913427561837456, 'f1-score': 0.8914429094252997, 'support': 1132}
 
----------
Epoch 9/40
time = 507.86 secondes

Train loss 0.14756934935306718 accuracy 0.9700452089309692 macro_avg {'precision': 0.9691267502960546, 'recall': 0.9688475840368147, 'f1-score': 0.9689675417838487, 'support': 10182} weighted_avg {'precision': 0.9699762287590145, 'recall': 0.9700451777646828, 'f1-score': 0.9699916745133841, 'support': 10182}
 
time = 17.73 secondes

Val loss 0.6527933601813036 accuracy 0.9081271886825562 macro_avg {'precision': 0.9113616302694989, 'recall': 0.9086862666395937, 'f1-score': 0.9078380658466649, 'support': 1132} weighted_avg {'precision': 0.9112937416655478, 'recall': 0.9081272084805654, 'f1-score': 0.9075396415076263, 'support': 1132}
 
----------
Epoch 10/40
time = 506.66 secondes

Train loss 0.13278850839013495 accuracy 0.9728933572769165 macro_avg {'precision': 0.9719978606598667, 'recall': 0.9721148947825393, 'f1-score': 0.9720181716844701, 'support': 10182} weighted_avg {'precision': 0.9730135415746232, 'recall': 0.9728933411903359, 'f1-score': 0.9729138612234899, 'support': 10182}
 
time = 17.62 secondes

Val loss 0.7952994355885968 accuracy 0.8948763608932495 macro_avg {'precision': 0.8998468577561199, 'recall': 0.9010282679123798, 'f1-score': 0.8959726390023992, 'support': 1132} weighted_avg {'precision': 0.9015347342207193, 'recall': 0.8948763250883393, 'f1-score': 0.8931542062165008, 'support': 1132}
 
----------
Epoch 11/40
time = 510.65 secondes

Train loss 0.12108428600327652 accuracy 0.9764290452003479 macro_avg {'precision': 0.975861326378429, 'recall': 0.9754843648626335, 'f1-score': 0.975631525636589, 'support': 10182} weighted_avg {'precision': 0.9764459045318709, 'recall': 0.9764289923394225, 'f1-score': 0.9763999159073957, 'support': 10182}
 
time = 18.29 secondes

Val loss 0.7318733275964738 accuracy 0.9010601043701172 macro_avg {'precision': 0.9082188708756125, 'recall': 0.9022208702028756, 'f1-score': 0.9015766449963287, 'support': 1132} weighted_avg {'precision': 0.9099116945254538, 'recall': 0.901060070671378, 'f1-score': 0.9020998353754948, 'support': 1132}
 
----------
Epoch 12/40
time = 514.91 secondes

Train loss 0.1046810666368481 accuracy 0.9799646735191345 macro_avg {'precision': 0.9792260857233034, 'recall': 0.9793297056334852, 'f1-score': 0.9792604089501291, 'support': 10182} weighted_avg {'precision': 0.9800496218452119, 'recall': 0.9799646434885091, 'f1-score': 0.9799902600721841, 'support': 10182}
 
time = 18.17 secondes

Val loss 0.6714287570515373 accuracy 0.9037102460861206 macro_avg {'precision': 0.9086791218039444, 'recall': 0.9052712638549962, 'f1-score': 0.9045348216677598, 'support': 1132} weighted_avg {'precision': 0.9100935207169409, 'recall': 0.9037102473498233, 'f1-score': 0.9043252072218139, 'support': 1132}
 
----------
Epoch 13/40
time = 513.81 secondes

Train loss 0.10010315587752706 accuracy 0.9819289445877075 macro_avg {'precision': 0.9815452284315827, 'recall': 0.9815957352058888, 'f1-score': 0.9815613849762901, 'support': 10182} weighted_avg {'precision': 0.9819514477677502, 'recall': 0.9819288941268906, 'f1-score': 0.9819309496861965, 'support': 10182}
 
time = 18.76 secondes

Val loss 0.69564059035259 accuracy 0.9045936465263367 macro_avg {'precision': 0.9142392243964368, 'recall': 0.9063457968573034, 'f1-score': 0.9079275894437522, 'support': 1132} weighted_avg {'precision': 0.9094771644738714, 'recall': 0.9045936395759717, 'f1-score': 0.9045342099303825, 'support': 1132}
 
----------
Epoch 14/40
time = 514.77 secondes

Train loss 0.10600607231570214 accuracy 0.9811432361602783 macro_avg {'precision': 0.9806071423767374, 'recall': 0.9808199442413154, 'f1-score': 0.9806965358369171, 'support': 10182} weighted_avg {'precision': 0.9812202407303454, 'recall': 0.981143193871538, 'f1-score': 0.9811650344264448, 'support': 10182}
 
time = 18.21 secondes

Val loss 0.7156078404676035 accuracy 0.9028268456459045 macro_avg {'precision': 0.9089195624290554, 'recall': 0.9071946197296533, 'f1-score': 0.9052367960733327, 'support': 1132} weighted_avg {'precision': 0.9087830792303073, 'recall': 0.9028268551236749, 'f1-score': 0.9028443030922206, 'support': 1132}
 
----------
Epoch 15/40
time = 514.77 secondes

Train loss 0.0892501992864172 accuracy 0.9830092787742615 macro_avg {'precision': 0.9831526940402888, 'recall': 0.9821926043835939, 'f1-score': 0.9826128339791216, 'support': 10182} weighted_avg {'precision': 0.9830984853368769, 'recall': 0.9830092319780004, 'f1-score': 0.9830039763696384, 'support': 10182}
 
time = 18.44 secondes

Val loss 0.6989357602869859 accuracy 0.9063604474067688 macro_avg {'precision': 0.9114907607665079, 'recall': 0.909864628012255, 'f1-score': 0.9088381445867808, 'support': 1132} weighted_avg {'precision': 0.9109411480144624, 'recall': 0.9063604240282686, 'f1-score': 0.9066770012113744, 'support': 1132}
 
----------
Epoch 16/40
time = 513.15 secondes

Train loss 0.08903520657071028 accuracy 0.9840896129608154 macro_avg {'precision': 0.9838562950434397, 'recall': 0.9836869549104325, 'f1-score': 0.9837553900905369, 'support': 10182} weighted_avg {'precision': 0.9841155227498904, 'recall': 0.9840895698291102, 'f1-score': 0.9840870893130353, 'support': 10182}
 
time = 18.54 secondes

Val loss 0.6869751709983947 accuracy 0.9081271886825562 macro_avg {'precision': 0.9146393280792919, 'recall': 0.9114782996623486, 'f1-score': 0.910872913686975, 'support': 1132} weighted_avg {'precision': 0.9153139242878282, 'recall': 0.9081272084805654, 'f1-score': 0.909364190326162, 'support': 1132}
 
----------
Epoch 17/40
time = 513.94 secondes

Train loss 0.07700745604302349 accuracy 0.985562801361084 macro_avg {'precision': 0.9854787331859649, 'recall': 0.9850112281138477, 'f1-score': 0.9852168849431528, 'support': 10182} weighted_avg {'precision': 0.9855938304734486, 'recall': 0.9855627578078963, 'f1-score': 0.985555266493075, 'support': 10182}
 
time = 18.55 secondes

Val loss 0.6292801494533885 accuracy 0.9196113348007202 macro_avg {'precision': 0.9235826699565598, 'recall': 0.923621622941275, 'f1-score': 0.9214726451307552, 'support': 1132} weighted_avg {'precision': 0.9243384656438064, 'recall': 0.9196113074204947, 'f1-score': 0.9198953430267915, 'support': 1132}
 
----------
Epoch 18/40
time = 516.75 secondes

Train loss 0.0807407821141263 accuracy 0.985660970211029 macro_avg {'precision': 0.985162370276113, 'recall': 0.9853758161336279, 'f1-score': 0.9852604162568657, 'support': 10182} weighted_avg {'precision': 0.9856919030439426, 'recall': 0.9856609703398154, 'f1-score': 0.9856682431651689, 'support': 10182}
 
time = 18.20 secondes

Val loss 0.6499994494058651 accuracy 0.9081271886825562 macro_avg {'precision': 0.9133332615340874, 'recall': 0.9124019139221163, 'f1-score': 0.9109679966354027, 'support': 1132} weighted_avg {'precision': 0.9135465467954861, 'recall': 0.9081272084805654, 'f1-score': 0.909035151337576, 'support': 1132}
 
----------
Epoch 19/40
time = 517.70 secondes

Train loss 0.07505032263358644 accuracy 0.9877234697341919 macro_avg {'precision': 0.9876592441839106, 'recall': 0.9871667767485757, 'f1-score': 0.9873850470405537, 'support': 10182} weighted_avg {'precision': 0.9877581570049153, 'recall': 0.9877234335101159, 'f1-score': 0.9877159587660805, 'support': 10182}
 
time = 18.43 secondes

Val loss 0.742241414167105 accuracy 0.9107773900032043 macro_avg {'precision': 0.9120620433502579, 'recall': 0.9117773232773283, 'f1-score': 0.9098551129992922, 'support': 1132} weighted_avg {'precision': 0.9145071321678797, 'recall': 0.9107773851590106, 'f1-score': 0.9105557055583945, 'support': 1132}
 
----------
Epoch 20/40
time = 519.13 secondes

Train loss 0.07342538091298698 accuracy 0.9868395328521729 macro_avg {'precision': 0.9865752484313022, 'recall': 0.9864647402352371, 'f1-score': 0.9865087725251064, 'support': 10182} weighted_avg {'precision': 0.9868563957469157, 'recall': 0.9868395207228442, 'f1-score': 0.9868364565677965, 'support': 10182}
 
time = 18.26 secondes

Val loss 0.7248951390557381 accuracy 0.9028268456459045 macro_avg {'precision': 0.9129100054420439, 'recall': 0.9081323613536677, 'f1-score': 0.9052838629361846, 'support': 1132} weighted_avg {'precision': 0.9124881014007962, 'recall': 0.9028268551236749, 'f1-score': 0.9014963790466378, 'support': 1132}
 
----------
Epoch 21/40
time = 521.21 secondes

Train loss 0.06388608504092125 accuracy 0.9879198670387268 macro_avg {'precision': 0.9870602306488678, 'recall': 0.9867307064863248, 'f1-score': 0.9868873620045184, 'support': 10182} weighted_avg {'precision': 0.9878947640208375, 'recall': 0.987919858573954, 'f1-score': 0.9879005566559375, 'support': 10182}
 
time = 18.50 secondes

Val loss 0.9369374489995633 accuracy 0.898409903049469 macro_avg {'precision': 0.9103622454457201, 'recall': 0.8992411224356223, 'f1-score': 0.9000528752452992, 'support': 1132} weighted_avg {'precision': 0.911368380319416, 'recall': 0.8984098939929329, 'f1-score': 0.9001995002090999, 'support': 1132}
 
----------
Epoch 22/40
time = 518.46 secondes

Train loss 0.058205102776708866 accuracy 0.9904733896255493 macro_avg {'precision': 0.9900452785740337, 'recall': 0.989924836282734, 'f1-score': 0.9899758498862535, 'support': 10182} weighted_avg {'precision': 0.9904858752528716, 'recall': 0.9904733844038499, 'f1-score': 0.9904702395235996, 'support': 10182}
 
time = 18.31 secondes

Val loss 0.7188055784000669 accuracy 0.9107773900032043 macro_avg {'precision': 0.9144771509412379, 'recall': 0.9127093226250895, 'f1-score': 0.9113491040996028, 'support': 1132} weighted_avg {'precision': 0.9161901879603126, 'recall': 0.9107773851590106, 'f1-score': 0.9114098617381123, 'support': 1132}
 
----------
Epoch 23/40
time = 518.68 secondes

Train loss 0.05429971344040309 accuracy 0.9907680749893188 macro_avg {'precision': 0.9906275815232812, 'recall': 0.9904950031562466, 'f1-score': 0.9905405130372478, 'support': 10182} weighted_avg {'precision': 0.9907928751766903, 'recall': 0.9907680219996071, 'f1-score': 0.990762398773797, 'support': 10182}
 
time = 18.65 secondes

Val loss 0.6720501019562632 accuracy 0.916077733039856 macro_avg {'precision': 0.919637169007282, 'recall': 0.9173412772275377, 'f1-score': 0.9170250225272356, 'support': 1132} weighted_avg {'precision': 0.9185297232968325, 'recall': 0.916077738515901, 'f1-score': 0.915775355815581, 'support': 1132}
 
----------
Epoch 24/40
time = 517.71 secondes

Train loss 0.061572860395941544 accuracy 0.9905716180801392 macro_avg {'precision': 0.9904051138278882, 'recall': 0.9902207962187013, 'f1-score': 0.9902966893829641, 'support': 10182} weighted_avg {'precision': 0.9905951759868235, 'recall': 0.990571596935769, 'f1-score': 0.9905668914057176, 'support': 10182}
 
time = 18.34 secondes

Val loss 0.6861049582376054 accuracy 0.9143109321594238 macro_avg {'precision': 0.9193100523984258, 'recall': 0.9176329497400058, 'f1-score': 0.9167463141021036, 'support': 1132} weighted_avg {'precision': 0.9183475844699135, 'recall': 0.9143109540636042, 'f1-score': 0.9144417247954016, 'support': 1132}
 
----------
Epoch 25/40
time = 519.80 secondes

Train loss 0.04914073276498471 accuracy 0.9914555549621582 macro_avg {'precision': 0.9909548664403808, 'recall': 0.9910869328519631, 'f1-score': 0.9910077951926546, 'support': 10182} weighted_avg {'precision': 0.9914898257104927, 'recall': 0.9914555097230406, 'f1-score': 0.9914612835013332, 'support': 10182}
 
time = 18.35 secondes

Val loss 0.6884180495240209 accuracy 0.9204947352409363 macro_avg {'precision': 0.9238135892655478, 'recall': 0.9240444514953705, 'f1-score': 0.9217014865972823, 'support': 1132} weighted_avg {'precision': 0.9251750578917892, 'recall': 0.9204946996466431, 'f1-score': 0.9205637276128031, 'support': 1132}
 
----------
Epoch 26/40
time = 519.35 secondes

Train loss 0.048764618406177296 accuracy 0.9909644722938538 macro_avg {'precision': 0.9909922746886364, 'recall': 0.9910666979216393, 'f1-score': 0.991020286815586, 'support': 10182} weighted_avg {'precision': 0.9909780119747141, 'recall': 0.9909644470634453, 'f1-score': 0.9909621446646798, 'support': 10182}
 
time = 18.45 secondes

Val loss 0.6081658836034401 accuracy 0.9204947352409363 macro_avg {'precision': 0.9272857145285718, 'recall': 0.9225477941777998, 'f1-score': 0.9234688909546016, 'support': 1132} weighted_avg {'precision': 0.9259896029504944, 'recall': 0.9204946996466431, 'f1-score': 0.9215819978985014, 'support': 1132}
 
----------
Epoch 27/40
time = 516.75 secondes

Train loss 0.039210024716258444 accuracy 0.9933215975761414 macro_avg {'precision': 0.9933534164893072, 'recall': 0.9932775935396982, 'f1-score': 0.993304485727878, 'support': 10182} weighted_avg {'precision': 0.9933470791744575, 'recall': 0.993321547829503, 'f1-score': 0.9933232027323982, 'support': 10182}
 
time = 18.36 secondes

Val loss 0.7534147412745728 accuracy 0.9125441908836365 macro_avg {'precision': 0.9171538473068737, 'recall': 0.917601777691376, 'f1-score': 0.9155122357005485, 'support': 1132} weighted_avg {'precision': 0.9178072376714693, 'recall': 0.9125441696113075, 'f1-score': 0.9131885778881467, 'support': 1132}
 
----------
Epoch 28/40
time = 518.64 secondes

Train loss 0.03306114523160377 accuracy 0.9939108490943909 macro_avg {'precision': 0.9938521240836671, 'recall': 0.9936555848676394, 'f1-score': 0.9937398610627447, 'support': 10182} weighted_avg {'precision': 0.9939356236221396, 'recall': 0.9939108230210175, 'f1-score': 0.9939116741862815, 'support': 10182}
 
time = 18.32 secondes

Val loss 0.6870154221098449 accuracy 0.926678478717804 macro_avg {'precision': 0.9283176377846851, 'recall': 0.9289041596391222, 'f1-score': 0.9274790851914044, 'support': 1132} weighted_avg {'precision': 0.9289137393882696, 'recall': 0.926678445229682, 'f1-score': 0.9266963271262507, 'support': 1132}
 
----------
Epoch 29/40
time = 517.71 secondes

Train loss 0.03845179546783191 accuracy 0.9939108490943909 macro_avg {'precision': 0.9937585389971975, 'recall': 0.9938513481514946, 'f1-score': 0.9937964624694487, 'support': 10182} weighted_avg {'precision': 0.9939266964649988, 'recall': 0.9939108230210175, 'f1-score': 0.9939100023927387, 'support': 10182}
 
time = 18.19 secondes

Val loss 0.8060569754637418 accuracy 0.9063604474067688 macro_avg {'precision': 0.9160398788060752, 'recall': 0.9101066457912251, 'f1-score': 0.9099125444681615, 'support': 1132} weighted_avg {'precision': 0.916227592618141, 'recall': 0.9063604240282686, 'f1-score': 0.9078059915783122, 'support': 1132}
 
----------
Epoch 30/40
time = 518.20 secondes

Train loss 0.04201234429644417 accuracy 0.9934197664260864 macro_avg {'precision': 0.9932216652517211, 'recall': 0.9931111105578166, 'f1-score': 0.9931620260105106, 'support': 10182} weighted_avg {'precision': 0.9934273890206411, 'recall': 0.9934197603614221, 'f1-score': 0.9934197016766129, 'support': 10182}
 
time = 18.23 secondes

Val loss 0.7590857931514415 accuracy 0.9098939895629883 macro_avg {'precision': 0.9130810209782204, 'recall': 0.9142106147512168, 'f1-score': 0.9106880340942908, 'support': 1132} weighted_avg {'precision': 0.9161483701503165, 'recall': 0.9098939929328622, 'f1-score': 0.9098582499823841, 'support': 1132}
 
----------
Epoch 31/40
time = 516.08 secondes

Train loss 0.03995625828822393 accuracy 0.9935179948806763 macro_avg {'precision': 0.9937448228427985, 'recall': 0.9933526572575635, 'f1-score': 0.9935214313571008, 'support': 10182} weighted_avg {'precision': 0.99357706909088, 'recall': 0.9935179728933412, 'f1-score': 0.9935198872486068, 'support': 10182}
 
time = 18.33 secondes

Val loss 0.6318957512953026 accuracy 0.926678478717804 macro_avg {'precision': 0.9314967214626076, 'recall': 0.9297971228060369, 'f1-score': 0.9296906948095082, 'support': 1132} weighted_avg {'precision': 0.9300565521353833, 'recall': 0.926678445229682, 'f1-score': 0.9273675931055247, 'support': 1132}
 
----------
Epoch 32/40
time = 516.84 secondes

Train loss 0.03322240672751322 accuracy 0.9956786632537842 macro_avg {'precision': 0.9957849418372373, 'recall': 0.9956505917959726, 'f1-score': 0.9957093401593013, 'support': 10182} weighted_avg {'precision': 0.9956910111827979, 'recall': 0.9956786485955608, 'f1-score': 0.9956762177916552, 'support': 10182}
 
time = 18.25 secondes

Val loss 0.6755649605165929 accuracy 0.9249116778373718 macro_avg {'precision': 0.9282373639309729, 'recall': 0.9276830920461536, 'f1-score': 0.9268420241245844, 'support': 1132} weighted_avg {'precision': 0.9276819941501997, 'recall': 0.9249116607773852, 'f1-score': 0.9251154165286929, 'support': 1132}
 
----------
Epoch 33/40
time = 517.48 secondes

Train loss 0.029180479068034705 accuracy 0.9953840374946594 macro_avg {'precision': 0.995525923277871, 'recall': 0.9955109907941692, 'f1-score': 0.9955113332370094, 'support': 10182} weighted_avg {'precision': 0.9953983515354382, 'recall': 0.9953840109998036, 'f1-score': 0.9953838033156005, 'support': 10182}
 
time = 18.35 secondes

Val loss 0.6436482204958914 accuracy 0.9275618195533752 macro_avg {'precision': 0.9354893286481559, 'recall': 0.9295988672451323, 'f1-score': 0.9313592321481445, 'support': 1132} weighted_avg {'precision': 0.9323131422083172, 'recall': 0.9275618374558304, 'f1-score': 0.9286786894980532, 'support': 1132}
 
----------
Epoch 34/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 2; 22.17 GiB total capacity; 11.20 GiB already allocated; 111.69 MiB free; 11.29 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_tail_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 192.00 MiB (GPU 2; 22.17 GiB total capacity; 10.97 GiB already allocated; 193.69 MiB free; 11.21 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
There are 3 GPU(s) available.
We will use the GPU: Tesla V100-PCIE-32GB
##########
20newsgroups_BERT_head_none_1
----------
Epoch 1/40
Exception
CUDA out of memory. Tried to allocate 96.00 MiB (GPU 2; 22.17 GiB total capacity; 11.33 GiB already allocated; 25.69 MiB free; 11.38 GiB reserved in total by PyTorch)
Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "test_trun_sum.py", line 78, in <module>
    data_train['data'] = bert_summarizer(data_train['data'])
  File "/vol/fob-vol3/nebenf20/wubingti/data/22_wub_longdocclassification/utils.py", line 137, in bert_summarizer
    summarized_docs = [bert_summarizer(doc, num_sentences=30) for doc in docs]
  File "/vol/fob-vol3/nebenf20/wubingti/data/22_wub_longdocclassification/utils.py", line 137, in <listcomp>
    summarized_docs = [bert_summarizer(doc, num_sentences=30) for doc in docs]
  File "/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/summary_processor.py", line 235, in __call__
    use_first, algorithm, num_sentences, return_as_list)
  File "/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/summary_processor.py", line 202, in run
    sentences, _ = self.cluster_runner(sentences, ratio, algorithm, use_first, num_sentences)
  File "/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/summary_processor.py", line 108, in cluster_runner
    hidden = self.model(sentences)
  File "/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/transformer_embeddings/bert_embedding.py", line 173, in __call__
    return self.create_matrix(content, hidden, reduce_option, hidden_concat)
  File "/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/transformer_embeddings/bert_embedding.py", line 154, in create_matrix
    ).data.cpu().numpy()) for t in content
  File "/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/transformer_embeddings/bert_embedding.py", line 154, in <listcomp>
    ).data.cpu().numpy()) for t in content
  File "/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/summarizer/transformer_embeddings/bert_embedding.py", line 108, in extract_embeddings
    pooled, hidden_states = self.model(tokens_tensor)[-2:]
  File "/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/vol/fob-vol3/nebenf20/wubingti/.local/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py", line 962, in forward
    buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)
RuntimeError: The expanded size of the tensor (513) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 513].  Tensor sizes: [1, 512]
